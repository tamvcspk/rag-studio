# 2.1.4 Hybrid Search Specification

**Phase:** 2.1 KB Data Model & API
**Component:** Hybrid Search
**Priority:** HIGH
**Dependencies:** 2.1.1 KB Schema, 1.1.2 LanceDB Integration, 1.3.1 Embedding Service

## Overview

Implement hybrid search combining vector (LanceDB ANN) and BM25 (Rust implementation) search with parallel execution, adaptive candidate sets, merge scoring algorithms, and performance optimization for <100ms retrieval targets.

## Requirements

### Functional Requirements

#### FR-2.1.4.1 Vector Search Integration
- Integrate with LanceDB for approximate nearest neighbor (ANN) search
- Support configurable similarity metrics (cosine, euclidean, dot product)
- Implement query vector generation via Embedding Service
- Support metadata filtering and pre-filtering for vector search
- Provide configurable Top-K candidate retrieval (50-500 candidates)

#### FR-2.1.4.2 BM25 Lexical Search
- Implement native Rust BM25 scoring algorithm for performance
- Support configurable BM25 parameters (k1=1.2, b=0.75 defaults)
- Build inverted indexes with term frequency and document frequency
- Support phrase queries and boolean operators (AND, OR, NOT)
- Implement efficient query processing with early termination

#### FR-2.1.4.3 Parallel Search Execution
- Execute vector and BM25 searches concurrently using tokio::join!
- Implement independent candidate retrieval with configurable limits
- Support timeout-based search termination for performance guarantees
- Provide fallback strategies for failed search components
- Optimize memory usage during parallel execution

#### FR-2.1.4.4 Hybrid Merge Scoring
- Implement configurable score fusion algorithms (linear, reciprocal rank fusion)
- Support adaptive weighting based on query characteristics
- Provide score normalization for fair comparison between search types
- Implement relevance score calibration and confidence estimation
- Support result re-ranking based on combined scores

#### FR-2.1.4.5 Adaptive Candidate Sets
- Dynamically adjust candidate set sizes based on overlap and quality
- Implement backfill strategies for insufficient results
- Support query expansion for improved recall
- Provide adaptive timeout adjustment based on collection size
- Optimize candidate selection for diverse result sets

#### FR-2.1.4.6 Filtering and Pre-filtering
- Support metadata-based filtering (collection, document type, date range)
- Implement efficient pre-filtering for vector search
- Support post-filtering for result refinement
- Provide combination of multiple filter criteria with AND/OR logic
- Optimize filter application for minimal performance impact

### Non-Functional Requirements

#### NFR-2.1.4.1 Performance
- Achieve <100ms P50 latency for hybrid search queries
- Support <200ms P95 latency under normal load conditions
- Handle concurrent search requests up to 100 QPS
- Maintain memory usage under 100MB per search operation

#### NFR-2.1.4.2 Scalability
- Scale to collections with 1M+ documents and 10M+ chunks
- Support vector dimensions up to 1536 (OpenAI embedding size)
- Handle query volumes up to 1000 QPS with horizontal scaling
- Optimize index sizes for efficient memory usage

#### NFR-2.1.4.3 Accuracy
- Achieve >90% recall@10 for relevant results
- Maintain >85% precision@10 for high-quality results
- Provide consistent scoring across different query types
- Support relevance evaluation and quality metrics

## Technical Specification

### Hybrid Search Architecture

```rust
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use tokio::time::{timeout, Duration};

#[derive(Debug, Clone)]
pub struct HybridSearchEngine {
    vector_db: Arc<dyn VectorDbService>,
    bm25_index: Arc<BM25Index>,
    embedding_service: Arc<dyn EmbeddingService>,
    config: SearchConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SearchConfig {
    pub vector_weight: f64,        // Default: 0.7
    pub bm25_weight: f64,          // Default: 0.3
    pub max_candidates: usize,     // Default: 200
    pub min_candidates: usize,     // Default: 50
    pub search_timeout_ms: u64,    // Default: 100
    pub fusion_algorithm: FusionAlgorithm,
    pub bm25_params: BM25Params,
    pub vector_params: VectorParams,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum FusionAlgorithm {
    LinearCombination,
    ReciprocalRankFusion { k: f64 }, // Default k=60
    WeightedSum,
    MaxScore,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BM25Params {
    pub k1: f64,  // Default: 1.2
    pub b: f64,   // Default: 0.75
    pub min_term_freq: usize,  // Default: 1
    pub max_query_terms: usize, // Default: 20
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct VectorParams {
    pub similarity_metric: SimilarityMetric,
    pub ef_search: usize,     // Default: 100 (for HNSW)
    pub nprobe: usize,        // Default: 10 (for IVF)
    pub rerank_candidates: bool, // Default: true
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum SimilarityMetric {
    Cosine,
    Euclidean,
    DotProduct,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SearchRequest {
    pub query: String,
    pub collection_ids: Vec<String>,
    pub top_k: usize,
    pub filters: Option<SearchFilters>,
    pub search_config: Option<SearchConfig>,
    pub include_scores: bool,
    pub include_embeddings: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SearchFilters {
    pub document_types: Option<Vec<String>>,
    pub date_range: Option<DateRange>,
    pub metadata_filters: Option<HashMap<String, serde_json::Value>>,
    pub chunk_types: Option<Vec<String>>,
    pub min_score: Option<f64>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DateRange {
    pub start: Option<chrono::DateTime<chrono::Utc>>,
    pub end: Option<chrono::DateTime<chrono::Utc>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SearchResult {
    pub results: Vec<SearchHit>,
    pub query_info: QueryInfo,
    pub performance_metrics: PerformanceMetrics,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SearchHit {
    pub chunk_id: String,
    pub document_id: String,
    pub collection_id: String,
    pub content: String,
    pub title: String,
    pub score: f64,
    pub vector_score: Option<f64>,
    pub bm25_score: Option<f64>,
    pub citation: Citation,
    pub metadata: serde_json::Value,
    pub embedding: Option<Vec<f32>>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Citation {
    pub title: String,
    pub source_url: Option<String>,
    pub source_path: Option<String>,
    pub anchor: Option<String>,
    pub license: Option<String>,
    pub version: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct QueryInfo {
    pub processed_query: String,
    pub query_vector_dim: usize,
    pub query_terms: Vec<String>,
    pub expansion_terms: Option<Vec<String>>,
    pub filters_applied: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformanceMetrics {
    pub total_time_ms: u64,
    pub vector_search_time_ms: u64,
    pub bm25_search_time_ms: u64,
    pub merge_time_ms: u64,
    pub vector_candidates: usize,
    pub bm25_candidates: usize,
    pub final_results: usize,
    pub cache_hit: bool,
}

impl HybridSearchEngine {
    pub fn new(
        vector_db: Arc<dyn VectorDbService>,
        bm25_index: Arc<BM25Index>,
        embedding_service: Arc<dyn EmbeddingService>,
        config: SearchConfig,
    ) -> Self {
        Self {
            vector_db,
            bm25_index,
            embedding_service,
            config,
        }
    }

    pub async fn search(&self, request: SearchRequest) -> Result<SearchResult, SearchError> {
        let start_time = std::time::Instant::now();
        let search_config = request.search_config.as_ref().unwrap_or(&self.config);

        // Preprocess query
        let query_info = self.preprocess_query(&request.query).await?;

        // Execute parallel searches with timeout
        let search_future = self.execute_parallel_searches(&request, &query_info, search_config);
        let (vector_results, bm25_results, timing) = timeout(
            Duration::from_millis(search_config.search_timeout_ms),
            search_future,
        ).await.map_err(|_| SearchError::Timeout)?;

        // Merge and rank results
        let merge_start = std::time::Instant::now();
        let merged_results = self.merge_results(
            vector_results,
            bm25_results,
            search_config,
            request.top_k,
        ).await?;
        let merge_time = merge_start.elapsed().as_millis() as u64;

        // Enrich with citations and metadata
        let final_results = self.enrich_results(merged_results, &request).await?;

        let total_time = start_time.elapsed().as_millis() as u64;

        Ok(SearchResult {
            results: final_results,
            query_info,
            performance_metrics: PerformanceMetrics {
                total_time_ms: total_time,
                vector_search_time_ms: timing.vector_time,
                bm25_search_time_ms: timing.bm25_time,
                merge_time_ms: merge_time,
                vector_candidates: timing.vector_candidates,
                bm25_candidates: timing.bm25_candidates,
                final_results: final_results.len(),
                cache_hit: false, // Will be set by caching layer
            },
        })
    }

    async fn preprocess_query(&self, query: &str) -> Result<QueryInfo, SearchError> {
        // Clean and normalize query
        let processed_query = self.normalize_query(query);

        // Generate query terms for BM25
        let query_terms = self.tokenize_query(&processed_query);

        // Generate query vector for vector search
        let query_vector = self.embedding_service
            .embed_texts(vec![processed_query.clone()])
            .await
            .map_err(|e| SearchError::EmbeddingError(e.to_string()))?;

        let query_vector_dim = query_vector.first()
            .map(|v| v.len())
            .unwrap_or(0);

        Ok(QueryInfo {
            processed_query,
            query_vector_dim,
            query_terms,
            expansion_terms: None, // Could implement query expansion here
            filters_applied: false, // Will be set during search
        })
    }

    fn normalize_query(&self, query: &str) -> String {
        // Remove extra whitespace, convert to lowercase, handle special characters
        query.trim()
            .to_lowercase()
            .split_whitespace()
            .collect::<Vec<_>>()
            .join(" ")
    }

    fn tokenize_query(&self, query: &str) -> Vec<String> {
        // Simple tokenization - could be enhanced with stemming, stop words, etc.
        query.split_whitespace()
            .filter(|term| term.len() > 1)
            .take(self.config.bm25_params.max_query_terms)
            .map(|s| s.to_string())
            .collect()
    }

    async fn execute_parallel_searches(
        &self,
        request: &SearchRequest,
        query_info: &QueryInfo,
        config: &SearchConfig,
    ) -> Result<(Vec<VectorResult>, Vec<BM25Result>, SearchTiming), SearchError> {
        let vector_search = self.vector_search(request, query_info, config);
        let bm25_search = self.bm25_search(request, query_info, config);

        // Execute searches in parallel
        let ((vector_results, vector_time), (bm25_results, bm25_time)) = tokio::join!(
            self.timed_search(vector_search),
            self.timed_search(bm25_search)
        );

        let vector_results = vector_results?;
        let bm25_results = bm25_search?;

        Ok((
            vector_results.results,
            bm25_results.results,
            SearchTiming {
                vector_time,
                bm25_time,
                vector_candidates: vector_results.results.len(),
                bm25_candidates: bm25_results.results.len(),
            },
        ))
    }

    async fn timed_search<T, F, E>(&self, search_future: F) -> (Result<T, E>, u64)
    where
        F: std::future::Future<Output = Result<T, E>>,
    {
        let start = std::time::Instant::now();
        let result = search_future.await;
        let elapsed = start.elapsed().as_millis() as u64;
        (result, elapsed)
    }

    async fn vector_search(
        &self,
        request: &SearchRequest,
        query_info: &QueryInfo,
        config: &SearchConfig,
    ) -> Result<VectorSearchResults, SearchError> {
        // Generate query embedding
        let query_embedding = self.embedding_service
            .embed_texts(vec![query_info.processed_query.clone()])
            .await
            .map_err(|e| SearchError::EmbeddingError(e.to_string()))?
            .into_iter()
            .next()
            .ok_or(SearchError::EmptyEmbedding)?;

        // Apply filters for vector search
        let vector_filters = self.build_vector_filters(request.filters.as_ref())?;

        // Search vector database
        let candidates = self.vector_db
            .search_similar(
                &query_embedding,
                config.max_candidates,
                vector_filters,
                config.vector_params.similarity_metric.clone(),
            )
            .await
            .map_err(|e| SearchError::VectorSearchError(e.to_string()))?;

        Ok(VectorSearchResults {
            results: candidates.into_iter()
                .map(|c| VectorResult {
                    chunk_id: c.id,
                    score: c.score,
                    metadata: c.metadata,
                })
                .collect(),
        })
    }

    async fn bm25_search(
        &self,
        request: &SearchRequest,
        query_info: &QueryInfo,
        config: &SearchConfig,
    ) -> Result<BM25SearchResults, SearchError> {
        // Apply filters for BM25 search
        let bm25_filters = self.build_bm25_filters(request.filters.as_ref())?;

        // Execute BM25 search
        let results = self.bm25_index
            .search(
                &query_info.query_terms,
                config.max_candidates,
                &config.bm25_params,
                bm25_filters,
            )
            .await?;

        Ok(BM25SearchResults {
            results: results.into_iter()
                .map(|r| BM25Result {
                    chunk_id: r.chunk_id,
                    score: r.score,
                    term_matches: r.term_matches,
                })
                .collect(),
        })
    }

    async fn merge_results(
        &self,
        vector_results: Vec<VectorResult>,
        bm25_results: Vec<BM25Result>,
        config: &SearchConfig,
        top_k: usize,
    ) -> Result<Vec<MergedResult>, SearchError> {
        // Create lookup maps for efficient merging
        let mut vector_map: HashMap<String, f64> = vector_results
            .into_iter()
            .map(|r| (r.chunk_id, r.score))
            .collect();

        let mut bm25_map: HashMap<String, f64> = bm25_results
            .into_iter()
            .map(|r| (r.chunk_id, r.score))
            .collect();

        // Collect all unique chunk IDs
        let mut all_chunk_ids: std::collections::HashSet<String> = vector_map.keys().cloned().collect();
        all_chunk_ids.extend(bm25_map.keys().cloned());

        // Calculate combined scores using fusion algorithm
        let mut merged_results = Vec::new();

        for chunk_id in all_chunk_ids {
            let vector_score = vector_map.remove(&chunk_id).unwrap_or(0.0);
            let bm25_score = bm25_map.remove(&chunk_id).unwrap_or(0.0);

            let combined_score = self.calculate_fusion_score(
                vector_score,
                bm25_score,
                &config.fusion_algorithm,
                config.vector_weight,
                config.bm25_weight,
            );

            merged_results.push(MergedResult {
                chunk_id,
                combined_score,
                vector_score: if vector_score > 0.0 { Some(vector_score) } else { None },
                bm25_score: if bm25_score > 0.0 { Some(bm25_score) } else { None },
            });
        }

        // Sort by combined score and take top-k
        merged_results.sort_by(|a, b| b.combined_score.partial_cmp(&a.combined_score).unwrap());
        merged_results.truncate(top_k);

        Ok(merged_results)
    }

    fn calculate_fusion_score(
        &self,
        vector_score: f64,
        bm25_score: f64,
        algorithm: &FusionAlgorithm,
        vector_weight: f64,
        bm25_weight: f64,
    ) -> f64 {
        match algorithm {
            FusionAlgorithm::LinearCombination => {
                vector_score * vector_weight + bm25_score * bm25_weight
            }
            FusionAlgorithm::ReciprocalRankFusion { k } => {
                // Simplified RRF - would need actual ranks in real implementation
                let vector_rrf = if vector_score > 0.0 { 1.0 / (k + 1.0) } else { 0.0 };
                let bm25_rrf = if bm25_score > 0.0 { 1.0 / (k + 1.0) } else { 0.0 };
                vector_rrf + bm25_rrf
            }
            FusionAlgorithm::WeightedSum => {
                (vector_score * vector_weight + bm25_score * bm25_weight) / (vector_weight + bm25_weight)
            }
            FusionAlgorithm::MaxScore => {
                vector_score.max(bm25_score)
            }
        }
    }

    async fn enrich_results(
        &self,
        merged_results: Vec<MergedResult>,
        request: &SearchRequest,
    ) -> Result<Vec<SearchHit>, SearchError> {
        let mut enriched_results = Vec::new();

        for result in merged_results {
            // Fetch chunk details from database
            let chunk_details = self.get_chunk_details(&result.chunk_id).await?;

            // Build citation
            let citation = Citation {
                title: chunk_details.document_title,
                source_url: chunk_details.source_url,
                source_path: chunk_details.source_path,
                anchor: chunk_details.citation_anchor,
                license: chunk_details.license,
                version: chunk_details.version,
            };

            let search_hit = SearchHit {
                chunk_id: result.chunk_id,
                document_id: chunk_details.document_id,
                collection_id: chunk_details.collection_id,
                content: chunk_details.content,
                title: chunk_details.document_title,
                score: result.combined_score,
                vector_score: result.vector_score,
                bm25_score: result.bm25_score,
                citation,
                metadata: chunk_details.metadata,
                embedding: if request.include_embeddings {
                    Some(chunk_details.embedding)
                } else {
                    None
                },
            };

            enriched_results.push(search_hit);
        }

        Ok(enriched_results)
    }

    async fn get_chunk_details(&self, chunk_id: &str) -> Result<ChunkDetails, SearchError> {
        // This would fetch detailed chunk information from the database
        // including content, metadata, citation info, etc.
        todo!("Implement chunk details fetching from database")
    }

    fn build_vector_filters(&self, filters: Option<&SearchFilters>) -> Result<VectorFilters, SearchError> {
        // Convert search filters to vector database filters
        todo!("Implement vector filter building")
    }

    fn build_bm25_filters(&self, filters: Option<&SearchFilters>) -> Result<BM25Filters, SearchError> {
        // Convert search filters to BM25 index filters
        todo!("Implement BM25 filter building")
    }
}

// Supporting data structures
#[derive(Debug, Clone)]
struct VectorResult {
    chunk_id: String,
    score: f64,
    metadata: serde_json::Value,
}

#[derive(Debug, Clone)]
struct BM25Result {
    chunk_id: String,
    score: f64,
    term_matches: Vec<String>,
}

#[derive(Debug, Clone)]
struct MergedResult {
    chunk_id: String,
    combined_score: f64,
    vector_score: Option<f64>,
    bm25_score: Option<f64>,
}

#[derive(Debug, Clone)]
struct VectorSearchResults {
    results: Vec<VectorResult>,
}

#[derive(Debug, Clone)]
struct BM25SearchResults {
    results: Vec<BM25Result>,
}

#[derive(Debug, Clone)]
struct SearchTiming {
    vector_time: u64,
    bm25_time: u64,
    vector_candidates: usize,
    bm25_candidates: usize,
}

#[derive(Debug, Clone)]
struct ChunkDetails {
    document_id: String,
    collection_id: String,
    content: String,
    document_title: String,
    source_url: Option<String>,
    source_path: Option<String>,
    citation_anchor: Option<String>,
    license: Option<String>,
    version: Option<String>,
    metadata: serde_json::Value,
    embedding: Vec<f32>,
}

// Filter types (to be implemented)
type VectorFilters = HashMap<String, serde_json::Value>;
type BM25Filters = HashMap<String, serde_json::Value>;

#[derive(Debug, thiserror::Error)]
pub enum SearchError {
    #[error("Search timeout exceeded")]
    Timeout,

    #[error("Embedding error: {0}")]
    EmbeddingError(String),

    #[error("Vector search error: {0}")]
    VectorSearchError(String),

    #[error("BM25 search error: {0}")]
    BM25SearchError(String),

    #[error("Empty embedding generated")]
    EmptyEmbedding,

    #[error("Database error: {0}")]
    DatabaseError(String),

    #[error("Invalid filter: {0}")]
    InvalidFilter(String),
}
```

### BM25 Index Implementation

```rust
use std::collections::HashMap;
use serde::{Deserialize, Serialize};

pub struct BM25Index {
    // Term frequency: term -> doc_id -> frequency
    term_freq: HashMap<String, HashMap<String, usize>>,
    // Document frequency: term -> number of documents containing term
    doc_freq: HashMap<String, usize>,
    // Document lengths: doc_id -> length in tokens
    doc_lengths: HashMap<String, usize>,
    // Total number of documents
    total_docs: usize,
    // Average document length
    avg_doc_length: f64,
}

impl BM25Index {
    pub fn new() -> Self {
        Self {
            term_freq: HashMap::new(),
            doc_freq: HashMap::new(),
            doc_lengths: HashMap::new(),
            total_docs: 0,
            avg_doc_length: 0.0,
        }
    }

    pub async fn add_document(&mut self, doc_id: String, tokens: Vec<String>) {
        let doc_length = tokens.len();

        // Update document length
        self.doc_lengths.insert(doc_id.clone(), doc_length);

        // Count term frequencies in this document
        let mut term_counts = HashMap::new();
        for token in tokens {
            *term_counts.entry(token).or_insert(0) += 1;
        }

        // Update global term frequencies and document frequencies
        for (term, count) in term_counts {
            self.term_freq
                .entry(term.clone())
                .or_insert_with(HashMap::new)
                .insert(doc_id.clone(), count);

            *self.doc_freq.entry(term).or_insert(0) += 1;
        }

        self.total_docs += 1;
        self.recalculate_avg_length();
    }

    pub async fn search(
        &self,
        query_terms: &[String],
        max_results: usize,
        params: &BM25Params,
        filters: BM25Filters,
    ) -> Result<Vec<BM25SearchResult>, SearchError> {
        let mut scores: HashMap<String, f64> = HashMap::new();

        for term in query_terms {
            if let Some(term_docs) = self.term_freq.get(term) {
                let df = self.doc_freq.get(term).unwrap_or(&0);
                let idf = self.calculate_idf(*df);

                for (doc_id, tf) in term_docs {
                    let doc_length = self.doc_lengths.get(doc_id).unwrap_or(&0);
                    let score = self.calculate_term_score(*tf, *doc_length, idf, params);
                    *scores.entry(doc_id.clone()).or_insert(0.0) += score;
                }
            }
        }

        // Apply filters and sort results
        let mut results: Vec<_> = scores
            .into_iter()
            .filter(|(doc_id, _score)| self.apply_filters(doc_id, &filters))
            .map(|(doc_id, score)| BM25SearchResult {
                chunk_id: doc_id,
                score,
                term_matches: query_terms.to_vec(),
            })
            .collect();

        results.sort_by(|a, b| b.score.partial_cmp(&a.score).unwrap());
        results.truncate(max_results);

        Ok(results)
    }

    fn calculate_idf(&self, df: usize) -> f64 {
        if df == 0 {
            0.0
        } else {
            ((self.total_docs as f64 - df as f64 + 0.5) / (df as f64 + 0.5)).ln()
        }
    }

    fn calculate_term_score(&self, tf: usize, doc_length: usize, idf: f64, params: &BM25Params) -> f64 {
        let tf_component = (tf as f64 * (params.k1 + 1.0)) /
            (tf as f64 + params.k1 * (1.0 - params.b + params.b * (doc_length as f64 / self.avg_doc_length)));

        idf * tf_component
    }

    fn recalculate_avg_length(&mut self) {
        if self.total_docs > 0 {
            let total_length: usize = self.doc_lengths.values().sum();
            self.avg_doc_length = total_length as f64 / self.total_docs as f64;
        }
    }

    fn apply_filters(&self, _doc_id: &str, _filters: &BM25Filters) -> bool {
        // Implement filter application logic
        true
    }
}

#[derive(Debug, Clone)]
pub struct BM25SearchResult {
    pub chunk_id: String,
    pub score: f64,
    pub term_matches: Vec<String>,
}
```

## Implementation Details

### Performance Optimization Strategies
- Use Rust's zero-cost abstractions for optimal performance
- Implement early termination for search operations
- Cache frequently accessed data structures in memory
- Optimize memory allocation patterns for reduced GC pressure

### Parallel Execution Architecture
- Use tokio::join! for concurrent vector and BM25 searches
- Implement timeout-based search termination
- Support graceful degradation if one search component fails
- Optimize thread pool usage for maximum throughput

### Score Fusion Algorithms
- Implement multiple fusion strategies for different use cases
- Support adaptive weighting based on query characteristics
- Provide score normalization for fair comparison
- Enable A/B testing of different fusion approaches

## Testing Strategy

### Unit Tests
- Test individual search components (vector, BM25, fusion)
- Verify score calculation accuracy
- Test filter application and query preprocessing
- Validate parallel execution and timeout handling

### Integration Tests
- Test end-to-end search functionality with real data
- Verify search result quality and relevance
- Test performance under various load conditions
- Validate cross-component integration

### Performance Tests
- Benchmark search latency with varying collection sizes
- Test concurrent search performance up to 100 QPS
- Measure memory usage patterns during search operations
- Validate scalability with large datasets

### Quality Tests
- Measure recall@10 and precision@10 with labeled datasets
- Test search result diversity and coverage
- Validate citation accuracy and completeness
- Assess user satisfaction with search results

## Acceptance Criteria

### AC-2.1.4.1 Search Implementation
- [ ] Vector search integrated with LanceDB
- [ ] BM25 search implemented in native Rust
- [ ] Parallel execution with tokio::join!
- [ ] Multiple score fusion algorithms supported

### AC-2.1.4.2 Performance Requirements
- [ ] <100ms P50 latency achieved
- [ ] <200ms P95 latency under normal load
- [ ] Support for 100 concurrent QPS
- [ ] Memory usage under 100MB per search

### AC-2.1.4.3 Search Quality
- [ ] >90% recall@10 for relevant results
- [ ] >85% precision@10 for high-quality results
- [ ] Consistent scoring across query types
- [ ] Mandatory citations for all results

### AC-2.1.4.4 Scalability
- [ ] Support for 1M+ documents and 10M+ chunks
- [ ] Efficient handling of 1536-dimensional vectors
- [ ] Optimized index sizes for memory efficiency
- [ ] Horizontal scaling capability demonstrated