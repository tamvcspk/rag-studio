# 2.3.3 Evaluation Gates Specification

**Phase:** 2.3 Citations & Quality
**Component:** Evaluation Gates
**Priority:** HIGH
**Dependencies:** 2.1.4 Hybrid Search, 2.1.3 Ingest Pipeline, 2.2.4 Performance

## Overview

Implement evaluation gates with recall@k metrics and drift detection to ensure quality assurance through smoke tests for quality assurance, statistical drift monitoring, and performance regression detection before index promotion.

## Requirements

### Functional Requirements

#### FR-2.3.3.1 Quality Metrics and Evaluation
- Implement recall@k evaluation metrics (k=1,5,10,20) for search quality assessment
- Calculate precision@k and mean reciprocal rank (MRR) for comprehensive evaluation
- Support F1-score calculation and normalized discounted cumulative gain (nDCG)
- Implement custom quality metrics for domain-specific evaluation
- Provide baseline establishment and comparative evaluation against previous versions

#### FR-2.3.3.2 Statistical Drift Detection
- Monitor statistical drift in embedding distributions using KL divergence and Wasserstein distance
- Detect concept drift in search result patterns and user query distributions
- Implement covariate shift detection for input feature distributions
- Monitor temporal drift patterns and seasonal variations
- Support configurable drift detection thresholds and sensitivity settings

#### FR-2.3.3.3 Smoke Tests and Quality Gates
- Execute comprehensive smoke tests before index promotion including basic functionality validation
- Implement quality gate thresholds with configurable pass/fail criteria
- Support automated rollback on quality gate failures
- Provide manual override capabilities with proper authorization and audit trails
- Execute performance regression tests as part of quality gates

#### FR-2.3.3.4 Evaluation Dataset Management
- Maintain golden datasets for consistent evaluation across versions
- Support automatic evaluation dataset generation from user interactions
- Implement evaluation dataset versioning and quality control
- Provide relevance judgment collection and validation workflows
- Support synthetic evaluation data generation for comprehensive testing

#### FR-2.3.3.5 Performance Regression Detection
- Monitor search latency trends and detect performance regressions
- Track cache hit rates, throughput metrics, and resource utilization patterns
- Implement memory usage and disk space consumption monitoring
- Detect accuracy-performance trade-off degradation
- Support configurable performance threshold enforcement

#### FR-2.3.3.6 Continuous Quality Monitoring
- Implement real-time quality monitoring and alerting systems
- Track quality metrics over time with trend analysis
- Support quality dashboards with visualization and reporting
- Provide quality metric aggregation and statistical analysis
- Enable proactive quality issue detection and notification

### Non-Functional Requirements

#### NFR-2.3.3.1 Evaluation Performance
- Quality evaluation must complete within 5 minutes for collections up to 100K documents
- Drift detection must run continuously with <1% system resource overhead
- Smoke tests must complete within 30 seconds for basic functionality validation
- Performance regression detection must provide results within 2 minutes

#### NFR-2.3.3.2 Accuracy and Reliability
- Evaluation metrics must be reproducible with <1% variance between runs
- Drift detection must have <5% false positive rate for stable systems
- Quality gates must provide 99.9% reliability in blocking defective releases
- Performance regression detection must identify >95% of actual regressions

#### NFR-2.3.3.3 Scalability and Maintainability
- Evaluation system must scale to multiple collections and concurrent evaluations
- Drift detection must handle high-throughput systems without impacting performance
- Quality gates must support different evaluation profiles for different environments
- System must support easy addition of new evaluation metrics and tests

## Technical Specification

### Evaluation Gates Architecture

```rust
use serde::{Deserialize, Serialize};
use std::collections::{HashMap, VecDeque};
use std::sync::Arc;
use tokio::sync::RwLock;
use chrono::{DateTime, Utc, Duration};

#[derive(Debug, Clone)]
pub struct EvaluationGateManager {
    config: EvaluationConfig,
    metrics_calculator: Arc<MetricsCalculator>,
    drift_detector: Arc<DriftDetector>,
    smoke_tester: Arc<SmokeTester>,
    regression_detector: Arc<RegressionDetector>,
    evaluation_store: Arc<dyn EvaluationStore>,
    dataset_manager: Arc<DatasetManager>,
    alerting_system: Arc<EvaluationAlerting>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EvaluationConfig {
    pub quality_gates_enabled: bool,
    pub drift_detection_enabled: bool,
    pub smoke_tests_enabled: bool,
    pub regression_detection_enabled: bool,
    pub evaluation_thresholds: EvaluationThresholds,
    pub drift_detection_config: DriftDetectionConfig,
    pub smoke_test_config: SmokeTestConfig,
    pub evaluation_schedule: EvaluationSchedule,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EvaluationThresholds {
    pub min_recall_at_1: f64,
    pub min_recall_at_5: f64,
    pub min_recall_at_10: f64,
    pub min_precision_at_5: f64,
    pub min_mrr: f64,
    pub min_ndcg_at_10: f64,
    pub max_latency_ms: u64,
    pub min_cache_hit_rate: f64,
    pub max_performance_regression_percent: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DriftDetectionConfig {
    pub detection_window_size: usize,
    pub kl_divergence_threshold: f64,
    pub wasserstein_distance_threshold: f64,
    pub statistical_significance_level: f64,
    pub minimum_samples_for_detection: usize,
    pub drift_detection_interval_minutes: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SmokeTestConfig {
    pub test_query_count: usize,
    pub max_test_duration_seconds: u64,
    pub required_response_coverage: f64,
    pub test_datasets: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EvaluationSchedule {
    pub continuous_monitoring_enabled: bool,
    pub daily_evaluation_enabled: bool,
    pub pre_promotion_evaluation_required: bool,
    pub post_deployment_validation_enabled: bool,
}

impl EvaluationGateManager {
    pub fn new(
        config: EvaluationConfig,
        evaluation_store: Arc<dyn EvaluationStore>,
        dataset_manager: Arc<DatasetManager>,
        alerting_system: Arc<EvaluationAlerting>,
    ) -> Self {
        let metrics_calculator = Arc::new(MetricsCalculator::new(&config));
        let drift_detector = Arc::new(DriftDetector::new(&config.drift_detection_config));
        let smoke_tester = Arc::new(SmokeTester::new(&config.smoke_test_config));
        let regression_detector = Arc::new(RegressionDetector::new(&config));

        Self {
            config,
            metrics_calculator,
            drift_detector,
            smoke_tester,
            regression_detector,
            evaluation_store,
            dataset_manager,
            alerting_system,
        }
    }

    pub async fn evaluate_before_promotion(
        &self,
        collection_id: &str,
        candidate_version: u32,
    ) -> Result<EvaluationResult, EvaluationError> {
        let mut evaluation_steps = Vec::new();

        // Step 1: Run smoke tests
        if self.config.smoke_tests_enabled {
            let smoke_result = self.run_smoke_tests(collection_id, candidate_version).await?;
            evaluation_steps.push(EvaluationStep {
                name: "smoke_tests".to_string(),
                result: smoke_result.clone(),
                duration_ms: smoke_result.duration_ms,
                passed: smoke_result.passed,
            });

            if !smoke_result.passed {
                return Ok(EvaluationResult {
                    collection_id: collection_id.to_string(),
                    version: candidate_version,
                    evaluation_type: EvaluationType::PrePromotion,
                    overall_passed: false,
                    steps: evaluation_steps,
                    quality_metrics: None,
                    drift_analysis: None,
                    performance_analysis: None,
                    recommendations: vec![
                        "Smoke tests failed. Review basic functionality.".to_string()
                    ],
                    timestamp: Utc::now(),
                });
            }
        }

        // Step 2: Calculate quality metrics
        let quality_result = self.calculate_quality_metrics(collection_id, candidate_version).await?;
        evaluation_steps.push(EvaluationStep {
            name: "quality_metrics".to_string(),
            result: StepResult {
                passed: quality_result.meets_thresholds(&self.config.evaluation_thresholds),
                score: Some(quality_result.overall_score()),
                details: serde_json::json!(quality_result),
                error_message: None,
                duration_ms: 0, // Would be tracked in real implementation
            },
            duration_ms: 0,
            passed: quality_result.meets_thresholds(&self.config.evaluation_thresholds),
        });

        // Step 3: Check for drift
        let mut drift_analysis = None;
        if self.config.drift_detection_enabled {
            let drift_result = self.detect_drift(collection_id, candidate_version).await?;
            drift_analysis = Some(drift_result.clone());

            evaluation_steps.push(EvaluationStep {
                name: "drift_detection".to_string(),
                result: StepResult {
                    passed: !drift_result.significant_drift_detected,
                    score: Some(1.0 - drift_result.drift_severity),
                    details: serde_json::json!(drift_result),
                    error_message: None,
                    duration_ms: 0,
                },
                duration_ms: 0,
                passed: !drift_result.significant_drift_detected,
            });
        }

        // Step 4: Performance regression check
        let mut performance_analysis = None;
        if self.config.regression_detection_enabled {
            let performance_result = self.detect_performance_regression(collection_id, candidate_version).await?;
            performance_analysis = Some(performance_result.clone());

            evaluation_steps.push(EvaluationStep {
                name: "performance_regression".to_string(),
                result: StepResult {
                    passed: !performance_result.regression_detected,
                    score: Some(performance_result.performance_score),
                    details: serde_json::json!(performance_result),
                    error_message: None,
                    duration_ms: 0,
                },
                duration_ms: 0,
                passed: !performance_result.regression_detected,
            });
        }

        // Determine overall result
        let overall_passed = evaluation_steps.iter().all(|step| step.passed);

        // Generate recommendations
        let recommendations = self.generate_recommendations(&evaluation_steps, &quality_result).await;

        // Store evaluation result
        let result = EvaluationResult {
            collection_id: collection_id.to_string(),
            version: candidate_version,
            evaluation_type: EvaluationType::PrePromotion,
            overall_passed,
            steps: evaluation_steps,
            quality_metrics: Some(quality_result),
            drift_analysis,
            performance_analysis,
            recommendations,
            timestamp: Utc::now(),
        };

        self.evaluation_store.store_evaluation_result(&result).await?;

        // Send alerts if needed
        if !overall_passed {
            self.alerting_system.send_evaluation_alert(&result).await?;
        }

        Ok(result)
    }

    async fn run_smoke_tests(
        &self,
        collection_id: &str,
        version: u32,
    ) -> Result<StepResult, EvaluationError> {
        self.smoke_tester.run_tests(collection_id, version).await
    }

    async fn calculate_quality_metrics(
        &self,
        collection_id: &str,
        version: u32,
    ) -> Result<QualityMetrics, EvaluationError> {
        // Get evaluation dataset
        let dataset = self.dataset_manager.get_evaluation_dataset(collection_id).await?;

        // Run evaluation queries
        let mut all_results = Vec::new();
        for query_item in &dataset.queries {
            let search_results = self.execute_evaluation_query(
                collection_id,
                version,
                &query_item.query,
                query_item.expected_results.len(),
            ).await?;

            all_results.push(EvaluationQueryResult {
                query: query_item.query.clone(),
                expected_results: query_item.expected_results.clone(),
                actual_results: search_results,
                relevance_scores: query_item.relevance_scores.clone(),
            });
        }

        // Calculate metrics
        self.metrics_calculator.calculate_metrics(&all_results).await
    }

    async fn execute_evaluation_query(
        &self,
        collection_id: &str,
        version: u32,
        query: &str,
        top_k: usize,
    ) -> Result<Vec<EvaluationSearchResult>, EvaluationError> {
        // This would call the actual search system
        // For now, return placeholder
        Ok(vec![
            EvaluationSearchResult {
                document_id: "doc1".to_string(),
                chunk_id: "chunk1".to_string(),
                score: 0.9,
                rank: 1,
            },
            EvaluationSearchResult {
                document_id: "doc2".to_string(),
                chunk_id: "chunk2".to_string(),
                score: 0.8,
                rank: 2,
            },
        ])
    }

    async fn detect_drift(
        &self,
        collection_id: &str,
        candidate_version: u32,
    ) -> Result<DriftAnalysis, EvaluationError> {
        self.drift_detector.analyze_drift(collection_id, candidate_version).await
    }

    async fn detect_performance_regression(
        &self,
        collection_id: &str,
        candidate_version: u32,
    ) -> Result<PerformanceAnalysis, EvaluationError> {
        self.regression_detector.analyze_performance(collection_id, candidate_version).await
    }

    async fn generate_recommendations(
        &self,
        steps: &[EvaluationStep],
        quality_metrics: &QualityMetrics,
    ) -> Vec<String> {
        let mut recommendations = Vec::new();

        // Check failed steps
        for step in steps {
            if !step.passed {
                match step.name.as_str() {
                    "smoke_tests" => {
                        recommendations.push("Basic functionality tests failed. Check search pipeline integrity.".to_string());
                    }
                    "quality_metrics" => {
                        if quality_metrics.recall_at_5 < self.config.evaluation_thresholds.min_recall_at_5 {
                            recommendations.push(format!(
                                "Recall@5 ({:.3}) below threshold ({:.3}). Consider adjusting search parameters or retraining embeddings.",
                                quality_metrics.recall_at_5, self.config.evaluation_thresholds.min_recall_at_5
                            ));
                        }
                        if quality_metrics.precision_at_5 < self.config.evaluation_thresholds.min_precision_at_5 {
                            recommendations.push(format!(
                                "Precision@5 ({:.3}) below threshold ({:.3}). Review result ranking and filtering.",
                                quality_metrics.precision_at_5, self.config.evaluation_thresholds.min_precision_at_5
                            ));
                        }
                    }
                    "drift_detection" => {
                        recommendations.push("Significant drift detected. Review data quality and model consistency.".to_string());
                    }
                    "performance_regression" => {
                        recommendations.push("Performance regression detected. Check system resources and optimization settings.".to_string());
                    }
                    _ => {}
                }
            }
        }

        if recommendations.is_empty() {
            recommendations.push("All evaluation gates passed successfully.".to_string());
        }

        recommendations
    }

    pub async fn start_continuous_monitoring(&self) -> Result<(), EvaluationError> {
        if !self.config.continuous_monitoring_enabled {
            return Ok(());
        }

        // Start drift detection monitoring
        if self.config.drift_detection_enabled {
            let drift_detector = self.drift_detector.clone();
            let interval = Duration::minutes(self.config.drift_detection_config.drift_detection_interval_minutes as i64);

            tokio::spawn(async move {
                let mut ticker = tokio::time::interval(interval.to_std().unwrap());
                loop {
                    ticker.tick().await;
                    // Run drift detection for all active collections
                    if let Err(e) = drift_detector.run_continuous_monitoring().await {
                        tracing::error!("Continuous drift detection error: {}", e);
                    }
                }
            });
        }

        tracing::info!("Continuous evaluation monitoring started");
        Ok(())
    }
}

#[derive(Debug, Clone)]
pub struct MetricsCalculator {
    config: EvaluationConfig,
}

impl MetricsCalculator {
    pub fn new(config: &EvaluationConfig) -> Self {
        Self {
            config: config.clone(),
        }
    }

    pub async fn calculate_metrics(
        &self,
        query_results: &[EvaluationQueryResult],
    ) -> Result<QualityMetrics, EvaluationError> {
        let mut recall_at_1_sum = 0.0;
        let mut recall_at_5_sum = 0.0;
        let mut recall_at_10_sum = 0.0;
        let mut precision_at_5_sum = 0.0;
        let mut mrr_sum = 0.0;
        let mut ndcg_at_10_sum = 0.0;

        for query_result in query_results {
            // Calculate recall@k
            recall_at_1_sum += self.calculate_recall_at_k(&query_result, 1);
            recall_at_5_sum += self.calculate_recall_at_k(&query_result, 5);
            recall_at_10_sum += self.calculate_recall_at_k(&query_result, 10);

            // Calculate precision@k
            precision_at_5_sum += self.calculate_precision_at_k(&query_result, 5);

            // Calculate MRR
            mrr_sum += self.calculate_mrr(&query_result);

            // Calculate NDCG@k
            ndcg_at_10_sum += self.calculate_ndcg_at_k(&query_result, 10);
        }

        let num_queries = query_results.len() as f64;

        Ok(QualityMetrics {
            recall_at_1: recall_at_1_sum / num_queries,
            recall_at_5: recall_at_5_sum / num_queries,
            recall_at_10: recall_at_10_sum / num_queries,
            precision_at_5: precision_at_5_sum / num_queries,
            mrr: mrr_sum / num_queries,
            ndcg_at_10: ndcg_at_10_sum / num_queries,
            query_count: query_results.len(),
            evaluation_timestamp: Utc::now(),
        })
    }

    fn calculate_recall_at_k(&self, query_result: &EvaluationQueryResult, k: usize) -> f64 {
        if query_result.expected_results.is_empty() {
            return 0.0;
        }

        let top_k_results: HashSet<_> = query_result.actual_results
            .iter()
            .take(k)
            .map(|r| &r.document_id)
            .collect();

        let expected_set: HashSet<_> = query_result.expected_results.iter().collect();
        let relevant_retrieved = top_k_results.intersection(&expected_set).count();

        relevant_retrieved as f64 / expected_set.len() as f64
    }

    fn calculate_precision_at_k(&self, query_result: &EvaluationQueryResult, k: usize) -> f64 {
        if query_result.actual_results.is_empty() {
            return 0.0;
        }

        let top_k_results: Vec<_> = query_result.actual_results
            .iter()
            .take(k)
            .collect();

        let expected_set: HashSet<_> = query_result.expected_results.iter().collect();
        let relevant_in_top_k = top_k_results
            .iter()
            .filter(|r| expected_set.contains(&r.document_id))
            .count();

        relevant_in_top_k as f64 / top_k_results.len() as f64
    }

    fn calculate_mrr(&self, query_result: &EvaluationQueryResult) -> f64 {
        let expected_set: HashSet<_> = query_result.expected_results.iter().collect();

        for (index, result) in query_result.actual_results.iter().enumerate() {
            if expected_set.contains(&result.document_id) {
                return 1.0 / (index + 1) as f64;
            }
        }

        0.0
    }

    fn calculate_ndcg_at_k(&self, query_result: &EvaluationQueryResult, k: usize) -> f64 {
        // Simplified NDCG calculation
        let relevance_map: HashMap<_, _> = query_result.expected_results
            .iter()
            .zip(query_result.relevance_scores.iter())
            .map(|(doc_id, score)| (doc_id, *score))
            .collect();

        let dcg = query_result.actual_results
            .iter()
            .take(k)
            .enumerate()
            .map(|(index, result)| {
                let relevance = relevance_map.get(&result.document_id).unwrap_or(&0.0);
                let discount = (index + 2) as f64;
                relevance / discount.log2()
            })
            .sum::<f64>();

        // Calculate ideal DCG
        let mut ideal_relevances: Vec<_> = query_result.relevance_scores.clone();
        ideal_relevances.sort_by(|a, b| b.partial_cmp(a).unwrap());

        let idcg = ideal_relevances
            .iter()
            .take(k)
            .enumerate()
            .map(|(index, relevance)| {
                let discount = (index + 2) as f64;
                relevance / discount.log2()
            })
            .sum::<f64>();

        if idcg == 0.0 { 0.0 } else { dcg / idcg }
    }
}

#[derive(Debug, Clone)]
pub struct DriftDetector {
    config: DriftDetectionConfig,
    historical_data: Arc<RwLock<VecDeque<DistributionSnapshot>>>,
}

impl DriftDetector {
    pub fn new(config: &DriftDetectionConfig) -> Self {
        Self {
            config: config.clone(),
            historical_data: Arc::new(RwLock::new(VecDeque::new())),
        }
    }

    pub async fn analyze_drift(
        &self,
        collection_id: &str,
        candidate_version: u32,
    ) -> Result<DriftAnalysis, EvaluationError> {
        // Get current and historical distributions
        let current_snapshot = self.capture_distribution_snapshot(collection_id, candidate_version).await?;

        let historical_snapshots = self.historical_data.read().await;
        if historical_snapshots.is_empty() {
            return Ok(DriftAnalysis {
                drift_detected: false,
                significant_drift_detected: false,
                drift_severity: 0.0,
                kl_divergence: None,
                wasserstein_distance: None,
                drift_components: Vec::new(),
                recommendations: vec![
                    "Insufficient historical data for drift detection.".to_string()
                ],
                timestamp: Utc::now(),
            });
        }

        // Calculate drift metrics
        let reference_snapshot = historical_snapshots.back().unwrap();
        let kl_div = self.calculate_kl_divergence(&current_snapshot, reference_snapshot)?;
        let wasserstein_dist = self.calculate_wasserstein_distance(&current_snapshot, reference_snapshot)?;

        // Determine drift significance
        let kl_significant = kl_div > self.config.kl_divergence_threshold;
        let wasserstein_significant = wasserstein_dist > self.config.wasserstein_distance_threshold;
        let significant_drift = kl_significant || wasserstein_significant;

        // Calculate overall drift severity
        let kl_severity = (kl_div / self.config.kl_divergence_threshold).min(1.0);
        let wasserstein_severity = (wasserstein_dist / self.config.wasserstein_distance_threshold).min(1.0);
        let drift_severity = kl_severity.max(wasserstein_severity);

        // Identify drift components
        let mut drift_components = Vec::new();
        if kl_significant {
            drift_components.push(DriftComponent {
                component_name: "embedding_distribution".to_string(),
                drift_type: DriftType::DistributionShift,
                severity: kl_severity,
                description: format!("KL divergence: {:.4}", kl_div),
            });
        }

        if wasserstein_significant {
            drift_components.push(DriftComponent {
                component_name: "feature_distribution".to_string(),
                drift_type: DriftType::CovariateShift,
                severity: wasserstein_severity,
                description: format!("Wasserstein distance: {:.4}", wasserstein_dist),
            });
        }

        // Generate recommendations
        let recommendations = self.generate_drift_recommendations(&drift_components);

        Ok(DriftAnalysis {
            drift_detected: drift_severity > 0.1,
            significant_drift_detected: significant_drift,
            drift_severity,
            kl_divergence: Some(kl_div),
            wasserstein_distance: Some(wasserstein_dist),
            drift_components,
            recommendations,
            timestamp: Utc::now(),
        })
    }

    async fn capture_distribution_snapshot(
        &self,
        _collection_id: &str,
        _version: u32,
    ) -> Result<DistributionSnapshot, EvaluationError> {
        // In real implementation, this would capture actual embedding distributions
        Ok(DistributionSnapshot {
            timestamp: Utc::now(),
            embedding_mean: vec![0.1, 0.2, 0.3], // Placeholder
            embedding_variance: vec![0.01, 0.02, 0.01], // Placeholder
            query_distribution: HashMap::new(),
            document_count: 1000,
            chunk_count: 10000,
        })
    }

    fn calculate_kl_divergence(
        &self,
        current: &DistributionSnapshot,
        reference: &DistributionSnapshot,
    ) -> Result<f64, EvaluationError> {
        // Simplified KL divergence calculation
        // In reality, this would use proper statistical methods
        let mut kl_div = 0.0;

        for (i, (p, q)) in current.embedding_mean.iter()
            .zip(reference.embedding_mean.iter())
            .enumerate() {
            if *q > 0.0 {
                kl_div += p * (p / q).ln();
            }
        }

        Ok(kl_div)
    }

    fn calculate_wasserstein_distance(
        &self,
        current: &DistributionSnapshot,
        reference: &DistributionSnapshot,
    ) -> Result<f64, EvaluationError> {
        // Simplified Wasserstein distance calculation
        let distance = current.embedding_mean.iter()
            .zip(reference.embedding_mean.iter())
            .map(|(a, b)| (a - b).abs())
            .sum::<f64>();

        Ok(distance / current.embedding_mean.len() as f64)
    }

    fn generate_drift_recommendations(&self, components: &[DriftComponent]) -> Vec<String> {
        let mut recommendations = Vec::new();

        for component in components {
            match component.drift_type {
                DriftType::DistributionShift => {
                    recommendations.push("Consider retraining embeddings with recent data.".to_string());
                }
                DriftType::CovariateShift => {
                    recommendations.push("Review input data preprocessing and normalization.".to_string());
                }
                DriftType::ConceptDrift => {
                    recommendations.push("Update evaluation datasets and relevance judgments.".to_string());
                }
            }
        }

        if recommendations.is_empty() {
            recommendations.push("No significant drift detected.".to_string());
        }

        recommendations
    }

    pub async fn run_continuous_monitoring(&self) -> Result<(), EvaluationError> {
        // Implementation for continuous drift monitoring
        // This would be called periodically
        Ok(())
    }
}

// Additional supporting components would be implemented similarly...
#[derive(Debug, Clone)]
pub struct SmokeTester {
    config: SmokeTestConfig,
}

#[derive(Debug, Clone)]
pub struct RegressionDetector {
    config: EvaluationConfig,
}

#[derive(Debug, Clone)]
pub struct DatasetManager {
    // Implementation for managing evaluation datasets
}

#[derive(Debug, Clone)]
pub struct EvaluationAlerting {
    // Implementation for evaluation alerting
}

// Supporting data structures
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct QualityMetrics {
    pub recall_at_1: f64,
    pub recall_at_5: f64,
    pub recall_at_10: f64,
    pub precision_at_5: f64,
    pub mrr: f64,
    pub ndcg_at_10: f64,
    pub query_count: usize,
    pub evaluation_timestamp: DateTime<Utc>,
}

impl QualityMetrics {
    pub fn meets_thresholds(&self, thresholds: &EvaluationThresholds) -> bool {
        self.recall_at_1 >= thresholds.min_recall_at_1 &&
        self.recall_at_5 >= thresholds.min_recall_at_5 &&
        self.recall_at_10 >= thresholds.min_recall_at_10 &&
        self.precision_at_5 >= thresholds.min_precision_at_5 &&
        self.mrr >= thresholds.min_mrr &&
        self.ndcg_at_10 >= thresholds.min_ndcg_at_10
    }

    pub fn overall_score(&self) -> f64 {
        (self.recall_at_5 + self.precision_at_5 + self.mrr + self.ndcg_at_10) / 4.0
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EvaluationResult {
    pub collection_id: String,
    pub version: u32,
    pub evaluation_type: EvaluationType,
    pub overall_passed: bool,
    pub steps: Vec<EvaluationStep>,
    pub quality_metrics: Option<QualityMetrics>,
    pub drift_analysis: Option<DriftAnalysis>,
    pub performance_analysis: Option<PerformanceAnalysis>,
    pub recommendations: Vec<String>,
    pub timestamp: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum EvaluationType {
    PrePromotion,
    PostDeployment,
    Continuous,
    Manual,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EvaluationStep {
    pub name: String,
    pub result: StepResult,
    pub duration_ms: u64,
    pub passed: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StepResult {
    pub passed: bool,
    pub score: Option<f64>,
    pub details: serde_json::Value,
    pub error_message: Option<String>,
    pub duration_ms: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DriftAnalysis {
    pub drift_detected: bool,
    pub significant_drift_detected: bool,
    pub drift_severity: f64,
    pub kl_divergence: Option<f64>,
    pub wasserstein_distance: Option<f64>,
    pub drift_components: Vec<DriftComponent>,
    pub recommendations: Vec<String>,
    pub timestamp: DateTime<Utc>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DriftComponent {
    pub component_name: String,
    pub drift_type: DriftType,
    pub severity: f64,
    pub description: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum DriftType {
    DistributionShift,
    CovariateShift,
    ConceptDrift,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformanceAnalysis {
    pub regression_detected: bool,
    pub performance_score: f64,
    pub latency_regression_percent: f64,
    pub throughput_regression_percent: f64,
    pub memory_regression_percent: f64,
    pub recommendations: Vec<String>,
    pub timestamp: DateTime<Utc>,
}

#[derive(Debug, Clone)]
pub struct DistributionSnapshot {
    pub timestamp: DateTime<Utc>,
    pub embedding_mean: Vec<f64>,
    pub embedding_variance: Vec<f64>,
    pub query_distribution: HashMap<String, f64>,
    pub document_count: usize,
    pub chunk_count: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EvaluationQueryResult {
    pub query: String,
    pub expected_results: Vec<String>,
    pub actual_results: Vec<EvaluationSearchResult>,
    pub relevance_scores: Vec<f64>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EvaluationSearchResult {
    pub document_id: String,
    pub chunk_id: String,
    pub score: f64,
    pub rank: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EvaluationDataset {
    pub dataset_id: String,
    pub collection_id: String,
    pub version: u32,
    pub queries: Vec<EvaluationQuery>,
    pub created_at: DateTime<Utc>,
    pub metadata: HashMap<String, serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EvaluationQuery {
    pub query: String,
    pub expected_results: Vec<String>,
    pub relevance_scores: Vec<f64>,
    pub query_type: QueryType,
    pub difficulty: QueryDifficulty,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum QueryType {
    Factual,
    Conceptual,
    Procedural,
    Multi_hop,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum QueryDifficulty {
    Easy,
    Medium,
    Hard,
}

use std::collections::HashSet;

#[async_trait::async_trait]
pub trait EvaluationStore: Send + Sync {
    async fn store_evaluation_result(&self, result: &EvaluationResult) -> Result<(), EvaluationError>;
    async fn get_evaluation_result(&self, collection_id: &str, version: u32) -> Result<Option<EvaluationResult>, EvaluationError>;
    async fn get_evaluation_history(&self, collection_id: &str) -> Result<Vec<EvaluationResult>, EvaluationError>;
    async fn store_quality_metrics(&self, metrics: &QualityMetrics) -> Result<(), EvaluationError>;
    async fn get_quality_trends(&self, collection_id: &str) -> Result<Vec<QualityMetrics>, EvaluationError>;
}

#[derive(Debug, thiserror::Error)]
pub enum EvaluationError {
    #[error("Evaluation failed for collection {collection_id} version {version}: {reason}")]
    EvaluationFailed {
        collection_id: String,
        version: u32,
        reason: String,
    },

    #[error("Quality gate failure: {gate_name} - {reason}")]
    QualityGateFailure {
        gate_name: String,
        reason: String,
    },

    #[error("Drift detection error: {reason}")]
    DriftDetectionError { reason: String },

    #[error("Performance regression detected: {details}")]
    PerformanceRegression { details: String },

    #[error("Evaluation dataset not found: {dataset_id}")]
    DatasetNotFound { dataset_id: String },

    #[error("Smoke test failure: {test_name} - {reason}")]
    SmokeTestFailure {
        test_name: String,
        reason: String,
    },

    #[error("Storage error: {error}")]
    StorageError { error: String },

    #[error("Statistical analysis error: {error}")]
    StatisticalError { error: String },

    #[error("Configuration error: {error}")]
    ConfigurationError { error: String },
}
```

## Implementation Details

### Quality Metrics Framework
- Implement comprehensive evaluation metrics including recall@k, precision@k, MRR, and nDCG
- Support configurable evaluation thresholds with pass/fail criteria
- Provide statistical significance testing and confidence intervals
- Enable custom metrics for domain-specific evaluation requirements

### Drift Detection System
- Use statistical methods (KL divergence, Wasserstein distance) for drift detection
- Monitor embedding distributions, query patterns, and result quality over time
- Implement adaptive thresholds based on historical variance and seasonality
- Support different types of drift: concept, covariate, and distribution shift

### Evaluation Gates Pipeline
- Implement sequential evaluation steps with early termination on failures
- Support configurable evaluation profiles for different environments
- Provide detailed evaluation reports with actionable recommendations
- Enable manual override capabilities with proper authorization and audit trails

## Testing Strategy

### Metrics Tests
- Validate evaluation metric calculations against known ground truth datasets
- Test metric accuracy with synthetic data and edge cases
- Verify statistical significance testing and confidence interval calculations
- Test custom metric integration and configuration

### Drift Detection Tests
- Test drift detection algorithms with controlled distribution shifts
- Validate false positive and false negative rates
- Test continuous monitoring performance and resource usage
- Verify drift severity calculation and threshold behavior

### Integration Tests
- Test end-to-end evaluation pipeline from dataset loading to final results
- Verify integration with search systems and performance monitoring
- Test evaluation gate enforcement and rollback mechanisms
- Validate alerting and notification systems

## Acceptance Criteria

### AC-2.3.3.1 Quality Metrics and Evaluation
- [ ] Comprehensive evaluation metrics (recall@k, precision@k, MRR, nDCG) implemented
- [ ] Configurable quality thresholds with pass/fail criteria
- [ ] Statistical significance testing and confidence intervals
- [ ] Custom metrics support for domain-specific requirements

### AC-2.3.3.2 Drift Detection System
- [ ] Statistical drift detection using KL divergence and Wasserstein distance
- [ ] Continuous monitoring with <1% system resource overhead
- [ ] <5% false positive rate for stable systems
- [ ] Adaptive thresholds based on historical patterns

### AC-2.3.3.3 Evaluation Gates Pipeline
- [ ] Sequential evaluation steps with early termination
- [ ] Quality evaluation completes within 5 minutes for 100K documents
- [ ] Automated rollback on quality gate failures
- [ ] Detailed evaluation reports with actionable recommendations

### AC-2.3.3.4 Performance and Reliability
- [ ] Smoke tests complete within 30 seconds
- [ ] Performance regression detection within 2 minutes
- [ ] 99.9% reliability in blocking defective releases
- [ ] Real-time quality monitoring and alerting system