# Storage Service Specification (Phase 1.1.3)

**Version:** 1.0
**Date:** September 15, 2025
**Status:** Implementation Ready
**Component:** Database & Storage Layer - Storage Service
**Priority:** CRITICAL

## Overview

This specification defines the Storage Service for RAG Studio's Phase 1.1.3 MVP implementation. The MVP focuses on essential local filesystem storage with basic quota management, simple cleanup functionality, and core ZIP pack operations. Advanced features like comprehensive auto-prune and detailed checksum validation are included with clear upgrade paths to production optimizations.

## Architecture

### Storage Service Architecture (MVP + Upgrade Path)

```mermaid
flowchart TB
  subgraph "Storage Service Architecture"
    direction TB

    subgraph "Storage Manager (MVP)"
      SM[StorageService<br/>Basic Quota Management]
      QM[Quota Monitor<br/>Simple Cleanup (MVP)]
      CS[Checksum Service<br/>Basic SHA-256 (MVP)]
      BP[Backup Processor<br/>Basic Full Backup (MVP)]
    end

    subgraph "Storage Layers (MVP)"
      direction LR
      FS[("Local FileSystem<br/>Primary Storage")]
      ZIP[("ZIP Packs<br/>Basic Archive (MVP)")]
      TEMP[("Temp Storage<br/>Simple Processing")]
    end

    subgraph "Storage Operations (MVP)"
      CRUD[CRUD Operations<br/>Basic Writes (MVP)]
      BACKUP[Backup Service<br/>Manual/Scheduled (MVP)]
      EXPORT[Export/Import<br/>Simple ZIP (MVP)]
      CLEAN[Cleanup Service<br/>Basic Age-based (MVP)]
    end

    SM --> FS
    SM --> ZIP
    SM --> TEMP
    QM --> FS
    CS --> FS
    BP --> ZIP

    CRUD --> SM
    BACKUP --> BP
    EXPORT --> ZIP
    CLEAN --> QM
  end
```

## Storage Configuration

### Configuration Schema

```rust
#[derive(Debug, Clone, serde::Deserialize)]
pub struct StorageConfig {
    // Base directories
    pub data_dir: PathBuf,
    pub temp_dir: PathBuf,
    pub backup_dir: PathBuf,
    pub cache_dir: PathBuf,

    // MVP: Simplified quota settings
    pub total_quota_gb: f64,           // Default: 2.0 GB (reduced for MVP)
    pub kb_quota_gb: f64,              // Default: 0.5 GB per KB (reduced for MVP)
    pub temp_quota_gb: f64,            // Default: 0.2 GB (reduced for MVP)
    pub backup_quota_gb: f64,          // Default: 1.0 GB (reduced for MVP)

    // MVP: Basic cleanup settings (upgrade to auto-prune)
    pub auto_prune_enabled: bool,      // Default: false for MVP
    pub simple_cleanup_enabled: bool,  // Default: true for MVP
    pub prune_threshold: f64,          // Default: 0.8 (80% usage for MVP)
    pub prune_target: f64,             // Default: 0.6 (60% usage for MVP)
    pub min_age_for_prune: Duration,   // Default: 7 days (longer for MVP)

    // MVP: Basic backup settings
    pub backup_enabled: bool,          // Default: false for MVP (manual)
    pub backup_interval: Duration,     // Default: 24 hours (less frequent)
    pub backup_retention_days: u32,    // Default: 7 days (shorter for MVP)
    pub incremental_backup: bool,      // Default: false for MVP (full backup only)

    // MVP: Conservative performance settings
    pub chunk_size: usize,             // Default: 32KB (smaller for MVP)
    pub compression_level: u32,        // Default: 3 (faster for MVP)
    pub max_concurrent_ops: usize,     // Default: 5 (reduced for MVP)
    pub checksum_validation: bool,     // Default: false for MVP (optional)
}

impl Default for StorageConfig {
    fn default() -> Self {
        // MVP: Conservative defaults
        Self {
            data_dir: PathBuf::from("data"),
            temp_dir: PathBuf::from("temp"),
            backup_dir: PathBuf::from("backups"),
            cache_dir: PathBuf::from("cache"),
            total_quota_gb: 2.0,                // Reduced for MVP
            kb_quota_gb: 0.5,                  // Reduced for MVP
            temp_quota_gb: 0.2,                // Reduced for MVP
            backup_quota_gb: 1.0,              // Reduced for MVP
            auto_prune_enabled: false,         // Disabled for MVP
            simple_cleanup_enabled: true,      // MVP alternative
            prune_threshold: 0.8,              // Higher threshold for MVP
            prune_target: 0.6,                 // Conservative target
            min_age_for_prune: Duration::from_secs(7 * 24 * 3600), // 7 days
            backup_enabled: false,             // Manual for MVP
            backup_interval: Duration::from_secs(24 * 3600), // Daily
            backup_retention_days: 7,          // Shorter for MVP
            incremental_backup: false,         // Full backup only for MVP
            chunk_size: 32 * 1024,             // Smaller chunks
            compression_level: 3,              // Faster compression
            max_concurrent_ops: 5,             // Reduced concurrency
            checksum_validation: false,        // Optional for MVP
        }
    }

    // Production configuration for upgrade path
    pub fn production() -> Self {
        Self {
            total_quota_gb: 5.0,
            kb_quota_gb: 1.0,
            temp_quota_gb: 0.5,
            backup_quota_gb: 2.0,
            auto_prune_enabled: true,
            simple_cleanup_enabled: false,
            prune_threshold: 0.9,
            prune_target: 0.7,
            min_age_for_prune: Duration::from_secs(24 * 3600),
            backup_enabled: true,
            backup_interval: Duration::from_secs(3600),
            backup_retention_days: 30,
            incremental_backup: true,
            chunk_size: 64 * 1024,
            compression_level: 6,
            max_concurrent_ops: 10,
            checksum_validation: true,
            ..Default::default()
        }
    }
}
```

### Service Implementation

```rust
use tokio::sync::{RwLock, Semaphore};
use tokio::fs;
use std::sync::Arc;
use sha2::{Sha256, Digest};
use zip::{ZipWriter, ZipReader, write::FileOptions};

pub struct StorageService {
    config: StorageConfig,
    quota_monitor: Arc<QuotaMonitor>,
    // MVP: Optional services based on configuration
    checksum_service: Option<Arc<ChecksumService>>, // None for MVP if disabled
    backup_service: Option<Arc<BackupService>>,     // None for MVP if disabled
    semaphore: Arc<Semaphore>,
    storage_stats: Arc<RwLock<StorageStats>>,
}

#[derive(Debug, Default)]
pub struct StorageStats {
    pub total_size: u64,
    pub kb_size: u64,
    pub temp_size: u64,
    pub backup_size: u64,
    pub file_count: u32,
    pub last_updated: SystemTime,
}

impl StorageService {
    pub async fn new(config: StorageConfig) -> Result<Self, StorageError> {
        // Create directories if they don't exist
        fs::create_dir_all(&config.data_dir).await?;
        fs::create_dir_all(&config.temp_dir).await?;
        fs::create_dir_all(&config.backup_dir).await?;
        fs::create_dir_all(&config.cache_dir).await?;

        let quota_monitor = Arc::new(QuotaMonitor::new(config.clone()));

        // MVP: Optional services based on configuration
        let checksum_service = if config.checksum_validation {
            Some(Arc::new(ChecksumService::new()))
        } else {
            None
        };

        let backup_service = if config.backup_enabled {
            Some(Arc::new(BackupService::new(config.clone())))
        } else {
            None
        };

        let semaphore = Arc::new(Semaphore::new(config.max_concurrent_ops));

        let service = Self {
            config,
            quota_monitor,
            checksum_service,
            backup_service,
            semaphore,
            storage_stats: Arc::new(RwLock::new(StorageStats::default())),
        };

        // Start background services
        service.start_background_services().await;

        Ok(service)
    }

    async fn start_background_services(&self) {
        let quota_monitor = Arc::clone(&self.quota_monitor);
        let backup_service = Arc::clone(&self.backup_service);

        // Start quota monitoring
        tokio::spawn(async move {
            quota_monitor.start_monitoring().await;
        });

        // Start backup service if enabled (MVP: manual backups)
        if let Some(backup_service) = &self.backup_service {
            if self.config.backup_enabled {
                let backup_service = Arc::clone(backup_service);
                tokio::spawn(async move {
                    backup_service.start_backup_scheduler().await;
                });
            }
        }
    }

    pub async fn store_file(&self, path: &Path, data: &[u8]) -> Result<FileMetadata, StorageError> {
        let _permit = self.semaphore.acquire().await?;

        // Check quota before storing
        self.quota_monitor.check_quota_for_write(data.len() as u64).await?;

        // MVP: Optional checksum calculation
        let checksum = if let Some(checksum_service) = &self.checksum_service {
            Some(checksum_service.calculate_checksum(data).await?)
        } else {
            None
        };

        // Atomic write operation
        let temp_path = self.get_temp_path(path);
        fs::write(&temp_path, data).await?;

        // Atomic move to final location
        fs::rename(&temp_path, path).await?;

        let metadata = FileMetadata {
            path: path.to_path_buf(),
            size: data.len() as u64,
            checksum,
            created_at: SystemTime::now(),
            modified_at: SystemTime::now(),
        };

        // Update storage stats
        self.update_stats_after_write(&metadata).await;

        tracing::debug!("Stored file: {} ({} bytes)", path.display(), data.len());
        Ok(metadata)
    }

    pub async fn load_file(&self, path: &Path) -> Result<Vec<u8>, StorageError> {
        let _permit = self.semaphore.acquire().await?;

        if !path.exists() {
            return Err(StorageError::FileNotFound(path.to_path_buf()));
        }

        let data = fs::read(path).await?;

        // MVP: Optional checksum verification
        if let Some(checksum_service) = &self.checksum_service {
            self.verify_file_integrity_with_service(path, &data, checksum_service).await?;
        }

        tracing::debug!("Loaded file: {} ({} bytes)", path.display(), data.len());
        Ok(data)
    }

    pub async fn delete_file(&self, path: &Path) -> Result<(), StorageError> {
        let _permit = self.semaphore.acquire().await?;

        if !path.exists() {
            return Err(StorageError::FileNotFound(path.to_path_buf()));
        }

        let metadata = fs::metadata(path).await?;
        fs::remove_file(path).await?;

        // Update storage stats
        self.update_stats_after_delete(metadata.len()).await;

        tracing::debug!("Deleted file: {}", path.display());
        Ok(())
    }

    pub async fn create_zip_pack(&self, kb_id: &str, files: Vec<PackFile>) -> Result<ZipPackMetadata, StorageError> {
        let _permit = self.semaphore.acquire().await?;

        let zip_path = self.get_zip_pack_path(kb_id);
        let temp_zip_path = self.get_temp_path(&zip_path);

        // Create ZIP pack
        let file = fs::File::create(&temp_zip_path).await?;
        let mut zip_writer = ZipWriter::new(file);

        let options = FileOptions::default()
            .compression_method(zip::CompressionMethod::Deflated)
            .compression_level(Some(self.config.compression_level as i64));

        let mut manifest = ZipManifest {
            kb_id: kb_id.to_string(),
            created_at: SystemTime::now(),
            files: Vec::new(),
            total_size: 0,
            checksum: String::new(),
        };

        for pack_file in files {
            zip_writer.start_file(&pack_file.relative_path, options)?;

            let file_data = fs::read(&pack_file.source_path).await?;
            zip_writer.write_all(&file_data)?;

            // MVP: Optional checksum calculation
            let file_checksum = if let Some(checksum_service) = &self.checksum_service {
                checksum_service.calculate_checksum(&file_data).await?
            } else {
                format!("{:x}", std::collections::hash_map::DefaultHasher::new().finish()) // Simple hash for MVP
            };

            manifest.files.push(PackFileInfo {
                relative_path: pack_file.relative_path.clone(),
                size: file_data.len() as u64,
                checksum: file_checksum,
                modified_at: pack_file.modified_at,
            });

            manifest.total_size += file_data.len() as u64;
        }

        // Add manifest to ZIP
        let manifest_json = serde_json::to_string_pretty(&manifest)?;
        zip_writer.start_file("manifest.json", options)?;
        zip_writer.write_all(manifest_json.as_bytes())?;

        zip_writer.finish()?;

        // Calculate pack checksum
        let zip_data = fs::read(&temp_zip_path).await?;
        manifest.checksum = self.checksum_service.calculate_checksum(&zip_data).await?;

        // Atomic move to final location
        fs::rename(&temp_zip_path, &zip_path).await?;

        let pack_metadata = ZipPackMetadata {
            path: zip_path,
            manifest,
            created_at: SystemTime::now(),
        };

        tracing::info!("Created ZIP pack for KB: {} ({} files, {} bytes)",
                      kb_id, pack_metadata.manifest.files.len(), pack_metadata.manifest.total_size);

        Ok(pack_metadata)
    }

    pub async fn extract_zip_pack(&self, zip_path: &Path, extract_to: &Path) -> Result<ZipManifest, StorageError> {
        let _permit = self.semaphore.acquire().await?;

        let file = fs::File::open(zip_path).await?;
        let mut zip_reader = ZipReader::new(file)?;

        // First, extract and validate manifest
        let manifest = self.extract_and_validate_manifest(&mut zip_reader).await?;

        // Create extraction directory
        fs::create_dir_all(extract_to).await?;

        // Extract all files
        for file_info in &manifest.files {
            let extract_path = extract_to.join(&file_info.relative_path);

            // Create parent directories
            if let Some(parent) = extract_path.parent() {
                fs::create_dir_all(parent).await?;
            }

            // Extract file
            let mut zip_file = zip_reader.by_name(&file_info.relative_path)?;
            let mut file_data = Vec::new();
            zip_file.read_to_end(&mut file_data)?;

            // MVP: Optional checksum verification
            if let Some(checksum_service) = &self.checksum_service {
                let calculated_checksum = checksum_service.calculate_checksum(&file_data).await?;
                if calculated_checksum != file_info.checksum {
                    return Err(StorageError::ChecksumMismatch {
                        expected: file_info.checksum.clone(),
                        calculated: calculated_checksum,
                        file_path: extract_path,
                    });
                }
            } else {
                // MVP: Skip checksum verification if service not available
                tracing::debug!("Skipping checksum verification for MVP mode");
            }

            // Write file
            fs::write(&extract_path, &file_data).await?;

            // Set modified time
            let modified_time = file_info.modified_at;
            // Note: Setting file times requires platform-specific code
        }

        tracing::info!("Extracted ZIP pack to: {} ({} files)",
                      extract_to.display(), manifest.files.len());

        Ok(manifest)
    }
}
```

## Quota Management

### Quota Monitor Implementation

```rust
pub struct QuotaMonitor {
    config: StorageConfig,
    usage_stats: Arc<RwLock<UsageStats>>,
}

#[derive(Debug, Default)]
struct UsageStats {
    total_usage: u64,
    kb_usage: u64,
    temp_usage: u64,
    backup_usage: u64,
    last_check: SystemTime,
}

impl QuotaMonitor {
    pub fn new(config: StorageConfig) -> Self {
        Self {
            config,
            usage_stats: Arc::new(RwLock::new(UsageStats::default())),
        }
    }

    pub async fn start_monitoring(self: Arc<Self>) {
        let mut interval = tokio::time::interval(Duration::from_secs(60)); // Check every minute

        loop {
            interval.tick().await;

            if let Err(e) = self.update_usage_stats().await {
                tracing::error!("Failed to update usage stats: {}", e);
            }

            // MVP: Simple cleanup instead of auto-prune
            if self.config.simple_cleanup_enabled {
                if let Err(e) = self.simple_cleanup().await {
                    tracing::error!("Simple cleanup failed: {}", e);
                }
            } else if self.config.auto_prune_enabled {
                // Production: Advanced auto-prune
                if let Err(e) = self.check_and_prune().await {
                    tracing::error!("Auto-prune failed: {}", e);
                }
            }
        }
    }

    pub async fn check_quota_for_write(&self, size: u64) -> Result<(), StorageError> {
        let stats = self.usage_stats.read().await;
        let total_quota_bytes = (self.config.total_quota_gb * 1_000_000_000.0) as u64;

        if stats.total_usage + size > total_quota_bytes {
            return Err(StorageError::QuotaExceeded {
                current: stats.total_usage,
                requested: size,
                limit: total_quota_bytes,
            });
        }

        Ok(())
    }

    async fn update_usage_stats(&self) -> Result<(), StorageError> {
        let data_usage = self.calculate_directory_size(&self.config.data_dir).await?;
        let temp_usage = self.calculate_directory_size(&self.config.temp_dir).await?;
        let backup_usage = self.calculate_directory_size(&self.config.backup_dir).await?;

        let mut stats = self.usage_stats.write().await;
        stats.total_usage = data_usage + temp_usage + backup_usage;
        stats.kb_usage = data_usage;
        stats.temp_usage = temp_usage;
        stats.backup_usage = backup_usage;
        stats.last_check = SystemTime::now();

        tracing::debug!("Updated usage stats: total={} MB, kb={} MB, temp={} MB, backup={} MB",
                       stats.total_usage / 1_000_000,
                       stats.kb_usage / 1_000_000,
                       stats.temp_usage / 1_000_000,
                       stats.backup_usage / 1_000_000);

        Ok(())
    }

    // MVP: Simple cleanup based on age and basic size limits
    async fn simple_cleanup(&self) -> Result<(), StorageError> {
        let stats = self.usage_stats.read().await;
        let total_quota_bytes = (self.config.total_quota_gb * 1_000_000_000.0) as u64;
        let usage_ratio = stats.total_usage as f64 / total_quota_bytes as f64;

        if usage_ratio >= self.config.prune_threshold {
            tracing::warn!("Storage usage at {:.1}%, triggering simple cleanup", usage_ratio * 100.0);

            drop(stats); // Release read lock

            // MVP: Simple cleanup - just remove old temp files
            let temp_freed = self.prune_temp_files().await?;
            tracing::info!("Simple cleanup freed {} bytes from temp files", temp_freed);
        }

        Ok(())
    }

    // Production: Advanced auto-prune (kept for upgrade path)
    async fn check_and_prune(&self) -> Result<(), StorageError> {
        let stats = self.usage_stats.read().await;
        let total_quota_bytes = (self.config.total_quota_gb * 1_000_000_000.0) as u64;
        let usage_ratio = stats.total_usage as f64 / total_quota_bytes as f64;

        if usage_ratio >= self.config.prune_threshold {
            tracing::warn!("Storage usage at {:.1}%, triggering auto-prune", usage_ratio * 100.0);

            drop(stats); // Release read lock
            self.prune_storage().await?;
        }

        Ok(())
    }

    async fn prune_storage(&self) -> Result<(), StorageError> {
        let target_bytes = (self.config.total_quota_gb * self.config.prune_target * 1_000_000_000.0) as u64;
        let current_stats = self.usage_stats.read().await;
        let bytes_to_free = current_stats.total_usage.saturating_sub(target_bytes);
        drop(current_stats);

        if bytes_to_free == 0 {
            return Ok(());
        }

        // Prune temporary files first
        let temp_freed = self.prune_temp_files().await?;

        // Prune old backups
        let backup_freed = self.prune_old_backups().await?;

        // If still need more space, prune old KB data
        let remaining_to_free = bytes_to_free.saturating_sub(temp_freed + backup_freed);
        if remaining_to_free > 0 {
            self.prune_old_kb_data(remaining_to_free).await?;
        }

        // Update stats after pruning
        self.update_usage_stats().await?;

        tracing::info!("Auto-prune completed: freed {} bytes", bytes_to_free);
        Ok(())
    }

    async fn prune_temp_files(&self) -> Result<u64, StorageError> {
        let min_age = SystemTime::now() - self.config.min_age_for_prune;
        let mut freed_bytes = 0u64;

        let mut entries = fs::read_dir(&self.config.temp_dir).await?;
        while let Some(entry) = entries.next_entry().await? {
            let metadata = entry.metadata().await?;

            if metadata.is_file() {
                if let Ok(created) = metadata.created() {
                    if created < min_age {
                        let size = metadata.len();
                        fs::remove_file(entry.path()).await?;
                        freed_bytes += size;
                        tracing::debug!("Pruned temp file: {}", entry.path().display());
                    }
                }
            }
        }

        Ok(freed_bytes)
    }

    async fn prune_old_backups(&self) -> Result<u64, StorageError> {
        let cutoff_time = SystemTime::now() - Duration::from_secs(self.config.backup_retention_days as u64 * 24 * 3600);
        let mut freed_bytes = 0u64;

        let mut entries = fs::read_dir(&self.config.backup_dir).await?;
        while let Some(entry) = entries.next_entry().await? {
            let metadata = entry.metadata().await?;

            if metadata.is_file() {
                if let Ok(created) = metadata.created() {
                    if created < cutoff_time {
                        let size = metadata.len();
                        fs::remove_file(entry.path()).await?;
                        freed_bytes += size;
                        tracing::debug!("Pruned old backup: {}", entry.path().display());
                    }
                }
            }
        }

        Ok(freed_bytes)
    }

    async fn calculate_directory_size(&self, path: &Path) -> Result<u64, StorageError> {
        if !path.exists() {
            return Ok(0);
        }

        let mut total_size = 0u64;
        let mut stack = vec![path.to_path_buf()];

        while let Some(current_path) = stack.pop() {
            let mut entries = fs::read_dir(&current_path).await?;

            while let Some(entry) = entries.next_entry().await? {
                let metadata = entry.metadata().await?;

                if metadata.is_dir() {
                    stack.push(entry.path());
                } else {
                    total_size += metadata.len();
                }
            }
        }

        Ok(total_size)
    }
}
```

## Checksum Service

### Integrity Validation

```rust
pub struct ChecksumService;

impl ChecksumService {
    pub fn new() -> Self {
        Self
    }

    pub async fn calculate_checksum(&self, data: &[u8]) -> Result<String, StorageError> {
        tokio::task::spawn_blocking(move || {
            let mut hasher = Sha256::new();
            hasher.update(data);
            Ok(format!("{:x}", hasher.finalize()))
        }).await?
    }

    pub async fn verify_file_checksum(&self, path: &Path, expected: &str) -> Result<bool, StorageError> {
        let data = fs::read(path).await?;
        let calculated = self.calculate_checksum(&data).await?;
        Ok(calculated == expected)
    }

    pub async fn calculate_file_checksum(&self, path: &Path) -> Result<String, StorageError> {
        let data = fs::read(path).await?;
        self.calculate_checksum(&data).await
    }
}
```

## Data Structures

### File and Pack Metadata

```rust
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct FileMetadata {
    pub path: PathBuf,
    pub size: u64,
    pub checksum: Option<String>,
    pub created_at: SystemTime,
    pub modified_at: SystemTime,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct PackFile {
    pub source_path: PathBuf,
    pub relative_path: String,
    pub modified_at: SystemTime,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct PackFileInfo {
    pub relative_path: String,
    pub size: u64,
    pub checksum: String,
    pub modified_at: SystemTime,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct ZipManifest {
    pub kb_id: String,
    pub created_at: SystemTime,
    pub files: Vec<PackFileInfo>,
    pub total_size: u64,
    pub checksum: String,
}

#[derive(Debug, Clone)]
pub struct ZipPackMetadata {
    pub path: PathBuf,
    pub manifest: ZipManifest,
    pub created_at: SystemTime,
}
```

## Backup Service

### Incremental Backup Implementation

```rust
pub struct BackupService {
    config: StorageConfig,
    last_backup: Arc<RwLock<Option<SystemTime>>>,
}

impl BackupService {
    pub fn new(config: StorageConfig) -> Self {
        Self {
            config,
            last_backup: Arc::new(RwLock::new(None)),
        }
    }

    pub async fn start_backup_scheduler(self: Arc<Self>) {
        let mut interval = tokio::time::interval(self.config.backup_interval);

        loop {
            interval.tick().await;

            if let Err(e) = self.perform_backup().await {
                tracing::error!("Backup failed: {}", e);
            }
        }
    }

    async fn perform_backup(&self) -> Result<(), StorageError> {
        // MVP: Always full backup for simplicity
        let backup_type = if self.config.incremental_backup {
            BackupType::Incremental // Production feature
        } else {
            BackupType::Full        // MVP default
        };

        let timestamp = Utc::now().format("%Y%m%d_%H%M%S");
        let backup_name = format!("backup_{}_{}", backup_type.as_str(), timestamp);
        let backup_path = self.config.backup_dir.join(&backup_name);

        match backup_type {
            BackupType::Full => {
                // MVP: Simple full backup
                self.perform_full_backup(&backup_path).await?
            },
            BackupType::Incremental => {
                // Production: Incremental backup (upgrade path)
                self.perform_incremental_backup(&backup_path).await?
            },
        }

        let mut last_backup = self.last_backup.write().await;
        *last_backup = Some(SystemTime::now());

        tracing::info!("Backup completed: {} (MVP mode: {})", backup_name, !self.config.incremental_backup);
        Ok(())
    }

    async fn perform_full_backup(&self, backup_path: &Path) -> Result<(), StorageError> {
        // Create full backup of data directory
        self.copy_directory_recursive(&self.config.data_dir, backup_path).await?;
        Ok(())
    }

    async fn perform_incremental_backup(&self, backup_path: &Path) -> Result<(), StorageError> {
        let last_backup_time = *self.last_backup.read().await;
        let since = last_backup_time.unwrap_or(UNIX_EPOCH);

        // Copy only files modified since last backup
        self.copy_modified_files(&self.config.data_dir, backup_path, since).await?;
        Ok(())
    }

    async fn copy_directory_recursive(&self, src: &Path, dest: &Path) -> Result<(), StorageError> {
        fs::create_dir_all(dest).await?;

        let mut entries = fs::read_dir(src).await?;
        while let Some(entry) = entries.next_entry().await? {
            let src_path = entry.path();
            let dest_path = dest.join(entry.file_name());

            if src_path.is_dir() {
                self.copy_directory_recursive(&src_path, &dest_path).await?;
            } else {
                fs::copy(&src_path, &dest_path).await?;
            }
        }

        Ok(())
    }

    async fn copy_modified_files(&self, src: &Path, dest: &Path, since: SystemTime) -> Result<(), StorageError> {
        fs::create_dir_all(dest).await?;

        let mut entries = fs::read_dir(src).await?;
        while let Some(entry) = entries.next_entry().await? {
            let src_path = entry.path();
            let metadata = entry.metadata().await?;

            if metadata.is_dir() {
                let dest_path = dest.join(entry.file_name());
                self.copy_modified_files(&src_path, &dest_path, since).await?;
            } else if let Ok(modified) = metadata.modified() {
                if modified > since {
                    let dest_path = dest.join(entry.file_name());
                    fs::copy(&src_path, &dest_path).await?;
                }
            }
        }

        Ok(())
    }
}

#[derive(Debug, Clone, Copy)]
enum BackupType {
    Full,
    Incremental,
}

impl BackupType {
    fn as_str(&self) -> &'static str {
        match self {
            BackupType::Full => "full",
            BackupType::Incremental => "incremental",
        }
    }
}
```

## Error Handling

### Error Types

```rust
#[derive(Debug, thiserror::Error)]
pub enum StorageError {
    #[error("File not found: {0}")]
    FileNotFound(PathBuf),

    #[error("Quota exceeded: current={current}, requested={requested}, limit={limit}")]
    QuotaExceeded {
        current: u64,
        requested: u64,
        limit: u64,
    },

    #[error("Checksum mismatch for {file_path:?}: expected {expected}, calculated {calculated}")]
    ChecksumMismatch {
        expected: String,
        calculated: String,
        file_path: PathBuf,
    },

    #[error("IO error: {0}")]
    IoError(#[from] tokio::io::Error),

    #[error("ZIP error: {0}")]
    ZipError(#[from] zip::result::ZipError),

    #[error("JSON serialization error: {0}")]
    JsonError(#[from] serde_json::Error),

    #[error("Task join error: {0}")]
    TaskError(#[from] tokio::task::JoinError),

    #[error("Semaphore acquisition error: {0}")]
    SemaphoreError(#[from] tokio::sync::AcquireError),

    #[error("Permission denied: {0}")]
    PermissionDenied(PathBuf),

    #[error("Backup error: {0}")]
    BackupError(String),

    #[error("Corruption detected: {0}")]
    CorruptionDetected(String),
}
```

## Testing Strategy

### Unit Tests

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;

    #[tokio::test]
    async fn test_file_operations() {
        let temp_dir = TempDir::new().unwrap();
        let config = StorageConfig {
            data_dir: temp_dir.path().to_path_buf(),
            ..Default::default()
        };

        let service = StorageService::new(config).await.unwrap();

        // Test store and load
        let test_data = b"Hello, World!";
        let test_path = temp_dir.path().join("test.txt");

        let metadata = service.store_file(&test_path, test_data).await.unwrap();
        assert_eq!(metadata.size, test_data.len() as u64);

        let loaded_data = service.load_file(&test_path).await.unwrap();
        assert_eq!(loaded_data, test_data);

        // Test delete
        service.delete_file(&test_path).await.unwrap();
        assert!(!test_path.exists());
    }

    #[tokio::test]
    async fn test_zip_pack_operations() {
        // Test ZIP pack creation and extraction
    }

    #[tokio::test]
    async fn test_quota_management() {
        // Test quota enforcement and auto-prune
    }

    #[tokio::test]
    async fn test_checksum_validation() {
        // Test checksum calculation and validation
    }

    #[tokio::test]
    async fn test_backup_operations() {
        // Test full and incremental backups
    }
}
```

## Success Criteria

### MVP Phase
- [ ] Local filesystem storage with basic quota management (1-2GB)
- [ ] Simple cleanup functionality based on file age
- [ ] Basic ZIP pack management for export/import
- [ ] Optional checksum validation (disabled by default)
- [ ] Basic write operations (atomic writes for upgrade path)
- [ ] Manual backup service (full backup only)
- [ ] Basic concurrent operation management (reduced limits)
- [ ] Essential error handling and recovery
- [ ] Basic storage statistics
- [ ] Simple directory operations

### Production Upgrade Path
- [ ] Advanced configurable quotas (1-5GB+)
- [ ] Intelligent auto-prune functionality with configurable thresholds
- [ ] Advanced ZIP pack management with manifests and integrity checks
- [ ] Comprehensive checksum validation for data integrity (SHA-256)
- [ ] Atomic write operations for full data consistency
- [ ] Automated backup service with full and incremental modes
- [ ] Advanced concurrent operation management with full semaphore control
- [ ] Comprehensive error handling and recovery mechanisms
- [ ] Detailed storage statistics and monitoring
- [ ] Optimized directory traversal and file management

## Dependencies

```toml
[dependencies]
tokio = { version = "1.0", features = ["full"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
sha2 = "0.10"
zip = "0.6"
thiserror = "1.0"
tracing = "0.1"
chrono = { version = "0.4", features = ["serde"] }
```

## Integration Points

### MVP Integration
- **SqlService**: Basic metadata storage for file tracking
- **VectorDbService**: Simple vector index file storage
- **StateManager**: Basic storage usage tracking via Arc<RwLock<AppState>>
- **Log Router**: Simple log file management
- **EmbeddingService**: Basic temporary file management for processing

### Production Upgrade Integration
- **SqlService**: Advanced metadata storage for file tracking and backup history
- **VectorDbService**: Advanced vector index file storage with atomic operations and backup
- **StateManager**: Actor-based storage usage tracking and health metrics
- **Log Router**: Advanced log file rotation and archival with retention policies
- **EmbeddingService**: Optimized temporary file management with cleanup automation