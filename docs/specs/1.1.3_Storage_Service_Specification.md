# Storage Service Specification (Phase 1.1.3)

**Version:** 1.0
**Date:** September 15, 2025
**Status:** Implementation Ready
**Component:** Database & Storage Layer - Storage Service
**Priority:** CRITICAL

## Overview

This specification defines the Storage Service for RAG Studio's Phase 1.1.3 implementation. The service manages local filesystem storage with configurable quotas (1-5GB), auto-prune functionality, ZIP pack management for knowledge bases, and comprehensive checksum validation for data integrity.

## Architecture

### Storage Service Architecture

```mermaid
flowchart TB
  subgraph "Storage Service Architecture"
    direction TB

    subgraph "Storage Manager"
      SM[StorageService<br/>Quota Management]
      QM[Quota Monitor<br/>Auto-Prune]
      CS[Checksum Service<br/>SHA-256]
      BP[Backup Processor<br/>Incremental]
    end

    subgraph "Storage Layers"
      direction LR
      FS[("Local FileSystem<br/>Primary Storage")]
      ZIP[("ZIP Packs<br/>Compressed Archive")]
      TEMP[("Temp Storage<br/>Processing")]
    end

    subgraph "Storage Operations"
      CRUD[CRUD Operations<br/>Atomic Writes]
      BACKUP[Backup Service<br/>Scheduled]
      EXPORT[Export/Import<br/>ZIP Manifest]
      CLEAN[Cleanup Service<br/>Orphan Detection]
    end

    SM --> FS
    SM --> ZIP
    SM --> TEMP
    QM --> FS
    CS --> FS
    BP --> ZIP

    CRUD --> SM
    BACKUP --> BP
    EXPORT --> ZIP
    CLEAN --> QM
  end
```

## Storage Configuration

### Configuration Schema

```rust
#[derive(Debug, Clone, serde::Deserialize)]
pub struct StorageConfig {
    // Base directories
    pub data_dir: PathBuf,
    pub temp_dir: PathBuf,
    pub backup_dir: PathBuf,
    pub cache_dir: PathBuf,

    // Quota settings
    pub total_quota_gb: f64,           // Default: 5.0 GB
    pub kb_quota_gb: f64,              // Default: 1.0 GB per KB
    pub temp_quota_gb: f64,            // Default: 0.5 GB
    pub backup_quota_gb: f64,          // Default: 2.0 GB

    // Auto-prune settings
    pub auto_prune_enabled: bool,      // Default: true
    pub prune_threshold: f64,          // Default: 0.9 (90% usage)
    pub prune_target: f64,             // Default: 0.7 (70% usage)
    pub min_age_for_prune: Duration,   // Default: 24 hours

    // Backup settings
    pub backup_enabled: bool,          // Default: true
    pub backup_interval: Duration,     // Default: 1 hour
    pub backup_retention_days: u32,    // Default: 30 days
    pub incremental_backup: bool,      // Default: true

    // Performance settings
    pub chunk_size: usize,             // Default: 64KB
    pub compression_level: u32,        // Default: 6 (balanced)
    pub max_concurrent_ops: usize,     // Default: 10
    pub checksum_validation: bool,     // Default: true
}

impl Default for StorageConfig {
    fn default() -> Self {
        Self {
            data_dir: PathBuf::from("data"),
            temp_dir: PathBuf::from("temp"),
            backup_dir: PathBuf::from("backups"),
            cache_dir: PathBuf::from("cache"),
            total_quota_gb: 5.0,
            kb_quota_gb: 1.0,
            temp_quota_gb: 0.5,
            backup_quota_gb: 2.0,
            auto_prune_enabled: true,
            prune_threshold: 0.9,
            prune_target: 0.7,
            min_age_for_prune: Duration::from_secs(24 * 3600),
            backup_enabled: true,
            backup_interval: Duration::from_secs(3600),
            backup_retention_days: 30,
            incremental_backup: true,
            chunk_size: 64 * 1024,
            compression_level: 6,
            max_concurrent_ops: 10,
            checksum_validation: true,
        }
    }
}
```

### Service Implementation

```rust
use tokio::sync::{RwLock, Semaphore};
use tokio::fs;
use std::sync::Arc;
use sha2::{Sha256, Digest};
use zip::{ZipWriter, ZipReader, write::FileOptions};

pub struct StorageService {
    config: StorageConfig,
    quota_monitor: Arc<QuotaMonitor>,
    checksum_service: Arc<ChecksumService>,
    backup_service: Arc<BackupService>,
    semaphore: Arc<Semaphore>,
    storage_stats: Arc<RwLock<StorageStats>>,
}

#[derive(Debug, Default)]
pub struct StorageStats {
    pub total_size: u64,
    pub kb_size: u64,
    pub temp_size: u64,
    pub backup_size: u64,
    pub file_count: u32,
    pub last_updated: SystemTime,
}

impl StorageService {
    pub async fn new(config: StorageConfig) -> Result<Self, StorageError> {
        // Create directories if they don't exist
        fs::create_dir_all(&config.data_dir).await?;
        fs::create_dir_all(&config.temp_dir).await?;
        fs::create_dir_all(&config.backup_dir).await?;
        fs::create_dir_all(&config.cache_dir).await?;

        let quota_monitor = Arc::new(QuotaMonitor::new(config.clone()));
        let checksum_service = Arc::new(ChecksumService::new());
        let backup_service = Arc::new(BackupService::new(config.clone()));
        let semaphore = Arc::new(Semaphore::new(config.max_concurrent_ops));

        let service = Self {
            config,
            quota_monitor,
            checksum_service,
            backup_service,
            semaphore,
            storage_stats: Arc::new(RwLock::new(StorageStats::default())),
        };

        // Start background services
        service.start_background_services().await;

        Ok(service)
    }

    async fn start_background_services(&self) {
        let quota_monitor = Arc::clone(&self.quota_monitor);
        let backup_service = Arc::clone(&self.backup_service);

        // Start quota monitoring
        tokio::spawn(async move {
            quota_monitor.start_monitoring().await;
        });

        // Start backup service if enabled
        if self.config.backup_enabled {
            tokio::spawn(async move {
                backup_service.start_backup_scheduler().await;
            });
        }
    }

    pub async fn store_file(&self, path: &Path, data: &[u8]) -> Result<FileMetadata, StorageError> {
        let _permit = self.semaphore.acquire().await?;

        // Check quota before storing
        self.quota_monitor.check_quota_for_write(data.len() as u64).await?;

        // Calculate checksum
        let checksum = if self.config.checksum_validation {
            Some(self.checksum_service.calculate_checksum(data).await?)
        } else {
            None
        };

        // Atomic write operation
        let temp_path = self.get_temp_path(path);
        fs::write(&temp_path, data).await?;

        // Atomic move to final location
        fs::rename(&temp_path, path).await?;

        let metadata = FileMetadata {
            path: path.to_path_buf(),
            size: data.len() as u64,
            checksum,
            created_at: SystemTime::now(),
            modified_at: SystemTime::now(),
        };

        // Update storage stats
        self.update_stats_after_write(&metadata).await;

        tracing::debug!("Stored file: {} ({} bytes)", path.display(), data.len());
        Ok(metadata)
    }

    pub async fn load_file(&self, path: &Path) -> Result<Vec<u8>, StorageError> {
        let _permit = self.semaphore.acquire().await?;

        if !path.exists() {
            return Err(StorageError::FileNotFound(path.to_path_buf()));
        }

        let data = fs::read(path).await?;

        // Verify checksum if validation is enabled
        if self.config.checksum_validation {
            self.verify_file_integrity(path, &data).await?;
        }

        tracing::debug!("Loaded file: {} ({} bytes)", path.display(), data.len());
        Ok(data)
    }

    pub async fn delete_file(&self, path: &Path) -> Result<(), StorageError> {
        let _permit = self.semaphore.acquire().await?;

        if !path.exists() {
            return Err(StorageError::FileNotFound(path.to_path_buf()));
        }

        let metadata = fs::metadata(path).await?;
        fs::remove_file(path).await?;

        // Update storage stats
        self.update_stats_after_delete(metadata.len()).await;

        tracing::debug!("Deleted file: {}", path.display());
        Ok(())
    }

    pub async fn create_zip_pack(&self, kb_id: &str, files: Vec<PackFile>) -> Result<ZipPackMetadata, StorageError> {
        let _permit = self.semaphore.acquire().await?;

        let zip_path = self.get_zip_pack_path(kb_id);
        let temp_zip_path = self.get_temp_path(&zip_path);

        // Create ZIP pack
        let file = fs::File::create(&temp_zip_path).await?;
        let mut zip_writer = ZipWriter::new(file);

        let options = FileOptions::default()
            .compression_method(zip::CompressionMethod::Deflated)
            .compression_level(Some(self.config.compression_level as i64));

        let mut manifest = ZipManifest {
            kb_id: kb_id.to_string(),
            created_at: SystemTime::now(),
            files: Vec::new(),
            total_size: 0,
            checksum: String::new(),
        };

        for pack_file in files {
            zip_writer.start_file(&pack_file.relative_path, options)?;

            let file_data = fs::read(&pack_file.source_path).await?;
            zip_writer.write_all(&file_data)?;

            let file_checksum = self.checksum_service.calculate_checksum(&file_data).await?;

            manifest.files.push(PackFileInfo {
                relative_path: pack_file.relative_path.clone(),
                size: file_data.len() as u64,
                checksum: file_checksum,
                modified_at: pack_file.modified_at,
            });

            manifest.total_size += file_data.len() as u64;
        }

        // Add manifest to ZIP
        let manifest_json = serde_json::to_string_pretty(&manifest)?;
        zip_writer.start_file("manifest.json", options)?;
        zip_writer.write_all(manifest_json.as_bytes())?;

        zip_writer.finish()?;

        // Calculate pack checksum
        let zip_data = fs::read(&temp_zip_path).await?;
        manifest.checksum = self.checksum_service.calculate_checksum(&zip_data).await?;

        // Atomic move to final location
        fs::rename(&temp_zip_path, &zip_path).await?;

        let pack_metadata = ZipPackMetadata {
            path: zip_path,
            manifest,
            created_at: SystemTime::now(),
        };

        tracing::info!("Created ZIP pack for KB: {} ({} files, {} bytes)",
                      kb_id, pack_metadata.manifest.files.len(), pack_metadata.manifest.total_size);

        Ok(pack_metadata)
    }

    pub async fn extract_zip_pack(&self, zip_path: &Path, extract_to: &Path) -> Result<ZipManifest, StorageError> {
        let _permit = self.semaphore.acquire().await?;

        let file = fs::File::open(zip_path).await?;
        let mut zip_reader = ZipReader::new(file)?;

        // First, extract and validate manifest
        let manifest = self.extract_and_validate_manifest(&mut zip_reader).await?;

        // Create extraction directory
        fs::create_dir_all(extract_to).await?;

        // Extract all files
        for file_info in &manifest.files {
            let extract_path = extract_to.join(&file_info.relative_path);

            // Create parent directories
            if let Some(parent) = extract_path.parent() {
                fs::create_dir_all(parent).await?;
            }

            // Extract file
            let mut zip_file = zip_reader.by_name(&file_info.relative_path)?;
            let mut file_data = Vec::new();
            zip_file.read_to_end(&mut file_data)?;

            // Verify checksum
            let calculated_checksum = self.checksum_service.calculate_checksum(&file_data).await?;
            if calculated_checksum != file_info.checksum {
                return Err(StorageError::ChecksumMismatch {
                    expected: file_info.checksum.clone(),
                    calculated: calculated_checksum,
                    file_path: extract_path,
                });
            }

            // Write file
            fs::write(&extract_path, &file_data).await?;

            // Set modified time
            let modified_time = file_info.modified_at;
            // Note: Setting file times requires platform-specific code
        }

        tracing::info!("Extracted ZIP pack to: {} ({} files)",
                      extract_to.display(), manifest.files.len());

        Ok(manifest)
    }
}
```

## Quota Management

### Quota Monitor Implementation

```rust
pub struct QuotaMonitor {
    config: StorageConfig,
    usage_stats: Arc<RwLock<UsageStats>>,
}

#[derive(Debug, Default)]
struct UsageStats {
    total_usage: u64,
    kb_usage: u64,
    temp_usage: u64,
    backup_usage: u64,
    last_check: SystemTime,
}

impl QuotaMonitor {
    pub fn new(config: StorageConfig) -> Self {
        Self {
            config,
            usage_stats: Arc::new(RwLock::new(UsageStats::default())),
        }
    }

    pub async fn start_monitoring(self: Arc<Self>) {
        let mut interval = tokio::time::interval(Duration::from_secs(60)); // Check every minute

        loop {
            interval.tick().await;

            if let Err(e) = self.update_usage_stats().await {
                tracing::error!("Failed to update usage stats: {}", e);
            }

            if self.config.auto_prune_enabled {
                if let Err(e) = self.check_and_prune().await {
                    tracing::error!("Auto-prune failed: {}", e);
                }
            }
        }
    }

    pub async fn check_quota_for_write(&self, size: u64) -> Result<(), StorageError> {
        let stats = self.usage_stats.read().await;
        let total_quota_bytes = (self.config.total_quota_gb * 1_000_000_000.0) as u64;

        if stats.total_usage + size > total_quota_bytes {
            return Err(StorageError::QuotaExceeded {
                current: stats.total_usage,
                requested: size,
                limit: total_quota_bytes,
            });
        }

        Ok(())
    }

    async fn update_usage_stats(&self) -> Result<(), StorageError> {
        let data_usage = self.calculate_directory_size(&self.config.data_dir).await?;
        let temp_usage = self.calculate_directory_size(&self.config.temp_dir).await?;
        let backup_usage = self.calculate_directory_size(&self.config.backup_dir).await?;

        let mut stats = self.usage_stats.write().await;
        stats.total_usage = data_usage + temp_usage + backup_usage;
        stats.kb_usage = data_usage;
        stats.temp_usage = temp_usage;
        stats.backup_usage = backup_usage;
        stats.last_check = SystemTime::now();

        tracing::debug!("Updated usage stats: total={} MB, kb={} MB, temp={} MB, backup={} MB",
                       stats.total_usage / 1_000_000,
                       stats.kb_usage / 1_000_000,
                       stats.temp_usage / 1_000_000,
                       stats.backup_usage / 1_000_000);

        Ok(())
    }

    async fn check_and_prune(&self) -> Result<(), StorageError> {
        let stats = self.usage_stats.read().await;
        let total_quota_bytes = (self.config.total_quota_gb * 1_000_000_000.0) as u64;
        let usage_ratio = stats.total_usage as f64 / total_quota_bytes as f64;

        if usage_ratio >= self.config.prune_threshold {
            tracing::warn!("Storage usage at {:.1}%, triggering auto-prune", usage_ratio * 100.0);

            drop(stats); // Release read lock
            self.prune_storage().await?;
        }

        Ok(())
    }

    async fn prune_storage(&self) -> Result<(), StorageError> {
        let target_bytes = (self.config.total_quota_gb * self.config.prune_target * 1_000_000_000.0) as u64;
        let current_stats = self.usage_stats.read().await;
        let bytes_to_free = current_stats.total_usage.saturating_sub(target_bytes);
        drop(current_stats);

        if bytes_to_free == 0 {
            return Ok(());
        }

        // Prune temporary files first
        let temp_freed = self.prune_temp_files().await?;

        // Prune old backups
        let backup_freed = self.prune_old_backups().await?;

        // If still need more space, prune old KB data
        let remaining_to_free = bytes_to_free.saturating_sub(temp_freed + backup_freed);
        if remaining_to_free > 0 {
            self.prune_old_kb_data(remaining_to_free).await?;
        }

        // Update stats after pruning
        self.update_usage_stats().await?;

        tracing::info!("Auto-prune completed: freed {} bytes", bytes_to_free);
        Ok(())
    }

    async fn prune_temp_files(&self) -> Result<u64, StorageError> {
        let min_age = SystemTime::now() - self.config.min_age_for_prune;
        let mut freed_bytes = 0u64;

        let mut entries = fs::read_dir(&self.config.temp_dir).await?;
        while let Some(entry) = entries.next_entry().await? {
            let metadata = entry.metadata().await?;

            if metadata.is_file() {
                if let Ok(created) = metadata.created() {
                    if created < min_age {
                        let size = metadata.len();
                        fs::remove_file(entry.path()).await?;
                        freed_bytes += size;
                        tracing::debug!("Pruned temp file: {}", entry.path().display());
                    }
                }
            }
        }

        Ok(freed_bytes)
    }

    async fn prune_old_backups(&self) -> Result<u64, StorageError> {
        let cutoff_time = SystemTime::now() - Duration::from_secs(self.config.backup_retention_days as u64 * 24 * 3600);
        let mut freed_bytes = 0u64;

        let mut entries = fs::read_dir(&self.config.backup_dir).await?;
        while let Some(entry) = entries.next_entry().await? {
            let metadata = entry.metadata().await?;

            if metadata.is_file() {
                if let Ok(created) = metadata.created() {
                    if created < cutoff_time {
                        let size = metadata.len();
                        fs::remove_file(entry.path()).await?;
                        freed_bytes += size;
                        tracing::debug!("Pruned old backup: {}", entry.path().display());
                    }
                }
            }
        }

        Ok(freed_bytes)
    }

    async fn calculate_directory_size(&self, path: &Path) -> Result<u64, StorageError> {
        if !path.exists() {
            return Ok(0);
        }

        let mut total_size = 0u64;
        let mut stack = vec![path.to_path_buf()];

        while let Some(current_path) = stack.pop() {
            let mut entries = fs::read_dir(&current_path).await?;

            while let Some(entry) = entries.next_entry().await? {
                let metadata = entry.metadata().await?;

                if metadata.is_dir() {
                    stack.push(entry.path());
                } else {
                    total_size += metadata.len();
                }
            }
        }

        Ok(total_size)
    }
}
```

## Checksum Service

### Integrity Validation

```rust
pub struct ChecksumService;

impl ChecksumService {
    pub fn new() -> Self {
        Self
    }

    pub async fn calculate_checksum(&self, data: &[u8]) -> Result<String, StorageError> {
        tokio::task::spawn_blocking(move || {
            let mut hasher = Sha256::new();
            hasher.update(data);
            Ok(format!("{:x}", hasher.finalize()))
        }).await?
    }

    pub async fn verify_file_checksum(&self, path: &Path, expected: &str) -> Result<bool, StorageError> {
        let data = fs::read(path).await?;
        let calculated = self.calculate_checksum(&data).await?;
        Ok(calculated == expected)
    }

    pub async fn calculate_file_checksum(&self, path: &Path) -> Result<String, StorageError> {
        let data = fs::read(path).await?;
        self.calculate_checksum(&data).await
    }
}
```

## Data Structures

### File and Pack Metadata

```rust
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct FileMetadata {
    pub path: PathBuf,
    pub size: u64,
    pub checksum: Option<String>,
    pub created_at: SystemTime,
    pub modified_at: SystemTime,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct PackFile {
    pub source_path: PathBuf,
    pub relative_path: String,
    pub modified_at: SystemTime,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct PackFileInfo {
    pub relative_path: String,
    pub size: u64,
    pub checksum: String,
    pub modified_at: SystemTime,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct ZipManifest {
    pub kb_id: String,
    pub created_at: SystemTime,
    pub files: Vec<PackFileInfo>,
    pub total_size: u64,
    pub checksum: String,
}

#[derive(Debug, Clone)]
pub struct ZipPackMetadata {
    pub path: PathBuf,
    pub manifest: ZipManifest,
    pub created_at: SystemTime,
}
```

## Backup Service

### Incremental Backup Implementation

```rust
pub struct BackupService {
    config: StorageConfig,
    last_backup: Arc<RwLock<Option<SystemTime>>>,
}

impl BackupService {
    pub fn new(config: StorageConfig) -> Self {
        Self {
            config,
            last_backup: Arc::new(RwLock::new(None)),
        }
    }

    pub async fn start_backup_scheduler(self: Arc<Self>) {
        let mut interval = tokio::time::interval(self.config.backup_interval);

        loop {
            interval.tick().await;

            if let Err(e) = self.perform_backup().await {
                tracing::error!("Backup failed: {}", e);
            }
        }
    }

    async fn perform_backup(&self) -> Result<(), StorageError> {
        let backup_type = if self.config.incremental_backup {
            BackupType::Incremental
        } else {
            BackupType::Full
        };

        let timestamp = Utc::now().format("%Y%m%d_%H%M%S");
        let backup_name = format!("backup_{}_{}", backup_type.as_str(), timestamp);
        let backup_path = self.config.backup_dir.join(&backup_name);

        match backup_type {
            BackupType::Full => self.perform_full_backup(&backup_path).await?,
            BackupType::Incremental => self.perform_incremental_backup(&backup_path).await?,
        }

        let mut last_backup = self.last_backup.write().await;
        *last_backup = Some(SystemTime::now());

        tracing::info!("Backup completed: {}", backup_name);
        Ok(())
    }

    async fn perform_full_backup(&self, backup_path: &Path) -> Result<(), StorageError> {
        // Create full backup of data directory
        self.copy_directory_recursive(&self.config.data_dir, backup_path).await?;
        Ok(())
    }

    async fn perform_incremental_backup(&self, backup_path: &Path) -> Result<(), StorageError> {
        let last_backup_time = *self.last_backup.read().await;
        let since = last_backup_time.unwrap_or(UNIX_EPOCH);

        // Copy only files modified since last backup
        self.copy_modified_files(&self.config.data_dir, backup_path, since).await?;
        Ok(())
    }

    async fn copy_directory_recursive(&self, src: &Path, dest: &Path) -> Result<(), StorageError> {
        fs::create_dir_all(dest).await?;

        let mut entries = fs::read_dir(src).await?;
        while let Some(entry) = entries.next_entry().await? {
            let src_path = entry.path();
            let dest_path = dest.join(entry.file_name());

            if src_path.is_dir() {
                self.copy_directory_recursive(&src_path, &dest_path).await?;
            } else {
                fs::copy(&src_path, &dest_path).await?;
            }
        }

        Ok(())
    }

    async fn copy_modified_files(&self, src: &Path, dest: &Path, since: SystemTime) -> Result<(), StorageError> {
        fs::create_dir_all(dest).await?;

        let mut entries = fs::read_dir(src).await?;
        while let Some(entry) = entries.next_entry().await? {
            let src_path = entry.path();
            let metadata = entry.metadata().await?;

            if metadata.is_dir() {
                let dest_path = dest.join(entry.file_name());
                self.copy_modified_files(&src_path, &dest_path, since).await?;
            } else if let Ok(modified) = metadata.modified() {
                if modified > since {
                    let dest_path = dest.join(entry.file_name());
                    fs::copy(&src_path, &dest_path).await?;
                }
            }
        }

        Ok(())
    }
}

#[derive(Debug, Clone, Copy)]
enum BackupType {
    Full,
    Incremental,
}

impl BackupType {
    fn as_str(&self) -> &'static str {
        match self {
            BackupType::Full => "full",
            BackupType::Incremental => "incremental",
        }
    }
}
```

## Error Handling

### Error Types

```rust
#[derive(Debug, thiserror::Error)]
pub enum StorageError {
    #[error("File not found: {0}")]
    FileNotFound(PathBuf),

    #[error("Quota exceeded: current={current}, requested={requested}, limit={limit}")]
    QuotaExceeded {
        current: u64,
        requested: u64,
        limit: u64,
    },

    #[error("Checksum mismatch for {file_path:?}: expected {expected}, calculated {calculated}")]
    ChecksumMismatch {
        expected: String,
        calculated: String,
        file_path: PathBuf,
    },

    #[error("IO error: {0}")]
    IoError(#[from] tokio::io::Error),

    #[error("ZIP error: {0}")]
    ZipError(#[from] zip::result::ZipError),

    #[error("JSON serialization error: {0}")]
    JsonError(#[from] serde_json::Error),

    #[error("Task join error: {0}")]
    TaskError(#[from] tokio::task::JoinError),

    #[error("Semaphore acquisition error: {0}")]
    SemaphoreError(#[from] tokio::sync::AcquireError),

    #[error("Permission denied: {0}")]
    PermissionDenied(PathBuf),

    #[error("Backup error: {0}")]
    BackupError(String),

    #[error("Corruption detected: {0}")]
    CorruptionDetected(String),
}
```

## Testing Strategy

### Unit Tests

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;

    #[tokio::test]
    async fn test_file_operations() {
        let temp_dir = TempDir::new().unwrap();
        let config = StorageConfig {
            data_dir: temp_dir.path().to_path_buf(),
            ..Default::default()
        };

        let service = StorageService::new(config).await.unwrap();

        // Test store and load
        let test_data = b"Hello, World!";
        let test_path = temp_dir.path().join("test.txt");

        let metadata = service.store_file(&test_path, test_data).await.unwrap();
        assert_eq!(metadata.size, test_data.len() as u64);

        let loaded_data = service.load_file(&test_path).await.unwrap();
        assert_eq!(loaded_data, test_data);

        // Test delete
        service.delete_file(&test_path).await.unwrap();
        assert!(!test_path.exists());
    }

    #[tokio::test]
    async fn test_zip_pack_operations() {
        // Test ZIP pack creation and extraction
    }

    #[tokio::test]
    async fn test_quota_management() {
        // Test quota enforcement and auto-prune
    }

    #[tokio::test]
    async fn test_checksum_validation() {
        // Test checksum calculation and validation
    }

    #[tokio::test]
    async fn test_backup_operations() {
        // Test full and incremental backups
    }
}
```

## Success Criteria

- [x] Local filesystem storage with configurable quotas (1-5GB)
- [x] Auto-prune functionality with configurable thresholds
- [x] ZIP pack management for knowledge base export/import
- [x] Checksum validation for data integrity (SHA-256)
- [x] Atomic write operations for data consistency
- [x] Backup service with full and incremental modes
- [x] Concurrent operation management with semaphores
- [x] Comprehensive error handling and recovery
- [x] Storage statistics and monitoring
- [x] Efficient directory traversal and file management

## Dependencies

```toml
[dependencies]
tokio = { version = "1.0", features = ["full"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
sha2 = "0.10"
zip = "0.6"
thiserror = "1.0"
tracing = "0.1"
chrono = { version = "0.4", features = ["serde"] }
```

## Integration Points

- **SqlService**: Metadata storage for file tracking and backup history
- **VectorDbService**: Vector index file storage and backup
- **StateManager**: Storage usage tracking and health metrics
- **Log Router**: Log file rotation and archival
- **EmbeddingService**: Temporary file management for processing