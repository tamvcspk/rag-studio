# Cache Service Specification (1.2.3)

**Version:** 1.0
**Date:** September 16, 2025
**Status:** Draft
**Phase:** 1.2 - State Management (Simplified MVP)

## Overview

This specification defines the Cache Service for RAG Studio's MVP implementation. The design uses simple memory caching with dashmap TTL for basic request-level caching, with clear upgrade path to layered caching architecture post-MVP.

## Architecture Decision

### MVP Approach: Simple Memory Caching
- **Pattern**: Single-layer memory cache using dashmap with TTL eviction
- **Rationale**: Simple to implement, provides immediate performance benefits, sufficient for MVP scale
- **Upgrade Path**: Clear migration to layered caching (Request/Feature/Document) with invalidation hooks
- **Trade-offs**: Less sophisticated than layered caching but much simpler to implement and debug

## Cache Service Interface

### Core Service Trait

```rust
use async_trait::async_trait;
use std::time::Duration;
use serde::{Serialize, Deserialize};

#[async_trait]
pub trait CacheService: Send + Sync {
    /// Get value from cache
    async fn get<T>(&self, key: &str) -> Result<Option<T>, CacheError>
    where
        T: for<'de> Deserialize<'de> + Send;

    /// Set value in cache with TTL
    async fn set<T>(&self, key: &str, value: &T, ttl: Option<Duration>) -> Result<(), CacheError>
    where
        T: Serialize + Send + Sync;

    /// Remove value from cache
    async fn remove(&self, key: &str) -> Result<(), CacheError>;

    /// Clear all cached values
    async fn clear(&self) -> Result<(), CacheError>;

    /// Get cache statistics
    async fn stats(&self) -> Result<CacheStats, CacheError>;

    /// Check if key exists in cache
    async fn exists(&self, key: &str) -> Result<bool, CacheError>;

    /// Set with condition (only if not exists)
    async fn set_nx<T>(&self, key: &str, value: &T, ttl: Option<Duration>) -> Result<bool, CacheError>
    where
        T: Serialize + Send + Sync;

    /// Batch get multiple keys
    async fn get_many<T>(&self, keys: &[String]) -> Result<Vec<(String, Option<T>)>, CacheError>
    where
        T: for<'de> Deserialize<'de> + Send;

    /// Batch set multiple keys
    async fn set_many<T>(&self, entries: &[(String, T)], ttl: Option<Duration>) -> Result<(), CacheError>
    where
        T: Serialize + Send + Sync;
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CacheStats {
    pub total_entries: usize,
    pub memory_usage_bytes: usize,
    pub hit_count: u64,
    pub miss_count: u64,
    pub eviction_count: u64,
    pub hit_rate: f64,
}

#[derive(Debug, thiserror::Error)]
pub enum CacheError {
    #[error("Serialization error: {0}")]
    SerializationError(#[from] serde_json::Error),

    #[error("Cache operation failed: {0}")]
    OperationFailed(String),

    #[error("Key not found: {0}")]
    KeyNotFound(String),

    #[error("Cache full: {0}")]
    CacheFull(String),
}
```

## Memory Cache Implementation

### DashMap-based Implementation

```rust
use dashmap::DashMap;
use std::time::{Duration, Instant, SystemTime, UNIX_EPOCH};
use std::sync::atomic::{AtomicU64, Ordering};
use tokio::sync::RwLock;
use tokio::time::{interval, sleep};

#[derive(Debug, Clone)]
struct CacheEntry {
    data: String, // JSON serialized data
    created_at: SystemTime,
    expires_at: Option<SystemTime>,
    last_accessed: SystemTime,
    access_count: u64,
}

impl CacheEntry {
    fn new(data: String, ttl: Option<Duration>) -> Self {
        let now = SystemTime::now();
        let expires_at = ttl.map(|duration| now + duration);

        Self {
            data,
            created_at: now,
            expires_at,
            last_accessed: now,
            access_count: 1,
        }
    }

    fn is_expired(&self) -> bool {
        if let Some(expires_at) = self.expires_at {
            SystemTime::now() > expires_at
        } else {
            false
        }
    }

    fn touch(&mut self) {
        self.last_accessed = SystemTime::now();
        self.access_count += 1;
    }

    fn age(&self) -> Duration {
        SystemTime::now()
            .duration_since(self.created_at)
            .unwrap_or_default()
    }

    fn size(&self) -> usize {
        std::mem::size_of::<Self>() + self.data.len()
    }
}

pub struct MemoryCacheService {
    cache: DashMap<String, CacheEntry>,
    config: CacheConfig,
    stats: CacheServiceStats,
    cleanup_task: Option<tokio::task::JoinHandle<()>>,
}

#[derive(Debug, Clone)]
pub struct CacheConfig {
    pub max_entries: usize,
    pub max_memory_bytes: usize,
    pub default_ttl: Duration,
    pub cleanup_interval: Duration,
    pub eviction_policy: EvictionPolicy,
}

#[derive(Debug, Clone)]
pub enum EvictionPolicy {
    LeastRecentlyUsed,
    LeastFrequentlyUsed,
    TimeToLive,
    FirstInFirstOut,
}

impl Default for CacheConfig {
    fn default() -> Self {
        Self {
            max_entries: 10000,
            max_memory_bytes: 100 * 1024 * 1024, // 100MB
            default_ttl: Duration::from_secs(3600), // 1 hour
            cleanup_interval: Duration::from_secs(60), // 1 minute
            eviction_policy: EvictionPolicy::LeastRecentlyUsed,
        }
    }
}

#[derive(Debug, Default)]
struct CacheServiceStats {
    hit_count: AtomicU64,
    miss_count: AtomicU64,
    eviction_count: AtomicU64,
    memory_usage: AtomicU64,
}

impl MemoryCacheService {
    pub fn new(config: CacheConfig) -> Self {
        let service = Self {
            cache: DashMap::new(),
            config,
            stats: CacheServiceStats::default(),
            cleanup_task: None,
        };

        service
    }

    pub async fn start_cleanup_task(&mut self) {
        let cache = self.cache.clone();
        let config = self.config.clone();
        let stats = self.stats.clone();
        let cleanup_interval = config.cleanup_interval;

        let handle = tokio::spawn(async move {
            let mut interval = interval(cleanup_interval);

            loop {
                interval.tick().await;
                Self::cleanup_expired_entries(&cache, &stats).await;
                Self::enforce_memory_limits(&cache, &config, &stats).await;
            }
        });

        self.cleanup_task = Some(handle);
    }

    async fn cleanup_expired_entries(
        cache: &DashMap<String, CacheEntry>,
        stats: &CacheServiceStats,
    ) {
        let expired_keys: Vec<String> = cache
            .iter()
            .filter_map(|entry| {
                if entry.value().is_expired() {
                    Some(entry.key().clone())
                } else {
                    None
                }
            })
            .collect();

        for key in expired_keys {
            if let Some((_, entry)) = cache.remove(&key) {
                stats.eviction_count.fetch_add(1, Ordering::Relaxed);
                stats.memory_usage.fetch_sub(entry.size() as u64, Ordering::Relaxed);
            }
        }
    }

    async fn enforce_memory_limits(
        cache: &DashMap<String, CacheEntry>,
        config: &CacheConfig,
        stats: &CacheServiceStats,
    ) {
        let current_memory = stats.memory_usage.load(Ordering::Relaxed) as usize;
        let current_entries = cache.len();

        if current_memory <= config.max_memory_bytes && current_entries <= config.max_entries {
            return;
        }

        // Collect entries for eviction based on policy
        let mut entries_for_eviction: Vec<(String, CacheEntry)> = cache
            .iter()
            .map(|entry| (entry.key().clone(), entry.value().clone()))
            .collect();

        // Sort by eviction policy
        match config.eviction_policy {
            EvictionPolicy::LeastRecentlyUsed => {
                entries_for_eviction.sort_by(|a, b| a.1.last_accessed.cmp(&b.1.last_accessed));
            }
            EvictionPolicy::LeastFrequentlyUsed => {
                entries_for_eviction.sort_by(|a, b| a.1.access_count.cmp(&b.1.access_count));
            }
            EvictionPolicy::TimeToLive => {
                entries_for_eviction.sort_by(|a, b| a.1.created_at.cmp(&b.1.created_at));
            }
            EvictionPolicy::FirstInFirstOut => {
                entries_for_eviction.sort_by(|a, b| a.1.created_at.cmp(&b.1.created_at));
            }
        }

        // Evict entries until we're under limits
        let mut evicted_memory = 0usize;
        let mut evicted_count = 0usize;

        for (key, entry) in entries_for_eviction {
            if current_memory - evicted_memory <= config.max_memory_bytes
                && current_entries - evicted_count <= config.max_entries {
                break;
            }

            if let Some((_, removed_entry)) = cache.remove(&key) {
                evicted_memory += removed_entry.size();
                evicted_count += 1;
                stats.eviction_count.fetch_add(1, Ordering::Relaxed);
            }
        }

        stats.memory_usage.fetch_sub(evicted_memory as u64, Ordering::Relaxed);
    }

    fn calculate_memory_usage(&self) -> usize {
        self.cache
            .iter()
            .map(|entry| entry.value().size())
            .sum()
    }

    fn update_memory_usage(&self) {
        let current_usage = self.calculate_memory_usage();
        self.stats.memory_usage.store(current_usage as u64, Ordering::Relaxed);
    }
}

#[async_trait]
impl CacheService for MemoryCacheService {
    async fn get<T>(&self, key: &str) -> Result<Option<T>, CacheError>
    where
        T: for<'de> Deserialize<'de> + Send,
    {
        if let Some(mut entry) = self.cache.get_mut(key) {
            if entry.is_expired() {
                // Remove expired entry
                drop(entry);
                let (_, removed_entry) = self.cache.remove(key).unwrap();
                self.stats.memory_usage.fetch_sub(removed_entry.size() as u64, Ordering::Relaxed);
                self.stats.eviction_count.fetch_add(1, Ordering::Relaxed);
                self.stats.miss_count.fetch_add(1, Ordering::Relaxed);
                return Ok(None);
            }

            // Update access tracking
            entry.touch();
            self.stats.hit_count.fetch_add(1, Ordering::Relaxed);

            // Deserialize data
            let data: T = serde_json::from_str(&entry.data)?;
            Ok(Some(data))
        } else {
            self.stats.miss_count.fetch_add(1, Ordering::Relaxed);
            Ok(None)
        }
    }

    async fn set<T>(&self, key: &str, value: &T, ttl: Option<Duration>) -> Result<(), CacheError>
    where
        T: Serialize + Send + Sync,
    {
        let data = serde_json::to_string(value)?;
        let ttl = ttl.unwrap_or(self.config.default_ttl);
        let entry = CacheEntry::new(data, Some(ttl));
        let entry_size = entry.size();

        // Check memory limits before inserting
        let current_memory = self.stats.memory_usage.load(Ordering::Relaxed) as usize;
        if current_memory + entry_size > self.config.max_memory_bytes {
            return Err(CacheError::CacheFull(format!(
                "Adding entry would exceed memory limit: {} + {} > {}",
                current_memory, entry_size, self.config.max_memory_bytes
            )));
        }

        let old_entry = self.cache.insert(key.to_string(), entry);

        // Update memory usage
        if let Some(old_entry) = old_entry {
            self.stats.memory_usage.fetch_sub(old_entry.size() as u64, Ordering::Relaxed);
        }
        self.stats.memory_usage.fetch_add(entry_size as u64, Ordering::Relaxed);

        Ok(())
    }

    async fn remove(&self, key: &str) -> Result<(), CacheError> {
        if let Some((_, entry)) = self.cache.remove(key) {
            self.stats.memory_usage.fetch_sub(entry.size() as u64, Ordering::Relaxed);
        }
        Ok(())
    }

    async fn clear(&self) -> Result<(), CacheError> {
        self.cache.clear();
        self.stats.memory_usage.store(0, Ordering::Relaxed);
        Ok(())
    }

    async fn stats(&self) -> Result<CacheStats, CacheError> {
        let hit_count = self.stats.hit_count.load(Ordering::Relaxed);
        let miss_count = self.stats.miss_count.load(Ordering::Relaxed);
        let total_requests = hit_count + miss_count;
        let hit_rate = if total_requests > 0 {
            hit_count as f64 / total_requests as f64
        } else {
            0.0
        };

        Ok(CacheStats {
            total_entries: self.cache.len(),
            memory_usage_bytes: self.stats.memory_usage.load(Ordering::Relaxed) as usize,
            hit_count,
            miss_count,
            eviction_count: self.stats.eviction_count.load(Ordering::Relaxed),
            hit_rate,
        })
    }

    async fn exists(&self, key: &str) -> Result<bool, CacheError> {
        if let Some(entry) = self.cache.get(key) {
            if entry.is_expired() {
                // Clean up expired entry
                drop(entry);
                if let Some((_, removed_entry)) = self.cache.remove(key) {
                    self.stats.memory_usage.fetch_sub(removed_entry.size() as u64, Ordering::Relaxed);
                    self.stats.eviction_count.fetch_add(1, Ordering::Relaxed);
                }
                Ok(false)
            } else {
                Ok(true)
            }
        } else {
            Ok(false)
        }
    }

    async fn set_nx<T>(&self, key: &str, value: &T, ttl: Option<Duration>) -> Result<bool, CacheError>
    where
        T: Serialize + Send + Sync,
    {
        if self.exists(key).await? {
            Ok(false)
        } else {
            self.set(key, value, ttl).await?;
            Ok(true)
        }
    }

    async fn get_many<T>(&self, keys: &[String]) -> Result<Vec<(String, Option<T>)>, CacheError>
    where
        T: for<'de> Deserialize<'de> + Send,
    {
        let mut results = Vec::with_capacity(keys.len());

        for key in keys {
            let value = self.get::<T>(key).await?;
            results.push((key.clone(), value));
        }

        Ok(results)
    }

    async fn set_many<T>(&self, entries: &[(String, T)], ttl: Option<Duration>) -> Result<(), CacheError>
    where
        T: Serialize + Send + Sync,
    {
        for (key, value) in entries {
            self.set(key, value, ttl).await?;
        }
        Ok(())
    }
}
```

## Cache Key Strategies

### Key Naming Conventions

```rust
pub struct CacheKeys;

impl CacheKeys {
    /// KB search results cache
    pub fn kb_search(collection: &str, query_hash: &str) -> String {
        format!("kb:search:{}:{}", collection, query_hash)
    }

    /// Document embedding cache
    pub fn document_embedding(doc_id: &str, model: &str) -> String {
        format!("embedding:doc:{}:{}", doc_id, model)
    }

    /// Chunk embedding cache
    pub fn chunk_embedding(chunk_id: &str, model: &str) -> String {
        format!("embedding:chunk:{}:{}", chunk_id, model)
    }

    /// Reranking results cache
    pub fn rerank_results(query_hash: &str, candidates_hash: &str) -> String {
        format!("rerank:{}:{}", query_hash, candidates_hash)
    }

    /// KB metadata cache
    pub fn kb_metadata(kb_id: &str) -> String {
        format!("kb:meta:{}", kb_id)
    }

    /// Pipeline run cache
    pub fn pipeline_run(run_id: &str) -> String {
        format!("run:status:{}", run_id)
    }

    /// Tool schema cache
    pub fn tool_schema(tool_id: &str) -> String {
        format!("tool:schema:{}", tool_id)
    }

    /// Settings cache
    pub fn app_settings() -> String {
        "settings:app".to_string()
    }

    /// Performance metrics cache
    pub fn metrics(component: &str, timeframe: &str) -> String {
        format!("metrics:{}:{}", component, timeframe)
    }
}

pub struct CacheKeyUtils;

impl CacheKeyUtils {
    /// Generate hash for query content
    pub fn query_hash(query: &str) -> String {
        use sha2::{Sha256, Digest};
        let mut hasher = Sha256::new();
        hasher.update(query.as_bytes());
        format!("{:x}", hasher.finalize())[..16].to_string()
    }

    /// Generate hash for search candidates
    pub fn candidates_hash(candidate_ids: &[String]) -> String {
        use sha2::{Sha256, Digest};
        let mut hasher = Sha256::new();
        let mut ids = candidate_ids.to_vec();
        ids.sort(); // Ensure consistent order
        hasher.update(ids.join(":").as_bytes());
        format!("{:x}", hasher.finalize())[..16].to_string()
    }

    /// Generate TTL based on data type and confidence
    pub fn adaptive_ttl(data_type: &str, confidence: Option<f64>) -> Duration {
        match data_type {
            "search_results" => {
                // Higher confidence = longer cache
                let base_ttl = Duration::from_secs(300); // 5 minutes
                if let Some(conf) = confidence {
                    if conf > 0.8 {
                        base_ttl * 4 // 20 minutes for high confidence
                    } else if conf > 0.5 {
                        base_ttl * 2 // 10 minutes for medium confidence
                    } else {
                        base_ttl // 5 minutes for low confidence
                    }
                } else {
                    base_ttl
                }
            }
            "embeddings" => Duration::from_secs(86400), // 24 hours - embeddings are expensive
            "metadata" => Duration::from_secs(3600),    // 1 hour - metadata changes less frequently
            "metrics" => Duration::from_secs(60),       // 1 minute - metrics are dynamic
            _ => Duration::from_secs(300),              // Default 5 minutes
        }
    }
}
```

## Integration with Knowledge Base Module

### KB Module Cache Integration

```rust
pub struct KbModule {
    state_manager: AppStateManager,
    sql_service: Arc<dyn SqlService>,
    vector_service: Arc<dyn VectorDbService>,
    cache_service: Arc<dyn CacheService>,
    embedding_service: Arc<dyn EmbeddingService>,
}

impl KbModule {
    pub async fn hybrid_search(&self, query: SearchQuery) -> Result<SearchResults, KbError> {
        let query_hash = CacheKeyUtils::query_hash(&query.text);
        let cache_key = CacheKeys::kb_search(&query.collection, &query_hash);

        // Try cache first
        if let Some(cached_results) = self.cache_service.get::<SearchResults>(&cache_key).await? {
            return Ok(cached_results);
        }

        // Set loading state
        self.state_manager.set_loading("kb_search".to_string(), true)?;

        // Check embedding cache
        let embedding_key = CacheKeys::chunk_embedding(&query.text, "sentence-transformers");
        let query_embedding = if let Some(cached_embedding) = self.cache_service.get::<Vec<f32>>(&embedding_key).await? {
            cached_embedding
        } else {
            // Generate embedding
            let embedding = self.embedding_service.embed_query(&query.text).await?;

            // Cache embedding with long TTL
            let embedding_ttl = CacheKeyUtils::adaptive_ttl("embeddings", None);
            self.cache_service.set(&embedding_key, &embedding, Some(embedding_ttl)).await?;

            embedding
        };

        // Perform hybrid search
        let (vector_results, bm25_results) = tokio::try_join!(
            self.vector_search(&query_embedding, &query),
            self.bm25_search(&query)
        )?;

        // Merge and score results
        let merged_results = self.merge_search_results(vector_results, bm25_results)?;

        // Rerank if needed
        let final_results = if query.rerank && merged_results.hits.len() > 1 {
            // Check rerank cache
            let candidates_hash = CacheKeyUtils::candidates_hash(
                &merged_results.hits.iter().map(|h| h.chunk_id.clone()).collect::<Vec<_>>()
            );
            let rerank_key = CacheKeys::rerank_results(&query_hash, &candidates_hash);

            if let Some(cached_rerank) = self.cache_service.get::<SearchResults>(&rerank_key).await? {
                cached_rerank
            } else {
                let reranked = self.rerank_results(&query.text, merged_results).await?;

                // Cache rerank results
                let rerank_ttl = CacheKeyUtils::adaptive_ttl("search_results", Some(reranked.confidence));
                self.cache_service.set(&rerank_key, &reranked, Some(rerank_ttl)).await?;

                reranked
            }
        } else {
            merged_results
        };

        // Cache final results with adaptive TTL
        let results_ttl = CacheKeyUtils::adaptive_ttl("search_results", Some(final_results.confidence));
        self.cache_service.set(&cache_key, &final_results, Some(results_ttl)).await?;

        // Update metrics
        self.update_search_metrics(&final_results).await?;

        // Clear loading state
        self.state_manager.set_loading("kb_search".to_string(), false)?;

        Ok(final_results)
    }

    async fn update_search_metrics(&self, results: &SearchResults) -> Result<(), KbError> {
        // Cache hit rate for this search
        let hit_rate = if results.cache_hit { 1.0 } else { 0.0 };

        self.state_manager.update_metric(
            "search_cache_hit_rate".to_string(),
            hit_rate,
            HashMap::new(),
        )?;

        self.state_manager.update_metric(
            "search_latency_ms".to_string(),
            results.latency_ms,
            HashMap::new(),
        )?;

        Ok(())
    }
}
```

## Cache Invalidation Strategies

### Event-based Invalidation

```rust
pub struct CacheInvalidator {
    cache_service: Arc<dyn CacheService>,
    state_manager: AppStateManager,
    event_receiver: broadcast::Receiver<StateEvent>,
}

impl CacheInvalidator {
    pub fn new(
        cache_service: Arc<dyn CacheService>,
        state_manager: AppStateManager,
    ) -> Self {
        let event_receiver = state_manager.subscribe();

        Self {
            cache_service,
            state_manager,
            event_receiver,
        }
    }

    pub async fn start_invalidation_task(&mut self) {
        let cache_service = self.cache_service.clone();
        let mut event_receiver = self.event_receiver.resubscribe();

        tokio::spawn(async move {
            while let Ok(event) = event_receiver.recv().await {
                if let Err(e) = Self::handle_invalidation_event(&cache_service, &event).await {
                    eprintln!("Cache invalidation failed: {}", e);
                }
            }
        });
    }

    async fn handle_invalidation_event(
        cache_service: &Arc<dyn CacheService>,
        event: &StateEvent,
    ) -> Result<(), CacheError> {
        match event {
            StateEvent::KbUpdated(kb_id) | StateEvent::KbAdded(kb_id) => {
                // Invalidate all search results for this KB
                Self::invalidate_kb_caches(cache_service, kb_id).await?;
            }
            StateEvent::PipelineCompleted(pipeline_id) => {
                // Invalidate related KB caches if pipeline affects KB
                Self::invalidate_pipeline_caches(cache_service, pipeline_id).await?;
            }
            StateEvent::SettingsUpdated => {
                // Clear settings cache
                cache_service.remove(&CacheKeys::app_settings()).await?;
            }
            StateEvent::ToolUpdated(tool_id) => {
                // Clear tool schema cache
                cache_service.remove(&CacheKeys::tool_schema(tool_id)).await?;
            }
            _ => {} // Other events don't require cache invalidation
        }

        Ok(())
    }

    async fn invalidate_kb_caches(
        cache_service: &Arc<dyn CacheService>,
        kb_id: &str,
    ) -> Result<(), CacheError> {
        // This is simplified - in practice, we'd need a way to find all related cache keys
        // For MVP, we can clear the entire cache when KB changes occur
        cache_service.clear().await?;
        Ok(())
    }

    async fn invalidate_pipeline_caches(
        cache_service: &Arc<dyn CacheService>,
        _pipeline_id: &str,
    ) -> Result<(), CacheError> {
        // For MVP, clear search caches when pipelines complete
        // In layered caching, we'd be more selective
        cache_service.clear().await?;
        Ok(())
    }
}
```

## Configuration and Tuning

### Cache Configuration

```rust
#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct CacheServiceConfig {
    /// Maximum number of entries in cache
    pub max_entries: usize,

    /// Maximum memory usage in bytes
    pub max_memory_mb: usize,

    /// Default TTL for cached items
    pub default_ttl_seconds: u64,

    /// How often to run cleanup (remove expired items)
    pub cleanup_interval_seconds: u64,

    /// Eviction policy when cache is full
    pub eviction_policy: String,

    /// Enable/disable cache statistics collection
    pub collect_stats: bool,

    /// Adaptive TTL settings
    pub adaptive_ttl: AdaptiveTtlConfig,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct AdaptiveTtlConfig {
    /// TTL for search results based on confidence
    pub search_results: TtlByConfidence,

    /// TTL for embeddings (usually long)
    pub embeddings_seconds: u64,

    /// TTL for metadata
    pub metadata_seconds: u64,

    /// TTL for metrics
    pub metrics_seconds: u64,
}

#[derive(Debug, Clone, serde::Serialize, serde::Deserialize)]
pub struct TtlByConfidence {
    pub high_confidence_seconds: u64,    // confidence > 0.8
    pub medium_confidence_seconds: u64,  // confidence > 0.5
    pub low_confidence_seconds: u64,     // confidence <= 0.5
}

impl Default for CacheServiceConfig {
    fn default() -> Self {
        Self {
            max_entries: 10000,
            max_memory_mb: 100,
            default_ttl_seconds: 3600,
            cleanup_interval_seconds: 60,
            eviction_policy: "lru".to_string(),
            collect_stats: true,
            adaptive_ttl: AdaptiveTtlConfig {
                search_results: TtlByConfidence {
                    high_confidence_seconds: 1200, // 20 minutes
                    medium_confidence_seconds: 600, // 10 minutes
                    low_confidence_seconds: 300,   // 5 minutes
                },
                embeddings_seconds: 86400,  // 24 hours
                metadata_seconds: 3600,     // 1 hour
                metrics_seconds: 60,        // 1 minute
            },
        }
    }
}

impl CacheServiceConfig {
    pub fn to_cache_config(&self) -> CacheConfig {
        CacheConfig {
            max_entries: self.max_entries,
            max_memory_bytes: self.max_memory_mb * 1024 * 1024,
            default_ttl: Duration::from_secs(self.default_ttl_seconds),
            cleanup_interval: Duration::from_secs(self.cleanup_interval_seconds),
            eviction_policy: match self.eviction_policy.as_str() {
                "lru" => EvictionPolicy::LeastRecentlyUsed,
                "lfu" => EvictionPolicy::LeastFrequentlyUsed,
                "ttl" => EvictionPolicy::TimeToLive,
                "fifo" => EvictionPolicy::FirstInFirstOut,
                _ => EvictionPolicy::LeastRecentlyUsed,
            },
        }
    }

    pub fn from_toml(path: &str) -> Result<Self, std::io::Error> {
        let content = std::fs::read_to_string(path)?;
        toml::from_str(&content).map_err(|e| {
            std::io::Error::new(std::io::ErrorKind::InvalidData, e)
        })
    }
}
```

## Performance Monitoring

### Cache Performance Metrics

```rust
pub struct CacheMetrics {
    cache_service: Arc<dyn CacheService>,
    state_manager: AppStateManager,
    collection_interval: Duration,
}

impl CacheMetrics {
    pub fn new(
        cache_service: Arc<dyn CacheService>,
        state_manager: AppStateManager,
        collection_interval_seconds: u64,
    ) -> Self {
        Self {
            cache_service,
            state_manager,
            collection_interval: Duration::from_secs(collection_interval_seconds),
        }
    }

    pub async fn start_metrics_collection(&self) {
        let cache_service = self.cache_service.clone();
        let state_manager = self.state_manager.clone();
        let mut interval = tokio::time::interval(self.collection_interval);

        tokio::spawn(async move {
            loop {
                interval.tick().await;

                if let Ok(stats) = cache_service.stats().await {
                    // Update state manager with cache metrics
                    let metrics = vec![
                        ("cache_entries_total".to_string(), stats.total_entries as f64),
                        ("cache_memory_usage_bytes".to_string(), stats.memory_usage_bytes as f64),
                        ("cache_hit_rate".to_string(), stats.hit_rate),
                        ("cache_hits_total".to_string(), stats.hit_count as f64),
                        ("cache_misses_total".to_string(), stats.miss_count as f64),
                        ("cache_evictions_total".to_string(), stats.eviction_count as f64),
                    ];

                    for (name, value) in metrics {
                        let _ = state_manager.update_metric(name, value, HashMap::new());
                    }
                }
            }
        });
    }
}
```

## Testing Strategy

### Unit Tests

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use tokio::time::{sleep, Duration};

    async fn create_test_cache() -> MemoryCacheService {
        let config = CacheConfig {
            max_entries: 100,
            max_memory_bytes: 1024 * 1024, // 1MB
            default_ttl: Duration::from_secs(60),
            cleanup_interval: Duration::from_millis(100),
            eviction_policy: EvictionPolicy::LeastRecentlyUsed,
        };

        let mut cache = MemoryCacheService::new(config);
        cache.start_cleanup_task().await;
        cache
    }

    #[tokio::test]
    async fn test_basic_cache_operations() {
        let cache = create_test_cache().await;

        // Test set and get
        let value = "test_value";
        cache.set("test_key", &value, None).await.unwrap();

        let retrieved: Option<String> = cache.get("test_key").await.unwrap();
        assert_eq!(retrieved, Some(value.to_string()));

        // Test remove
        cache.remove("test_key").await.unwrap();
        let retrieved: Option<String> = cache.get("test_key").await.unwrap();
        assert_eq!(retrieved, None);
    }

    #[tokio::test]
    async fn test_ttl_expiration() {
        let cache = create_test_cache().await;

        // Set with short TTL
        let value = "test_value";
        let ttl = Duration::from_millis(50);
        cache.set("test_key", &value, Some(ttl)).await.unwrap();

        // Should be available immediately
        let retrieved: Option<String> = cache.get("test_key").await.unwrap();
        assert_eq!(retrieved, Some(value.to_string()));

        // Wait for expiration
        sleep(Duration::from_millis(100)).await;

        // Should be expired
        let retrieved: Option<String> = cache.get("test_key").await.unwrap();
        assert_eq!(retrieved, None);
    }

    #[tokio::test]
    async fn test_cache_stats() {
        let cache = create_test_cache().await;

        // Add some data
        cache.set("key1", &"value1", None).await.unwrap();
        cache.set("key2", &"value2", None).await.unwrap();

        // Generate hits and misses
        let _: Option<String> = cache.get("key1").await.unwrap(); // hit
        let _: Option<String> = cache.get("key1").await.unwrap(); // hit
        let _: Option<String> = cache.get("nonexistent").await.unwrap(); // miss

        let stats = cache.stats().await.unwrap();
        assert_eq!(stats.total_entries, 2);
        assert_eq!(stats.hit_count, 2);
        assert_eq!(stats.miss_count, 1);
        assert_eq!(stats.hit_rate, 2.0 / 3.0);
    }

    #[tokio::test]
    async fn test_memory_eviction() {
        let config = CacheConfig {
            max_entries: 2,
            max_memory_bytes: 1024,
            default_ttl: Duration::from_secs(60),
            cleanup_interval: Duration::from_millis(10),
            eviction_policy: EvictionPolicy::LeastRecentlyUsed,
        };

        let mut cache = MemoryCacheService::new(config);
        cache.start_cleanup_task().await;

        // Fill cache to capacity
        cache.set("key1", &"value1", None).await.unwrap();
        cache.set("key2", &"value2", None).await.unwrap();

        // Access key1 to make it more recently used
        let _: Option<String> = cache.get("key1").await.unwrap();

        // Add another item, should evict key2 (least recently used)
        cache.set("key3", &"value3", None).await.unwrap();

        // Allow cleanup task to run
        sleep(Duration::from_millis(50)).await;

        // key1 and key3 should exist, key2 should be evicted
        assert!(cache.exists("key1").await.unwrap());
        assert!(cache.exists("key3").await.unwrap());
        // Note: Due to async nature, key2 might still exist briefly
    }

    #[tokio::test]
    async fn test_batch_operations() {
        let cache = create_test_cache().await;

        // Test batch set
        let entries = vec![
            ("key1".to_string(), "value1".to_string()),
            ("key2".to_string(), "value2".to_string()),
            ("key3".to_string(), "value3".to_string()),
        ];

        cache.set_many(&entries, None).await.unwrap();

        // Test batch get
        let keys = vec!["key1".to_string(), "key2".to_string(), "key4".to_string()];
        let results: Vec<(String, Option<String>)> = cache.get_many(&keys).await.unwrap();

        assert_eq!(results.len(), 3);
        assert_eq!(results[0].1, Some("value1".to_string()));
        assert_eq!(results[1].1, Some("value2".to_string()));
        assert_eq!(results[2].1, None);
    }

    #[tokio::test]
    async fn test_set_nx() {
        let cache = create_test_cache().await;

        // First set should succeed
        let success = cache.set_nx("key1", &"value1", None).await.unwrap();
        assert!(success);

        // Second set of same key should fail
        let success = cache.set_nx("key1", &"value2", None).await.unwrap();
        assert!(!success);

        // Value should remain unchanged
        let retrieved: Option<String> = cache.get("key1").await.unwrap();
        assert_eq!(retrieved, Some("value1".to_string()));
    }
}

#[cfg(test)]
mod integration_tests {
    use super::*;

    #[tokio::test]
    async fn test_kb_cache_integration() {
        // This would test the KB module integration with caching
        // Including search result caching, embedding caching, etc.
    }

    #[tokio::test]
    async fn test_cache_invalidation() {
        // Test that cache invalidation works correctly with state events
    }

    #[tokio::test]
    async fn test_performance_under_load() {
        // Test cache performance under concurrent load
    }
}
```

## Error Handling and Recovery

### Cache Error Recovery

```rust
impl MemoryCacheService {
    /// Handle cache errors gracefully
    pub async fn handle_error(&self, error: &CacheError) -> Result<(), CacheError> {
        match error {
            CacheError::CacheFull(_) => {
                // Force eviction of some entries
                self.force_eviction(0.2).await?; // Evict 20% of entries
            }
            CacheError::SerializationError(_) => {
                // Remove problematic entries - this is a placeholder
                // In practice, we'd need to track which key caused the issue
            }
            _ => {} // Other errors don't require special handling
        }
        Ok(())
    }

    async fn force_eviction(&self, percentage: f64) -> Result<(), CacheError> {
        let total_entries = self.cache.len();
        let entries_to_evict = (total_entries as f64 * percentage) as usize;

        let entries: Vec<String> = self.cache
            .iter()
            .take(entries_to_evict)
            .map(|entry| entry.key().clone())
            .collect();

        for key in entries {
            self.remove(&key).await?;
        }

        Ok(())
    }
}
```

## Future Migration Path

### Layered Caching Upgrade
The current single-layer cache provides a foundation for layered caching:

1. **Request Layer**: Short-term cache for immediate requests (30s-5min TTL)
2. **Feature Layer**: Medium-term cache for feature data (5min-1hr TTL)
3. **Document Layer**: Long-term cache for document data (1hr-24hr TTL)

### Redis Integration
For production scaling, the interface supports Redis backend:

```rust
// Future Redis implementation
pub struct RedisCacheService {
    client: redis::Client,
    // ... implementation
}

#[async_trait]
impl CacheService for RedisCacheService {
    // Implement trait methods using Redis operations
}
```

## Implementation Priority

1. **Core Cache Service** - Implement basic memory caching with DashMap
2. **TTL and Cleanup** - Add expiration and cleanup mechanisms
3. **Integration Points** - Integrate with KB Module and other services
4. **Cache Keys Strategy** - Implement key naming and hashing utilities
5. **Performance Monitoring** - Add metrics collection and reporting
6. **Invalidation System** - Implement event-based cache invalidation
7. **Configuration** - Add configurable cache policies and limits
8. **Testing** - Comprehensive test coverage for all operations

## Success Criteria

- [ ] Cache service provides significant performance improvement for search operations
- [ ] Memory usage stays within configured limits (100MB default)
- [ ] Hit rate >80% for repeated search queries
- [ ] TTL expiration works correctly and prevents stale data
- [ ] Cache invalidation correctly handles state changes
- [ ] Performance metrics are accurately collected and reported
- [ ] Integration with KB Module is seamless and improves search latency
- [ ] >90% test coverage for cache operations and edge cases