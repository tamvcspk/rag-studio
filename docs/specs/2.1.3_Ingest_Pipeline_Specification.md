# 2.1.3 Ingest Pipeline Specification

**Phase:** 2.1 KB Data Model & API
**Component:** Ingest Pipeline
**Priority:** HIGH
**Dependencies:** 2.1.1 KB Schema, 1.3.1 Embedding Service, 1.2.1 AppState

## Overview

Implement the ETL ingest pipeline with fetch→parse→chunk→embed→index workflow, supporting parallel processing with Tokio, retry/backoff for failed operations, progress tracking, and reporting.

## Requirements

### Functional Requirements

#### FR-2.1.3.1 Document Ingestion Workflow
- Support fetch→parse→normalize→chunk→annotate→embed→index→eval→pack pipeline
- Handle multiple document formats: PDF, DOCX, TXT, MD, HTML, CSV, JSON
- Implement parallel processing with configurable concurrency limits
- Support batch processing for multiple documents simultaneously
- Provide resumable operations for interrupted processing

#### FR-2.1.3.2 Content Parsing and Normalization
- Extract text content preserving document structure (headers, tables, lists)
- Normalize encoding to UTF-8 with fallback detection
- Preserve metadata (title, author, creation date, license information)
- Support custom parsers for domain-specific formats
- Handle corrupted or malformed documents gracefully

#### FR-2.1.3.3 Text Chunking Strategy
- Implement semantic chunking with configurable boundaries (sentence, paragraph, section)
- Support configurable chunk sizes (256-1024 tokens) with overlap (10-50%)
- Preserve document structure context in chunk metadata
- Handle code blocks, tables, and structured content appropriately
- Maintain chunk-to-document relationships with citation anchors

#### FR-2.1.3.4 Embedding Generation
- Integrate with out-of-process Embedding Worker via JSON communication (MVP)
- Support batch processing for efficiency (32-64 embeddings per batch)
- Implement embedding caching to skip unchanged content
- Handle embedding failures with retry logic and fallback strategies
- Track embedding model version and configuration

#### FR-2.1.3.5 Index Construction
- Build vector indexes using LanceDB with optimized configuration
- Construct BM25 lexical indexes with configurable parameters
- Support parallel index building for vector and lexical components
- Implement atomic index promotion from staging to active
- Validate index integrity before promotion

#### FR-2.1.3.6 Progress Tracking and Reporting
- Provide real-time progress updates via Tauri events
- Track pipeline stage completion (parsing, chunking, embedding, indexing)
- Report processing statistics (documents/chunks processed, errors, timing)
- Support cancellation of long-running operations
- Log detailed operation metrics for performance analysis

### Non-Functional Requirements

#### NFR-2.1.3.1 Performance
- Process documents at minimum 10MB/minute with parallel execution
- Support concurrent processing of up to 100 documents
- Maintain memory usage under 200MB during processing
- Complete embedding operations within 30 seconds per batch

#### NFR-2.1.3.2 Reliability
- Implement comprehensive retry logic with exponential backoff
- Support graceful handling of out-of-memory conditions
- Ensure data consistency with atomic operations
- Provide detailed error reporting and recovery suggestions

#### NFR-2.1.3.3 Scalability
- Support documents up to 100MB with streaming processing
- Handle collections up to 10,000 documents efficiently
- Scale processing throughput with available CPU cores
- Optimize memory usage for large document sets

## Technical Specification

### Pipeline Architecture

```rust
use serde::{Deserialize, Serialize};
use tokio::sync::{mpsc, Semaphore};
use std::sync::Arc;
use chrono::{DateTime, Utc};

#[derive(Debug, Clone)]
pub struct IngestPipeline {
    config: PipelineConfig,
    embedding_service: Arc<dyn EmbeddingService>,
    vector_db: Arc<dyn VectorDbService>,
    sql_service: Arc<dyn SqlService>,
    progress_tx: mpsc::UnboundedSender<ProgressUpdate>,
    semaphore: Arc<Semaphore>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PipelineConfig {
    pub max_concurrent_documents: usize,
    pub max_concurrent_chunks: usize,
    pub chunk_size: usize,
    pub chunk_overlap: usize,
    pub batch_size: usize,
    pub retry_attempts: usize,
    pub retry_delay_ms: u64,
    pub max_document_size_mb: usize,
    pub supported_formats: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IngestRequest {
    pub collection_id: String,
    pub documents: Vec<DocumentSource>,
    pub options: IngestOptions,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DocumentSource {
    pub source_type: SourceType,
    pub path: String,
    pub metadata: Option<serde_json::Value>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum SourceType {
    LocalFile,
    Url,
    Memory(Vec<u8>),
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IngestOptions {
    pub force_reprocess: bool,
    pub skip_embedding_cache: bool,
    pub chunk_strategy: ChunkStrategy,
    pub embedding_model: Option<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ChunkStrategy {
    FixedSize { size: usize, overlap: usize },
    Semantic { min_size: usize, max_size: usize },
    Document, // Treat entire document as one chunk
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ProgressUpdate {
    pub job_id: String,
    pub stage: PipelineStage,
    pub completed: usize,
    pub total: usize,
    pub current_item: Option<String>,
    pub timestamp: DateTime<Utc>,
    pub errors: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum PipelineStage {
    Fetching,
    Parsing,
    Chunking,
    Embedding,
    Indexing,
    Completed,
    Failed,
}

impl IngestPipeline {
    pub fn new(
        config: PipelineConfig,
        embedding_service: Arc<dyn EmbeddingService>,
        vector_db: Arc<dyn VectorDbService>,
        sql_service: Arc<dyn SqlService>,
        progress_tx: mpsc::UnboundedSender<ProgressUpdate>,
    ) -> Self {
        let semaphore = Arc::new(Semaphore::new(config.max_concurrent_documents));

        Self {
            config,
            embedding_service,
            vector_db,
            sql_service,
            progress_tx,
            semaphore,
        }
    }

    pub async fn process_documents(
        &self,
        request: IngestRequest,
    ) -> Result<IngestResult, IngestError> {
        let job_id = uuid::Uuid::new_v4().to_string();
        let mut results = Vec::new();

        self.send_progress(ProgressUpdate {
            job_id: job_id.clone(),
            stage: PipelineStage::Fetching,
            completed: 0,
            total: request.documents.len(),
            current_item: None,
            timestamp: Utc::now(),
            errors: Vec::new(),
        }).await;

        // Process documents concurrently with semaphore limiting
        let tasks: Vec<_> = request.documents
            .into_iter()
            .enumerate()
            .map(|(index, doc_source)| {
                let pipeline = self.clone();
                let job_id = job_id.clone();
                let collection_id = request.collection_id.clone();
                let options = request.options.clone();

                tokio::spawn(async move {
                    let _permit = pipeline.semaphore.acquire().await.unwrap();
                    pipeline.process_single_document(
                        &job_id,
                        &collection_id,
                        index,
                        doc_source,
                        &options,
                    ).await
                })
            })
            .collect();

        // Collect results
        for (index, task) in tasks.into_iter().enumerate() {
            match task.await {
                Ok(Ok(result)) => results.push(result),
                Ok(Err(error)) => {
                    eprintln!("Document {} failed: {}", index, error);
                    results.push(DocumentResult {
                        source_path: format!("document_{}", index),
                        status: ProcessingStatus::Failed,
                        error: Some(error.to_string()),
                        chunks_created: 0,
                        processing_time_ms: 0,
                    });
                }
                Err(join_error) => {
                    eprintln!("Task {} panicked: {}", index, join_error);
                }
            }
        }

        self.send_progress(ProgressUpdate {
            job_id: job_id.clone(),
            stage: PipelineStage::Completed,
            completed: results.len(),
            total: results.len(),
            current_item: None,
            timestamp: Utc::now(),
            errors: results.iter()
                .filter_map(|r| r.error.as_ref())
                .cloned()
                .collect(),
        }).await;

        Ok(IngestResult {
            job_id,
            total_documents: results.len(),
            successful_documents: results.iter().filter(|r| r.status == ProcessingStatus::Success).count(),
            failed_documents: results.iter().filter(|r| r.status == ProcessingStatus::Failed).count(),
            total_chunks: results.iter().map(|r| r.chunks_created).sum(),
            processing_time_ms: results.iter().map(|r| r.processing_time_ms).sum(),
            results,
        })
    }

    async fn process_single_document(
        &self,
        job_id: &str,
        collection_id: &str,
        index: usize,
        doc_source: DocumentSource,
        options: &IngestOptions,
    ) -> Result<DocumentResult, IngestError> {
        let start_time = std::time::Instant::now();

        // Stage 1: Fetch document content
        self.send_progress(ProgressUpdate {
            job_id: job_id.to_string(),
            stage: PipelineStage::Fetching,
            completed: index,
            total: index + 1,
            current_item: Some(doc_source.path.clone()),
            timestamp: Utc::now(),
            errors: Vec::new(),
        }).await;

        let content = self.fetch_document(&doc_source).await?;

        // Stage 2: Parse document
        self.send_progress(ProgressUpdate {
            job_id: job_id.to_string(),
            stage: PipelineStage::Parsing,
            completed: index,
            total: index + 1,
            current_item: Some(doc_source.path.clone()),
            timestamp: Utc::now(),
            errors: Vec::new(),
        }).await;

        let parsed_doc = self.parse_document(&doc_source, content).await?;

        // Check for existing document (delta detection)
        if !options.force_reprocess {
            if let Some(existing) = self.check_existing_document(collection_id, &parsed_doc.fingerprint).await? {
                if existing.fingerprint == parsed_doc.fingerprint {
                    return Ok(DocumentResult {
                        source_path: doc_source.path,
                        status: ProcessingStatus::Skipped,
                        error: None,
                        chunks_created: existing.chunk_count as usize,
                        processing_time_ms: start_time.elapsed().as_millis() as u64,
                    });
                }
            }
        }

        // Stage 3: Chunk document
        self.send_progress(ProgressUpdate {
            job_id: job_id.to_string(),
            stage: PipelineStage::Chunking,
            completed: index,
            total: index + 1,
            current_item: Some(doc_source.path.clone()),
            timestamp: Utc::now(),
            errors: Vec::new(),
        }).await;

        let chunks = self.chunk_document(&parsed_doc, &options.chunk_strategy).await?;

        // Stage 4: Generate embeddings
        self.send_progress(ProgressUpdate {
            job_id: job_id.to_string(),
            stage: PipelineStage::Embedding,
            completed: index,
            total: index + 1,
            current_item: Some(doc_source.path.clone()),
            timestamp: Utc::now(),
            errors: Vec::new(),
        }).await;

        let embeddings = self.generate_embeddings(&chunks, options).await?;

        // Stage 5: Index content
        self.send_progress(ProgressUpdate {
            job_id: job_id.to_string(),
            stage: PipelineStage::Indexing,
            completed: index,
            total: index + 1,
            current_item: Some(doc_source.path.clone()),
            timestamp: Utc::now(),
            errors: Vec::new(),
        }).await;

        let document_id = self.store_document(collection_id, &parsed_doc).await?;
        let chunk_ids = self.store_chunks(collection_id, document_id, &chunks, &embeddings).await?;

        Ok(DocumentResult {
            source_path: doc_source.path,
            status: ProcessingStatus::Success,
            error: None,
            chunks_created: chunk_ids.len(),
            processing_time_ms: start_time.elapsed().as_millis() as u64,
        })
    }

    async fn fetch_document(&self, source: &DocumentSource) -> Result<Vec<u8>, IngestError> {
        match &source.source_type {
            SourceType::LocalFile => {
                let path = std::path::Path::new(&source.path);
                if !path.exists() {
                    return Err(IngestError::FileNotFound(source.path.clone()));
                }

                let metadata = tokio::fs::metadata(path).await?;
                if metadata.len() > (self.config.max_document_size_mb * 1024 * 1024) as u64 {
                    return Err(IngestError::FileTooLarge {
                        path: source.path.clone(),
                        size_mb: metadata.len() / (1024 * 1024),
                        max_size_mb: self.config.max_document_size_mb,
                    });
                }

                tokio::fs::read(path).await.map_err(IngestError::from)
            }
            SourceType::Url => {
                // Implement HTTP fetch with retry logic
                self.fetch_from_url(&source.path).await
            }
            SourceType::Memory(data) => Ok(data.clone()),
        }
    }

    async fn fetch_from_url(&self, url: &str) -> Result<Vec<u8>, IngestError> {
        use tokio::time::{sleep, Duration};

        for attempt in 0..self.config.retry_attempts {
            match reqwest::get(url).await {
                Ok(response) => {
                    if response.status().is_success() {
                        return response.bytes().await
                            .map(|bytes| bytes.to_vec())
                            .map_err(|e| IngestError::FetchError(e.to_string()));
                    } else {
                        return Err(IngestError::HttpError {
                            url: url.to_string(),
                            status: response.status().as_u16(),
                        });
                    }
                }
                Err(e) if attempt < self.config.retry_attempts - 1 => {
                    eprintln!("Fetch attempt {} failed: {}", attempt + 1, e);
                    sleep(Duration::from_millis(
                        self.config.retry_delay_ms * (1 << attempt)
                    )).await;
                }
                Err(e) => return Err(IngestError::FetchError(e.to_string())),
            }
        }

        unreachable!()
    }

    async fn parse_document(
        &self,
        source: &DocumentSource,
        content: Vec<u8>,
    ) -> Result<ParsedDocument, IngestError> {
        let file_extension = std::path::Path::new(&source.path)
            .extension()
            .and_then(|ext| ext.to_str())
            .unwrap_or("")
            .to_lowercase();

        let parser = DocumentParserFactory::create_parser(&file_extension)?;
        let parsed = parser.parse(content).await?;

        // Calculate fingerprint
        use sha2::{Sha256, Digest};
        let mut hasher = Sha256::new();
        hasher.update(&parsed.content);
        hasher.update(&serde_json::to_string(&parsed.metadata).unwrap_or_default());
        let fingerprint = format!("sha256:{:x}", hasher.finalize());

        Ok(ParsedDocument {
            title: parsed.title,
            content: parsed.content,
            metadata: parsed.metadata,
            fingerprint,
            source_path: source.path.clone(),
            content_type: file_extension,
            size: parsed.content.len(),
        })
    }

    async fn chunk_document(
        &self,
        document: &ParsedDocument,
        strategy: &ChunkStrategy,
    ) -> Result<Vec<DocumentChunk>, IngestError> {
        match strategy {
            ChunkStrategy::FixedSize { size, overlap } => {
                self.chunk_fixed_size(&document.content, *size, *overlap).await
            }
            ChunkStrategy::Semantic { min_size, max_size } => {
                self.chunk_semantic(&document.content, *min_size, *max_size).await
            }
            ChunkStrategy::Document => {
                Ok(vec![DocumentChunk {
                    content: document.content.clone(),
                    position: 0,
                    start_offset: 0,
                    end_offset: document.content.len(),
                    token_count: self.estimate_token_count(&document.content),
                    chunk_type: ChunkType::Text,
                    metadata: serde_json::Value::Null,
                }])
            }
        }
    }

    async fn chunk_fixed_size(
        &self,
        content: &str,
        chunk_size: usize,
        overlap: usize,
    ) -> Result<Vec<DocumentChunk>, IngestError> {
        let mut chunks = Vec::new();
        let mut position = 0;
        let mut start = 0;

        while start < content.len() {
            let end = std::cmp::min(start + chunk_size, content.len());
            let chunk_content = &content[start..end];

            if !chunk_content.trim().is_empty() {
                chunks.push(DocumentChunk {
                    content: chunk_content.to_string(),
                    position,
                    start_offset: start,
                    end_offset: end,
                    token_count: self.estimate_token_count(chunk_content),
                    chunk_type: ChunkType::Text,
                    metadata: serde_json::Value::Null,
                });

                position += 1;
            }

            start = if end == content.len() {
                break;
            } else {
                std::cmp::max(start + chunk_size - overlap, start + 1)
            };
        }

        Ok(chunks)
    }

    async fn chunk_semantic(
        &self,
        content: &str,
        min_size: usize,
        max_size: usize,
    ) -> Result<Vec<DocumentChunk>, IngestError> {
        // Implement semantic chunking based on sentence boundaries
        let sentences = content.split('.').collect::<Vec<_>>();
        let mut chunks = Vec::new();
        let mut current_chunk = String::new();
        let mut position = 0;
        let mut start_offset = 0;

        for sentence in sentences {
            let sentence = sentence.trim();
            if sentence.is_empty() {
                continue;
            }

            let potential_chunk = if current_chunk.is_empty() {
                sentence.to_string()
            } else {
                format!("{}. {}", current_chunk, sentence)
            };

            if potential_chunk.len() > max_size && !current_chunk.is_empty() {
                // Finalize current chunk
                let end_offset = start_offset + current_chunk.len();
                chunks.push(DocumentChunk {
                    content: current_chunk.clone(),
                    position,
                    start_offset,
                    end_offset,
                    token_count: self.estimate_token_count(&current_chunk),
                    chunk_type: ChunkType::Text,
                    metadata: serde_json::Value::Null,
                });

                position += 1;
                start_offset = end_offset;
                current_chunk = sentence.to_string();
            } else {
                current_chunk = potential_chunk;
            }
        }

        // Add final chunk if it meets minimum size
        if current_chunk.len() >= min_size {
            chunks.push(DocumentChunk {
                content: current_chunk,
                position,
                start_offset,
                end_offset: start_offset + current_chunk.len(),
                token_count: self.estimate_token_count(&current_chunk),
                chunk_type: ChunkType::Text,
                metadata: serde_json::Value::Null,
            });
        }

        Ok(chunks)
    }

    fn estimate_token_count(&self, text: &str) -> usize {
        // Simple token estimation: average 4 characters per token
        (text.len() + 3) / 4
    }

    async fn generate_embeddings(
        &self,
        chunks: &[DocumentChunk],
        options: &IngestOptions,
    ) -> Result<Vec<Vec<f32>>, IngestError> {
        let texts: Vec<String> = chunks.iter().map(|c| c.content.clone()).collect();

        // Check embedding cache if not skipping
        if !options.skip_embedding_cache {
            // Implement cache check logic
        }

        // Generate embeddings in batches
        let mut embeddings = Vec::new();
        for batch in texts.chunks(self.config.batch_size) {
            let batch_embeddings = self.embedding_service
                .embed_texts(batch.to_vec())
                .await
                .map_err(|e| IngestError::EmbeddingError(e.to_string()))?;

            embeddings.extend(batch_embeddings);
        }

        Ok(embeddings)
    }

    async fn store_document(
        &self,
        collection_id: &str,
        document: &ParsedDocument,
    ) -> Result<i64, IngestError> {
        use crate::schema::Document;

        let doc = Document {
            id: None,
            collection_id: collection_id.parse().unwrap(), // Assume collection_id is numeric
            title: document.title.clone(),
            source_path: Some(document.source_path.clone()),
            source_url: None,
            content_type: document.content_type.clone(),
            license: None,
            language: None,
            encoding: "utf-8".to_string(),
            fingerprint: document.fingerprint.clone(),
            size: document.size as i64,
            chunk_count: 0, // Will be updated after chunks are stored
            status: crate::schema::DocumentStatus::Processing,
            version: 1,
            parent_id: None,
            created_at: chrono::Utc::now(),
            updated_at: chrono::Utc::now(),
            indexed_at: None,
            metadata: Some(document.metadata.clone()),
            error_message: None,
        };

        self.sql_service.create_document(doc).await
            .map_err(|e| IngestError::DatabaseError(e.to_string()))
    }

    async fn store_chunks(
        &self,
        collection_id: &str,
        document_id: i64,
        chunks: &[DocumentChunk],
        embeddings: &[Vec<f32>],
    ) -> Result<Vec<i64>, IngestError> {
        use crate::schema::Chunk;

        let mut chunk_ids = Vec::new();

        for (i, (chunk, embedding)) in chunks.iter().zip(embeddings.iter()).enumerate() {
            // Store chunk in SQL
            let db_chunk = Chunk {
                id: None,
                document_id,
                collection_id: collection_id.parse().unwrap(),
                content: chunk.content.clone(),
                position: chunk.position as i32,
                start_offset: chunk.start_offset as i32,
                end_offset: chunk.end_offset as i32,
                token_count: chunk.token_count as i32,
                chunk_type: chunk.chunk_type.clone(),
                embedding_id: None, // Will be set after vector storage
                citation_anchor: None,
                language: None,
                metadata: Some(chunk.metadata.clone()),
                created_at: chrono::Utc::now(),
            };

            let chunk_id = self.sql_service.create_chunk(db_chunk).await
                .map_err(|e| IngestError::DatabaseError(e.to_string()))?;

            // Store embedding in vector database
            let embedding_id = format!("chunk_{}_{}", document_id, chunk_id);
            self.vector_db.store_embedding(&embedding_id, embedding.clone()).await
                .map_err(|e| IngestError::VectorStoreError(e.to_string()))?;

            // Update chunk with embedding_id
            self.sql_service.update_chunk_embedding_id(chunk_id, &embedding_id).await
                .map_err(|e| IngestError::DatabaseError(e.to_string()))?;

            chunk_ids.push(chunk_id);
        }

        Ok(chunk_ids)
    }

    async fn check_existing_document(
        &self,
        collection_id: &str,
        fingerprint: &str,
    ) -> Result<Option<ExistingDocument>, IngestError> {
        self.sql_service.find_document_by_fingerprint(collection_id, fingerprint).await
            .map_err(|e| IngestError::DatabaseError(e.to_string()))
    }

    async fn send_progress(&self, update: ProgressUpdate) {
        if let Err(_) = self.progress_tx.send(update) {
            eprintln!("Failed to send progress update - receiver may be dropped");
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ParsedDocument {
    pub title: String,
    pub content: String,
    pub metadata: serde_json::Value,
    pub fingerprint: String,
    pub source_path: String,
    pub content_type: String,
    pub size: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DocumentChunk {
    pub content: String,
    pub position: usize,
    pub start_offset: usize,
    pub end_offset: usize,
    pub token_count: usize,
    pub chunk_type: ChunkType,
    pub metadata: serde_json::Value,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct IngestResult {
    pub job_id: String,
    pub total_documents: usize,
    pub successful_documents: usize,
    pub failed_documents: usize,
    pub total_chunks: usize,
    pub processing_time_ms: u64,
    pub results: Vec<DocumentResult>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DocumentResult {
    pub source_path: String,
    pub status: ProcessingStatus,
    pub error: Option<String>,
    pub chunks_created: usize,
    pub processing_time_ms: u64,
}

#[derive(Debug, Clone, PartialEq, Serialize, Deserialize)]
pub enum ProcessingStatus {
    Success,
    Failed,
    Skipped,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ExistingDocument {
    pub id: i64,
    pub fingerprint: String,
    pub chunk_count: i64,
}

#[derive(Debug, thiserror::Error)]
pub enum IngestError {
    #[error("File not found: {0}")]
    FileNotFound(String),

    #[error("File too large: {path} ({size_mb}MB > {max_size_mb}MB)")]
    FileTooLarge { path: String, size_mb: u64, max_size_mb: usize },

    #[error("HTTP error for {url}: status {status}")]
    HttpError { url: String, status: u16 },

    #[error("Fetch error: {0}")]
    FetchError(String),

    #[error("Parse error: {0}")]
    ParseError(String),

    #[error("Embedding error: {0}")]
    EmbeddingError(String),

    #[error("Database error: {0}")]
    DatabaseError(String),

    #[error("Vector store error: {0}")]
    VectorStoreError(String),

    #[error("IO error: {0}")]
    Io(#[from] std::io::Error),

    #[error("Unsupported format: {0}")]
    UnsupportedFormat(String),
}

// Document parser factory and implementations would be defined here
pub struct DocumentParserFactory;

impl DocumentParserFactory {
    pub fn create_parser(file_extension: &str) -> Result<Box<dyn DocumentParser>, IngestError> {
        match file_extension {
            "txt" | "md" => Ok(Box::new(TextParser)),
            "pdf" => Ok(Box::new(PdfParser)),
            "docx" => Ok(Box::new(DocxParser)),
            "html" | "htm" => Ok(Box::new(HtmlParser)),
            "json" => Ok(Box::new(JsonParser)),
            "csv" => Ok(Box::new(CsvParser)),
            _ => Err(IngestError::UnsupportedFormat(file_extension.to_string())),
        }
    }
}

#[async_trait::async_trait]
pub trait DocumentParser: Send + Sync {
    async fn parse(&self, content: Vec<u8>) -> Result<ParsedContent, IngestError>;
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ParsedContent {
    pub title: String,
    pub content: String,
    pub metadata: serde_json::Value,
}

// Individual parser implementations
pub struct TextParser;
pub struct PdfParser;
pub struct DocxParser;
pub struct HtmlParser;
pub struct JsonParser;
pub struct CsvParser;

// Parser implementations would go here...
```

## Implementation Details

### Parallel Processing Strategy
- Use Tokio semaphores to limit concurrent document processing
- Implement batch processing for embeddings to optimize throughput
- Support graceful cancellation of long-running operations
- Provide configurable concurrency limits based on system resources

### Error Handling and Retry Logic
- Implement exponential backoff for transient failures
- Provide detailed error context and recovery suggestions
- Support selective retry of failed operations
- Maintain operation logs for debugging and auditing

### Progress Tracking System
- Use Tauri events for real-time progress updates to frontend
- Track progress at multiple granularities (job, document, chunk)
- Provide detailed timing and performance metrics
- Support cancellation and cleanup of interrupted operations

## Testing Strategy

### Unit Tests
- Test individual pipeline stages (fetch, parse, chunk, embed, index)
- Verify error handling and retry logic
- Test progress tracking and cancellation
- Validate parallel processing with various concurrency levels

### Integration Tests
- Test end-to-end pipeline with real documents
- Verify integration with embedding service and databases
- Test concurrent processing under load
- Validate progress reporting accuracy

### Performance Tests
- Benchmark processing throughput with various document sizes
- Test memory usage patterns with large document sets
- Measure embedding generation performance
- Validate scalability with increasing concurrency

## Acceptance Criteria

### AC-2.1.3.1 Pipeline Implementation
- [ ] All pipeline stages implemented and functional
- [ ] Support for all specified document formats
- [ ] Parallel processing with configurable limits
- [ ] Resumable operations for interrupted processing

### AC-2.1.3.2 Performance Requirements
- [ ] Process documents at minimum 10MB/minute
- [ ] Support concurrent processing of up to 100 documents
- [ ] Memory usage stays under 200MB during processing
- [ ] Embedding operations complete within 30 seconds per batch

### AC-2.1.3.3 Error Handling
- [ ] Comprehensive retry logic with exponential backoff
- [ ] Graceful handling of various error conditions
- [ ] Detailed error reporting and recovery suggestions
- [ ] Data consistency maintained during failures

### AC-2.1.3.4 Progress Tracking
- [ ] Real-time progress updates via Tauri events
- [ ] Detailed stage completion tracking
- [ ] Operation cancellation support
- [ ] Performance metrics collection and reporting