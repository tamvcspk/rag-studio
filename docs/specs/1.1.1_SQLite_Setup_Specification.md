# SQLite Setup Specification (Phase 1.1.1)

**Version:** 1.0
**Date:** September 15, 2025
**Status:** Implementation Ready
**Component:** Database & Storage Layer - SQLite Setup
**Priority:** CRITICAL

## Overview

This specification defines the SQLite database setup for RAG Studio's Phase 1.1.1 MVP implementation. The system uses a pragmatic approach with a single database (app_meta.db) for MVP, with clear upgrade path to split database architecture (app_meta.db + events.db) for production. Implements async WAL mode for optimal performance with event sourcing preparation.

## Architecture

### Database Design (MVP + Upgrade Path)

```mermaid
flowchart TB
  subgraph "SQLite MVP Architecture"
    direction TB
    APP[("app_meta.db<br/>WAL Mode NORMAL<br/>All Data (MVP)")]

    subgraph "Application Services"
      SQL[SqlService<br/>Diesel ORM]
      STATE[StateManager<br/>Shared State (MVP)]
      LOG[Log Router<br/>Structured Logging]
    end

    SQL --> APP
    STATE --> SQL
    LOG --> APP
  end

  subgraph "Production Upgrade Path"
    APP2[("app_meta.db<br/>Metadata Only")]
    EVT[("events.db<br/>Critical Events ACID")]
    STATE2[StateManager<br/>Actor-based]
    LOG2[Log Router<br/>Multi-sink]
  end
```

## Database Specifications

### app_meta.db (MVP - All Data)

**Purpose (MVP):** Store all application data including configuration, KB metadata, pipeline definitions, operational state, and event sourcing preparation
**WAL Mode:** NORMAL (balance performance/durability)
**Concurrency:** Read-heavy with periodic writes
**Upgrade Path:** Split to separate events.db for production event sourcing

#### Schema Design

```sql
-- Configuration and Settings
CREATE TABLE settings (
    id INTEGER PRIMARY KEY,
    key TEXT NOT NULL UNIQUE,
    value TEXT NOT NULL,
    schema_version INTEGER NOT NULL DEFAULT 1,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- Knowledge Base Metadata
CREATE TABLE knowledge_bases (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL,
    version INTEGER NOT NULL DEFAULT 1,
    status TEXT NOT NULL CHECK (status IN ('active', 'inactive', 'building', 'error')),
    embedder_model TEXT NOT NULL,
    chunk_size INTEGER NOT NULL DEFAULT 512,
    chunk_overlap INTEGER NOT NULL DEFAULT 50,
    description TEXT,
    metadata JSON,
    health_score REAL DEFAULT 1.0,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    pinned_version INTEGER,
    UNIQUE(name, version)
);

-- Document Metadata
CREATE TABLE documents (
    id TEXT PRIMARY KEY,
    kb_id TEXT NOT NULL,
    title TEXT NOT NULL,
    source_path TEXT NOT NULL,
    content_hash TEXT NOT NULL,
    license_info TEXT,
    metadata JSON,
    chunk_count INTEGER DEFAULT 0,
    size_bytes INTEGER DEFAULT 0,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (kb_id) REFERENCES knowledge_bases(id) ON DELETE CASCADE
);

-- Document Chunks Metadata
CREATE TABLE document_chunks (
    id TEXT PRIMARY KEY,
    document_id TEXT NOT NULL,
    kb_id TEXT NOT NULL,
    chunk_index INTEGER NOT NULL,
    content TEXT NOT NULL,
    content_hash TEXT NOT NULL,
    token_count INTEGER,
    metadata JSON,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (document_id) REFERENCES documents(id) ON DELETE CASCADE,
    FOREIGN KEY (kb_id) REFERENCES knowledge_bases(id) ON DELETE CASCADE,
    UNIQUE(document_id, chunk_index)
);

-- Pipeline Definitions
CREATE TABLE pipelines (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL UNIQUE,
    description TEXT,
    config JSON NOT NULL,
    status TEXT NOT NULL CHECK (status IN ('active', 'inactive', 'draft')),
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- Pipeline Run Tracking
CREATE TABLE pipeline_runs (
    id TEXT PRIMARY KEY,
    pipeline_id TEXT NOT NULL,
    kb_id TEXT,
    status TEXT NOT NULL CHECK (status IN ('pending', 'running', 'completed', 'failed', 'cancelled')),
    started_at DATETIME,
    completed_at DATETIME,
    error_message TEXT,
    metrics JSON,
    artifacts JSON,
    FOREIGN KEY (pipeline_id) REFERENCES pipelines(id),
    FOREIGN KEY (kb_id) REFERENCES knowledge_bases(id)
);

-- Tool Registry
CREATE TABLE tools (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL UNIQUE,
    type TEXT NOT NULL,
    config JSON NOT NULL,
    schema JSON NOT NULL,
    enabled BOOLEAN DEFAULT TRUE,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- Schedule Definitions
CREATE TABLE schedules (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL UNIQUE,
    cron_expression TEXT NOT NULL,
    pipeline_id TEXT NOT NULL,
    enabled BOOLEAN DEFAULT TRUE,
    last_run DATETIME,
    next_run DATETIME,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (pipeline_id) REFERENCES pipelines(id)
);

-- Flow Definitions
CREATE TABLE flows (
    id TEXT PRIMARY KEY,
    name TEXT NOT NULL UNIQUE,
    description TEXT,
    definition JSON NOT NULL,
    enabled BOOLEAN DEFAULT TRUE,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- Database Schema Versioning
CREATE TABLE schema_migrations (
    version INTEGER PRIMARY KEY,
    applied_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    description TEXT
);

-- Indexes for Performance
CREATE INDEX idx_documents_kb_id ON documents(kb_id);
CREATE INDEX idx_documents_hash ON documents(content_hash);
CREATE INDEX idx_chunks_document_id ON document_chunks(document_id);
CREATE INDEX idx_chunks_kb_id ON document_chunks(kb_id);
CREATE INDEX idx_chunks_hash ON document_chunks(content_hash);
CREATE INDEX idx_pipeline_runs_pipeline_id ON pipeline_runs(pipeline_id);
CREATE INDEX idx_pipeline_runs_status ON pipeline_runs(status);
CREATE INDEX idx_pipeline_runs_started_at ON pipeline_runs(started_at);
CREATE INDEX idx_schedules_next_run ON schedules(next_run);
CREATE INDEX idx_kb_status ON knowledge_bases(status);
CREATE INDEX idx_kb_name_version ON knowledge_bases(name, version);
```

### Event Sourcing Preparation (MVP)

**Purpose (MVP):** Event sourcing tables included in app_meta.db for preparation, with upgrade path to separate events.db
**Implementation:** Schema ready but simplified usage for MVP
**Upgrade Path:** Move to dedicated events.db with WAL FULL mode for production

#### Event Sourcing Schema (Added to app_meta.db)

```sql
-- Critical Event Log (MVP: in app_meta.db, Production: separate events.db)
CREATE TABLE events (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    event_id TEXT NOT NULL UNIQUE,
    event_type TEXT NOT NULL,
    aggregate_id TEXT NOT NULL,
    aggregate_type TEXT NOT NULL,
    sequence_number INTEGER NOT NULL,
    event_data JSON NOT NULL,
    metadata JSON,
    trace_id TEXT,
    user_id TEXT,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    checksum TEXT NOT NULL
);

-- Aggregate Snapshots for Performance (MVP: basic usage)
CREATE TABLE aggregate_snapshots (
    aggregate_id TEXT PRIMARY KEY,
    aggregate_type TEXT NOT NULL,
    sequence_number INTEGER NOT NULL,
    snapshot_data JSON NOT NULL,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- Event Processing Checkpoints (MVP: preparation only)
CREATE TABLE event_checkpoints (
    processor_name TEXT PRIMARY KEY,
    last_processed_sequence INTEGER NOT NULL DEFAULT 0,
    updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

-- Indexes for Event Sourcing
CREATE INDEX idx_events_aggregate ON events(aggregate_id, aggregate_type, sequence_number);
CREATE INDEX idx_events_type ON events(event_type);
CREATE INDEX idx_events_timestamp ON events(timestamp);
CREATE INDEX idx_events_trace_id ON events(trace_id);
CREATE UNIQUE INDEX idx_events_sequence ON events(aggregate_id, aggregate_type, sequence_number);
```

## Configuration Parameters

### Connection Pool Settings

```rust
// SqlService Configuration (MVP + Upgrade Path)
pub struct SqlConfig {
    // app_meta.db settings (MVP: single database)
    pub app_db_path: PathBuf,
    pub app_pool_size: u32,           // Default: 10
    pub app_connection_timeout: Duration, // Default: 30s
    pub app_wal_mode: WalMode,        // NORMAL

    // events.db settings (Production upgrade path)
    pub events_db_path: Option<PathBuf>, // None for MVP, Some for production
    pub events_pool_size: u32,        // Default: 5 (used when events_db enabled)
    pub events_connection_timeout: Duration, // Default: 10s
    pub events_wal_mode: WalMode,     // FULL (for separate events.db)

    // MVP flag
    pub use_split_databases: bool,    // false for MVP, true for production

    // Common settings
    pub busy_timeout: Duration,       // Default: 5s
    pub journal_size_limit: i64,      // Default: 64MB
    pub checkpoint_interval: Duration, // Default: 10s
    pub vacuum_interval: Duration,    // Default: 24h
    pub backup_enabled: bool,         // Default: true
    pub backup_interval: Duration,    // Default: 1h
}

pub enum WalMode {
    Normal,  // app_meta.db - balance performance/durability
    Full,    // events.db - maximum durability
}
```

### Performance Optimizations

```sql
-- WAL Mode Configuration
PRAGMA journal_mode = WAL;
PRAGMA synchronous = NORMAL; -- app_meta.db
PRAGMA synchronous = FULL;   -- events.db

-- Performance Settings
PRAGMA cache_size = -64000;  -- 64MB cache
PRAGMA temp_store = MEMORY;
PRAGMA mmap_size = 134217728; -- 128MB mmap
PRAGMA busy_timeout = 5000;   -- 5s with jitter
PRAGMA wal_autocheckpoint = 1000;

-- Concurrency Settings
PRAGMA read_uncommitted = false;
PRAGMA foreign_keys = true;
PRAGMA recursive_triggers = false;
```

## Implementation Requirements

### Diesel ORM Integration

```rust
// Database Models
#[derive(Queryable, Insertable, Identifiable)]
#[diesel(table_name = knowledge_bases)]
pub struct KnowledgeBase {
    pub id: String,
    pub name: String,
    pub version: i32,
    pub status: String,
    pub embedder_model: String,
    pub chunk_size: i32,
    pub chunk_overlap: i32,
    pub description: Option<String>,
    pub metadata: Option<String>, // JSON
    pub health_score: f64,
    pub created_at: NaiveDateTime,
    pub updated_at: NaiveDateTime,
    pub pinned_version: Option<i32>,
}

#[derive(Queryable, Insertable)]
#[diesel(table_name = events)]
pub struct Event {
    pub event_id: String,
    pub event_type: String,
    pub aggregate_id: String,
    pub aggregate_type: String,
    pub sequence_number: i32,
    pub event_data: String, // JSON
    pub metadata: Option<String>, // JSON
    pub trace_id: Option<String>,
    pub user_id: Option<String>,
    pub timestamp: NaiveDateTime,
    pub checksum: String,
}
```

### Connection Management

```rust
use diesel::prelude::*;
use diesel::r2d2::{self, ConnectionManager, Pool};

pub struct SqlService {
    app_pool: Pool<ConnectionManager<SqliteConnection>>,
    events_pool: Option<Pool<ConnectionManager<SqliteConnection>>>, // None for MVP
    config: SqlConfig,
}

impl SqlService {
    pub async fn new(config: SqlConfig) -> Result<Self, SqlError> {
        let app_manager = ConnectionManager::<SqliteConnection>::new(&config.app_db_path);
        let app_pool = Pool::builder()
            .max_size(config.app_pool_size)
            .connection_timeout(config.app_connection_timeout)
            .build(app_manager)?;

        // MVP: Optional events pool (None for single database)
        let events_pool = if config.use_split_databases {
            if let Some(events_db_path) = &config.events_db_path {
                let events_manager = ConnectionManager::<SqliteConnection>::new(events_db_path);
                let pool = Pool::builder()
                    .max_size(config.events_pool_size)
                    .connection_timeout(config.events_connection_timeout)
                    .build(events_manager)?;
                Some(pool)
            } else {
                return Err(SqlError::ConfigurationError("events_db_path required when use_split_databases is true".to_string()));
            }
        } else {
            None
        };

        Ok(Self {
            app_pool,
            events_pool,
            config,
        })
    }

    pub async fn get_app_connection(&self) -> Result<PooledConnection<ConnectionManager<SqliteConnection>>, r2d2::Error> {
        self.app_pool.get()
    }

    // MVP: Events use app connection if events_pool is None
    pub async fn get_events_connection(&self) -> Result<PooledConnection<ConnectionManager<SqliteConnection>>, r2d2::Error> {
        match &self.events_pool {
            Some(pool) => pool.get(),
            None => self.app_pool.get(), // MVP: Use app_pool for events
        }
    }
}
```

## Migration System Integration

### Migration Framework

```rust
use diesel_migrations::{embed_migrations, EmbeddedMigrations, MigrationHarness};

// MVP: Single migration set (includes event sourcing tables)
pub const APP_MIGRATIONS: EmbeddedMigrations = embed_migrations!("migrations/app_meta/");

// Production: Separate events migrations (when split database is enabled)
pub const EVENTS_MIGRATIONS: EmbeddedMigrations = embed_migrations!("migrations/events/");

impl SqlService {
    pub async fn run_migrations(&self) -> Result<(), Box<dyn std::error::Error>> {
        // Always run app_meta.db migrations
        let mut app_conn = self.get_app_connection().await?;
        app_conn.run_pending_migrations(APP_MIGRATIONS)?;

        // Production: Run events.db migrations if split database is enabled
        if self.config.use_split_databases {
            let mut events_conn = self.get_events_connection().await?;
            events_conn.run_pending_migrations(EVENTS_MIGRATIONS)?;
        }
        // MVP: Event sourcing tables are included in APP_MIGRATIONS

        Ok(())
    }
}
```

## Backup & Recovery

### Backup Strategy

```rust
impl SqlService {
    pub async fn backup_databases(&self) -> Result<(), SqlError> {
        let timestamp = Utc::now().format("%Y%m%d_%H%M%S");

        // Always backup app_meta.db
        let app_backup_path = format!("backups/app_meta_{}.db", timestamp);
        self.backup_database(&self.config.app_db_path, &app_backup_path).await?;

        // Production: Backup events.db if split database is enabled
        if self.config.use_split_databases {
            if let Some(events_db_path) = &self.config.events_db_path {
                let events_backup_path = format!("backups/events_{}.db", timestamp);
                self.backup_database(events_db_path, &events_backup_path).await?;
            }
        }
        // MVP: All data backed up with app_meta.db

        Ok(())
    }

    async fn backup_database(&self, source: &Path, dest: &str) -> Result<(), SqlError> {
        // SQLite VACUUM INTO for consistent backup
        let conn = Connection::open(source)?;
        conn.execute(&format!("VACUUM INTO '{}'", dest), [])?;
        Ok(())
    }
}
```

## Performance Monitoring

### Health Metrics

```rust
#[derive(Debug, Clone)]
pub struct DatabaseHealthMetrics {
    pub app_db_size: u64,
    pub events_db_size: Option<u64>, // None for MVP (single database)
    pub app_pool_active: u32,
    pub events_pool_active: Option<u32>, // None for MVP (single pool)
    pub wal_checkpoint_age: Duration,
    pub vacuum_last_run: DateTime<Utc>,
    pub backup_last_run: DateTime<Utc>,
    pub is_split_database: bool, // MVP vs Production mode
}

impl SqlService {
    pub async fn health_check(&self) -> DatabaseHealthMetrics {
        // Implementation for gathering health metrics
    }
}
```

## Error Handling

### Error Types

```rust
#[derive(Debug, thiserror::Error)]
pub enum SqlError {
    #[error("Database connection failed: {0}")]
    ConnectionFailed(#[from] r2d2::Error),

    #[error("Migration failed: {0}")]
    MigrationFailed(String),

    #[error("Query execution failed: {0}")]
    QueryFailed(#[from] diesel::result::Error),

    #[error("Backup operation failed: {0}")]
    BackupFailed(String),

    #[error("Transaction failed: {0}")]
    TransactionFailed(String),

    #[error("Configuration error: {0}")]
    ConfigurationError(String),
}
```

## Testing Strategy

### Unit Tests

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;

    #[tokio::test]
    async fn test_database_setup() {
        let temp_dir = TempDir::new().unwrap();
        let config = SqlConfig::test_config(temp_dir.path());

        let sql_service = SqlService::new(config).await.unwrap();
        sql_service.run_migrations().await.unwrap();

        // Verify tables exist
        let conn = sql_service.get_app_connection().await.unwrap();
        // Test queries...
    }

    #[tokio::test]
    async fn test_event_storage() {
        // Test event sourcing functionality
    }
}
```

## Success Criteria

### MVP Phase
- [ ] Single database architecture implemented (app_meta.db with event sourcing tables)
- [ ] Async WAL mode configured with NORMAL durability for balanced performance
- [ ] Diesel ORM integration with connection pooling
- [ ] Migration system for schema versioning (single migration set)
- [ ] Backup and restore functionality (single database)
- [ ] Performance optimization (busy_timeout, jitter, checkpoints)
- [ ] Error handling and health monitoring
- [ ] Event sourcing schema preparation (tables ready, basic usage)
- [ ] Comprehensive test coverage

### Production Upgrade Path
- [ ] Split database architecture (app_meta.db + events.db)
- [ ] Separate WAL modes (NORMAL for app, FULL for events)
- [ ] Dual connection pools with appropriate sizing
- [ ] Separate migration sets for each database
- [ ] Full event sourcing with undo/redo functionality
- [ ] ACID compliance for critical events in dedicated database

## Dependencies

```toml
[dependencies]
diesel = { version = "2.1", features = ["sqlite", "chrono", "r2d2"] }
diesel_migrations = "2.1"
r2d2 = "0.8"
chrono = { version = "0.4", features = ["serde"] }
tokio = { version = "1.0", features = ["full"] }
thiserror = "1.0"
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
tracing = "0.1"
```

## Integration Points

### MVP Integration
- **StateManager**: Arc<RwLock<AppState>> shared state with SQL persistence
- **Log Router**: Structured logging with app_meta.db for critical events
- **Migration System**: Single migration set with forward/backward compatibility
- **Backup Service**: Single database backup scheduling via StorageService
- **Performance Monitoring**: Health metrics with single database monitoring

### Production Upgrade Integration
- **StateManager**: Actor-based state management with dual SQL persistence
- **Log Router**: Multi-sink logging with dedicated events.db for ACID compliance
- **Migration System**: Separate migration sets for app_meta.db and events.db
- **Backup Service**: Dual database backup with different retention policies
- **Performance Monitoring**: Comprehensive health metrics for both databases