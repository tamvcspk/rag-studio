# 2.2.4 Performance Specification

**Phase:** 2.2 MCP Server Implementation
**Component:** Performance & Monitoring
**Priority:** HIGH
**Dependencies:** 2.2.1 Tool Registry, 2.2.3 Communication, 2.1.4 Hybrid Search

## Overview

Implement performance monitoring and optimization for MCP server to achieve <100ms retrieval targets with basic monitoring, simple latency tracking, and performance metrics collection, with upgrade path to full P50/P95 monitoring post-MVP.

## Requirements

### Functional Requirements

#### FR-2.2.4.1 Performance Targets and SLAs
- Achieve <100ms P50 latency for `kb.hybrid_search` operations
- Maintain <200ms P95 latency under normal load conditions (up to 50 concurrent requests)
- Support minimum 100 requests per second throughput
- Keep MCP server memory usage under 50MB during normal operations
- Achieve <5ms protocol overhead for message processing

#### FR-2.2.4.2 Latency Tracking and Measurement
- Implement comprehensive latency tracking for all MCP operations
- Track end-to-end latency from request receipt to response transmission
- Measure component-level latency (protocol, validation, tool execution, serialization)
- Support histogram-based latency collection for percentile calculations
- Provide real-time latency monitoring and alerting

#### FR-2.2.4.3 Throughput and Concurrency Monitoring
- Monitor concurrent request processing and queue depths
- Track request rates and throughput patterns over time
- Implement backpressure detection and congestion control
- Support load shedding when approaching capacity limits
- Provide resource utilization metrics (CPU, memory, I/O)

#### FR-2.2.4.4 Performance Metrics Collection
- Collect detailed performance metrics for all system components
- Track cache hit rates and effectiveness for hybrid search
- Monitor database query performance and connection pool usage
- Measure embedding service performance and batch efficiency
- Support custom metrics for domain-specific optimizations

#### FR-2.2.4.5 Performance Monitoring Dashboard
- Provide real-time performance dashboard via Tauri events
- Display key performance indicators and trends
- Support configurable alerting for performance degradation
- Enable performance regression detection and notification
- Integrate with existing UI components for seamless experience

#### FR-2.2.4.6 Performance Optimization Framework
- Implement automatic performance tuning for common scenarios
- Support configurable performance profiles (development, production)
- Provide performance benchmarking and regression testing tools
- Enable A/B testing for performance optimizations
- Support gradual rollout of performance improvements

### Non-Functional Requirements

#### NFR-2.2.4.1 Performance Targets
- `kb.hybrid_search`: <100ms P50, <200ms P95 latency
- Tool registry operations: <10ms latency overhead
- Message processing: <1ms protocol overhead
- Memory usage: <50MB normal operation, <100MB peak
- Throughput: >100 RPS sustained, >200 RPS burst

#### NFR-2.2.4.2 Monitoring Overhead
- Performance monitoring must consume <2% of system resources
- Metrics collection must not impact operation latency by >1ms
- Performance data storage must use <10MB disk space per hour
- Real-time dashboard updates must not affect server performance

#### NFR-2.2.4.3 Scalability and Reliability
- Performance monitoring must scale with system load
- Metrics collection must be reliable and not lose data
- Performance alerts must have <30 second detection latency
- System must maintain performance under 2x expected load

## Technical Specification

### Performance Monitoring Architecture

```rust
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::sync::Arc;
use std::time::{Duration, Instant};
use tokio::sync::RwLock;

#[derive(Debug, Clone)]
pub struct PerformanceMonitor {
    config: PerformanceConfig,
    metrics_collector: Arc<MetricsCollector>,
    latency_tracker: Arc<LatencyTracker>,
    throughput_monitor: Arc<ThroughputMonitor>,
    resource_monitor: Arc<ResourceMonitor>,
    alerting_system: Arc<AlertingSystem>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformanceConfig {
    pub target_p50_latency_ms: u64,
    pub target_p95_latency_ms: u64,
    pub max_memory_usage_mb: usize,
    pub target_throughput_rps: u64,
    pub monitoring_interval_ms: u64,
    pub metrics_retention_hours: u64,
    pub alert_thresholds: AlertThresholds,
    pub performance_profile: PerformanceProfile,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum PerformanceProfile {
    Development,
    Testing,
    Production,
    HighThroughput,
    LowLatency,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AlertThresholds {
    pub latency_p95_warning_ms: u64,
    pub latency_p95_critical_ms: u64,
    pub memory_usage_warning_percent: f64,
    pub memory_usage_critical_percent: f64,
    pub throughput_warning_percent: f64,
    pub error_rate_warning_percent: f64,
}

impl PerformanceMonitor {
    pub fn new(config: PerformanceConfig) -> Self {
        let metrics_collector = Arc::new(MetricsCollector::new(&config));
        let latency_tracker = Arc::new(LatencyTracker::new(&config));
        let throughput_monitor = Arc::new(ThroughputMonitor::new(&config));
        let resource_monitor = Arc::new(ResourceMonitor::new(&config));
        let alerting_system = Arc::new(AlertingSystem::new(&config));

        Self {
            config,
            metrics_collector,
            latency_tracker,
            throughput_monitor,
            resource_monitor,
            alerting_system,
        }
    }

    pub async fn start_monitoring(&self) -> Result<(), PerformanceError> {
        // Start background monitoring tasks
        let interval = Duration::from_millis(self.config.monitoring_interval_ms);

        // Start metrics collection
        let metrics_collector = self.metrics_collector.clone();
        tokio::spawn(async move {
            let mut ticker = tokio::time::interval(interval);
            loop {
                ticker.tick().await;
                if let Err(e) = metrics_collector.collect_metrics().await {
                    tracing::error!("Metrics collection error: {}", e);
                }
            }
        });

        // Start resource monitoring
        let resource_monitor = self.resource_monitor.clone();
        let alerting_system = self.alerting_system.clone();
        tokio::spawn(async move {
            let mut ticker = tokio::time::interval(interval);
            loop {
                ticker.tick().await;
                match resource_monitor.collect_resource_metrics().await {
                    Ok(metrics) => {
                        if let Err(e) = alerting_system.check_resource_alerts(&metrics).await {
                            tracing::error!("Alert checking error: {}", e);
                        }
                    }
                    Err(e) => tracing::error!("Resource monitoring error: {}", e),
                }
            }
        });

        tracing::info!("Performance monitoring started");
        Ok(())
    }

    pub async fn track_operation<F, R>(&self, operation: &str, future: F) -> Result<R, PerformanceError>
    where
        F: std::future::Future<Output = Result<R, Box<dyn std::error::Error + Send + Sync>>>,
    {
        let start_time = Instant::now();
        let operation_id = uuid::Uuid::new_v4().to_string();

        // Start operation tracking
        self.throughput_monitor.start_operation(operation, &operation_id).await;

        // Execute operation
        let result = future.await;
        let duration = start_time.elapsed();

        // Record latency
        self.latency_tracker.record_latency(operation, duration).await;

        // Complete operation tracking
        let success = result.is_ok();
        self.throughput_monitor.complete_operation(operation, &operation_id, success).await;

        // Update metrics
        self.metrics_collector.record_operation(operation, duration, success).await;

        // Check for performance alerts
        self.check_performance_alerts(operation, duration).await?;

        match result {
            Ok(value) => Ok(value),
            Err(e) => Err(PerformanceError::OperationFailed {
                operation: operation.to_string(),
                error: e.to_string(),
                duration_ms: duration.as_millis() as u64,
            }),
        }
    }

    async fn check_performance_alerts(
        &self,
        operation: &str,
        duration: Duration,
    ) -> Result<(), PerformanceError> {
        let duration_ms = duration.as_millis() as u64;

        // Check latency thresholds
        if duration_ms > self.config.alert_thresholds.latency_p95_critical_ms {
            self.alerting_system.send_alert(Alert {
                severity: AlertSeverity::Critical,
                category: AlertCategory::Latency,
                message: format!(
                    "Operation {} exceeded critical latency threshold: {}ms > {}ms",
                    operation,
                    duration_ms,
                    self.config.alert_thresholds.latency_p95_critical_ms
                ),
                timestamp: chrono::Utc::now(),
                metadata: serde_json::json!({
                    "operation": operation,
                    "duration_ms": duration_ms,
                    "threshold_ms": self.config.alert_thresholds.latency_p95_critical_ms
                }),
            }).await;
        } else if duration_ms > self.config.alert_thresholds.latency_p95_warning_ms {
            self.alerting_system.send_alert(Alert {
                severity: AlertSeverity::Warning,
                category: AlertCategory::Latency,
                message: format!(
                    "Operation {} exceeded warning latency threshold: {}ms > {}ms",
                    operation,
                    duration_ms,
                    self.config.alert_thresholds.latency_p95_warning_ms
                ),
                timestamp: chrono::Utc::now(),
                metadata: serde_json::json!({
                    "operation": operation,
                    "duration_ms": duration_ms,
                    "threshold_ms": self.config.alert_thresholds.latency_p95_warning_ms
                }),
            }).await;
        }

        Ok(())
    }

    pub async fn get_performance_summary(&self) -> PerformanceSummary {
        let latency_stats = self.latency_tracker.get_statistics().await;
        let throughput_stats = self.throughput_monitor.get_statistics().await;
        let resource_stats = self.resource_monitor.get_current_metrics().await
            .unwrap_or_default();

        PerformanceSummary {
            timestamp: chrono::Utc::now(),
            latency_statistics: latency_stats,
            throughput_statistics: throughput_stats,
            resource_usage: resource_stats,
            alert_summary: self.alerting_system.get_alert_summary().await,
            performance_health: self.calculate_performance_health().await,
        }
    }

    async fn calculate_performance_health(&self) -> PerformanceHealth {
        let latency_stats = self.latency_tracker.get_statistics().await;
        let resource_stats = self.resource_monitor.get_current_metrics().await
            .unwrap_or_default();

        let mut health_score = 100.0;
        let mut issues = Vec::new();

        // Check latency health
        if let Some(p95) = latency_stats.get("kb.hybrid_search")
            .and_then(|stats| stats.percentiles.get(&95)) {
            if *p95 > self.config.target_p95_latency_ms as f64 {
                health_score -= 30.0;
                issues.push(format!(
                    "Search latency P95 {}ms exceeds target {}ms",
                    p95,
                    self.config.target_p95_latency_ms
                ));
            }
        }

        // Check memory health
        let memory_usage_percent = (resource_stats.memory_usage_bytes as f64 /
            (self.config.max_memory_usage_mb * 1024 * 1024) as f64) * 100.0;

        if memory_usage_percent > self.config.alert_thresholds.memory_usage_critical_percent {
            health_score -= 40.0;
            issues.push(format!(
                "Memory usage {:.1}% exceeds critical threshold {:.1}%",
                memory_usage_percent,
                self.config.alert_thresholds.memory_usage_critical_percent
            ));
        } else if memory_usage_percent > self.config.alert_thresholds.memory_usage_warning_percent {
            health_score -= 20.0;
            issues.push(format!(
                "Memory usage {:.1}% exceeds warning threshold {:.1}%",
                memory_usage_percent,
                self.config.alert_thresholds.memory_usage_warning_percent
            ));
        }

        let status = match health_score {
            score if score >= 90.0 => HealthStatus::Excellent,
            score if score >= 75.0 => HealthStatus::Good,
            score if score >= 50.0 => HealthStatus::Warning,
            _ => HealthStatus::Critical,
        };

        PerformanceHealth {
            status,
            score: health_score,
            issues,
            recommendations: self.generate_recommendations(&issues).await,
        }
    }

    async fn generate_recommendations(&self, issues: &[String]) -> Vec<String> {
        let mut recommendations = Vec::new();

        for issue in issues {
            if issue.contains("latency") {
                recommendations.push("Consider enabling caching or optimizing database queries".to_string());
                recommendations.push("Review embedding service batch sizes and timeouts".to_string());
            }
            if issue.contains("memory") {
                recommendations.push("Reduce cache sizes or implement more aggressive cache eviction".to_string());
                recommendations.push("Check for memory leaks in long-running operations".to_string());
            }
        }

        if recommendations.is_empty() {
            recommendations.push("System is performing well".to_string());
        }

        recommendations
    }
}

#[derive(Debug, Clone)]
pub struct LatencyTracker {
    config: PerformanceConfig,
    histograms: Arc<RwLock<HashMap<String, LatencyHistogram>>>,
}

impl LatencyTracker {
    pub fn new(config: &PerformanceConfig) -> Self {
        Self {
            config: config.clone(),
            histograms: Arc::new(RwLock::new(HashMap::new())),
        }
    }

    pub async fn record_latency(&self, operation: &str, duration: Duration) {
        let duration_ms = duration.as_millis() as f64;

        let mut histograms = self.histograms.write().await;
        let histogram = histograms
            .entry(operation.to_string())
            .or_insert_with(|| LatencyHistogram::new());

        histogram.record(duration_ms);
    }

    pub async fn get_statistics(&self) -> HashMap<String, LatencyStatistics> {
        let histograms = self.histograms.read().await;
        let mut statistics = HashMap::new();

        for (operation, histogram) in histograms.iter() {
            statistics.insert(operation.clone(), histogram.get_statistics());
        }

        statistics
    }
}

#[derive(Debug, Clone)]
pub struct LatencyHistogram {
    samples: Vec<f64>,
    max_samples: usize,
}

impl LatencyHistogram {
    pub fn new() -> Self {
        Self {
            samples: Vec::new(),
            max_samples: 10000, // Keep last 10k samples
        }
    }

    pub fn record(&mut self, value: f64) {
        self.samples.push(value);

        // Keep only recent samples
        if self.samples.len() > self.max_samples {
            self.samples.remove(0);
        }
    }

    pub fn get_statistics(&self) -> LatencyStatistics {
        if self.samples.is_empty() {
            return LatencyStatistics::default();
        }

        let mut sorted_samples = self.samples.clone();
        sorted_samples.sort_by(|a, b| a.partial_cmp(b).unwrap());

        let count = sorted_samples.len();
        let sum: f64 = sorted_samples.iter().sum();
        let mean = sum / count as f64;

        let mut percentiles = HashMap::new();
        for &p in &[50, 90, 95, 99] {
            let index = ((p as f64 / 100.0) * (count - 1) as f64) as usize;
            percentiles.insert(p, sorted_samples[index]);
        }

        LatencyStatistics {
            count,
            mean,
            min: sorted_samples[0],
            max: sorted_samples[count - 1],
            percentiles,
        }
    }
}

#[derive(Debug, Clone)]
pub struct ThroughputMonitor {
    config: PerformanceConfig,
    active_operations: Arc<RwLock<HashMap<String, OperationMetrics>>>,
    completed_operations: Arc<RwLock<HashMap<String, CompletedOperationStats>>>,
}

impl ThroughputMonitor {
    pub fn new(config: &PerformanceConfig) -> Self {
        Self {
            config: config.clone(),
            active_operations: Arc::new(RwLock::new(HashMap::new())),
            completed_operations: Arc::new(RwLock::new(HashMap::new())),
        }
    }

    pub async fn start_operation(&self, operation: &str, operation_id: &str) {
        let mut active = self.active_operations.write().await;
        let metrics = active
            .entry(operation.to_string())
            .or_insert_with(|| OperationMetrics::new());

        metrics.start_operation(operation_id);
    }

    pub async fn complete_operation(&self, operation: &str, operation_id: &str, success: bool) {
        let mut active = self.active_operations.write().await;
        if let Some(metrics) = active.get_mut(operation) {
            metrics.complete_operation(operation_id, success);
        }

        // Update completed operation stats
        let mut completed = self.completed_operations.write().await;
        let stats = completed
            .entry(operation.to_string())
            .or_insert_with(|| CompletedOperationStats::new());

        stats.record_completion(success);
    }

    pub async fn get_statistics(&self) -> HashMap<String, ThroughputStatistics> {
        let active = self.active_operations.read().await;
        let completed = self.completed_operations.read().await;
        let mut statistics = HashMap::new();

        for (operation, metrics) in active.iter() {
            let completed_stats = completed.get(operation);

            statistics.insert(
                operation.clone(),
                ThroughputStatistics {
                    active_operations: metrics.active_count(),
                    completed_operations: completed_stats.map(|s| s.total_completed).unwrap_or(0),
                    success_rate: completed_stats.map(|s| s.success_rate()).unwrap_or(0.0),
                    current_rps: completed_stats.map(|s| s.current_rps()).unwrap_or(0.0),
                },
            );
        }

        statistics
    }
}

#[derive(Debug, Clone)]
pub struct OperationMetrics {
    active_operations: HashMap<String, Instant>,
}

impl OperationMetrics {
    pub fn new() -> Self {
        Self {
            active_operations: HashMap::new(),
        }
    }

    pub fn start_operation(&mut self, operation_id: &str) {
        self.active_operations.insert(operation_id.to_string(), Instant::now());
    }

    pub fn complete_operation(&mut self, operation_id: &str, _success: bool) {
        self.active_operations.remove(operation_id);
    }

    pub fn active_count(&self) -> usize {
        self.active_operations.len()
    }
}

#[derive(Debug, Clone)]
pub struct CompletedOperationStats {
    pub total_completed: u64,
    pub total_successful: u64,
    recent_completions: Vec<(Instant, bool)>,
}

impl CompletedOperationStats {
    pub fn new() -> Self {
        Self {
            total_completed: 0,
            total_successful: 0,
            recent_completions: Vec::new(),
        }
    }

    pub fn record_completion(&mut self, success: bool) {
        self.total_completed += 1;
        if success {
            self.total_successful += 1;
        }

        self.recent_completions.push((Instant::now(), success));

        // Keep only recent completions (last minute)
        let cutoff = Instant::now() - Duration::from_secs(60);
        self.recent_completions.retain(|(timestamp, _)| *timestamp > cutoff);
    }

    pub fn success_rate(&self) -> f64 {
        if self.total_completed == 0 {
            0.0
        } else {
            self.total_successful as f64 / self.total_completed as f64
        }
    }

    pub fn current_rps(&self) -> f64 {
        self.recent_completions.len() as f64 / 60.0
    }
}

// Resource monitoring and other supporting structures...
#[derive(Debug, Clone)]
pub struct ResourceMonitor {
    config: PerformanceConfig,
}

impl ResourceMonitor {
    pub fn new(config: &PerformanceConfig) -> Self {
        Self {
            config: config.clone(),
        }
    }

    pub async fn collect_resource_metrics(&self) -> Result<ResourceMetrics, PerformanceError> {
        // Platform-specific resource monitoring
        Ok(ResourceMetrics {
            memory_usage_bytes: self.get_memory_usage().await?,
            cpu_usage_percent: self.get_cpu_usage().await?,
            disk_usage_bytes: self.get_disk_usage().await?,
            network_bytes_per_second: self.get_network_usage().await?,
        })
    }

    pub async fn get_current_metrics(&self) -> Result<ResourceMetrics, PerformanceError> {
        self.collect_resource_metrics().await
    }

    async fn get_memory_usage(&self) -> Result<usize, PerformanceError> {
        // Simplified implementation - would use system APIs in production
        Ok(25 * 1024 * 1024) // 25MB placeholder
    }

    async fn get_cpu_usage(&self) -> Result<f64, PerformanceError> {
        Ok(15.0) // 15% placeholder
    }

    async fn get_disk_usage(&self) -> Result<usize, PerformanceError> {
        Ok(100 * 1024 * 1024) // 100MB placeholder
    }

    async fn get_network_usage(&self) -> Result<f64, PerformanceError> {
        Ok(1024.0) // 1KB/s placeholder
    }
}

// Supporting data structures
#[derive(Debug, Clone, Serialize, Deserialize, Default)]
pub struct LatencyStatistics {
    pub count: usize,
    pub mean: f64,
    pub min: f64,
    pub max: f64,
    pub percentiles: HashMap<u8, f64>, // P50, P90, P95, P99
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ThroughputStatistics {
    pub active_operations: usize,
    pub completed_operations: u64,
    pub success_rate: f64,
    pub current_rps: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize, Default)]
pub struct ResourceMetrics {
    pub memory_usage_bytes: usize,
    pub cpu_usage_percent: f64,
    pub disk_usage_bytes: usize,
    pub network_bytes_per_second: f64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformanceSummary {
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub latency_statistics: HashMap<String, LatencyStatistics>,
    pub throughput_statistics: HashMap<String, ThroughputStatistics>,
    pub resource_usage: ResourceMetrics,
    pub alert_summary: AlertSummary,
    pub performance_health: PerformanceHealth,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformanceHealth {
    pub status: HealthStatus,
    pub score: f64,
    pub issues: Vec<String>,
    pub recommendations: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum HealthStatus {
    Excellent,
    Good,
    Warning,
    Critical,
}

// Alerting system structures
#[derive(Debug, Clone)]
pub struct AlertingSystem {
    config: PerformanceConfig,
    recent_alerts: Arc<RwLock<Vec<Alert>>>,
}

impl AlertingSystem {
    pub fn new(config: &PerformanceConfig) -> Self {
        Self {
            config: config.clone(),
            recent_alerts: Arc::new(RwLock::new(Vec::new())),
        }
    }

    pub async fn send_alert(&self, alert: Alert) {
        tracing::warn!("PERFORMANCE ALERT: {} - {}", alert.severity, alert.message);

        let mut alerts = self.recent_alerts.write().await;
        alerts.push(alert);

        // Keep only recent alerts (last hour)
        let cutoff = chrono::Utc::now() - chrono::Duration::hours(1);
        alerts.retain(|alert| alert.timestamp > cutoff);
    }

    pub async fn check_resource_alerts(&self, metrics: &ResourceMetrics) -> Result<(), PerformanceError> {
        let memory_usage_percent = (metrics.memory_usage_bytes as f64 /
            (self.config.max_memory_usage_mb * 1024 * 1024) as f64) * 100.0;

        if memory_usage_percent > self.config.alert_thresholds.memory_usage_critical_percent {
            self.send_alert(Alert {
                severity: AlertSeverity::Critical,
                category: AlertCategory::Memory,
                message: format!(
                    "Memory usage critical: {:.1}% > {:.1}%",
                    memory_usage_percent,
                    self.config.alert_thresholds.memory_usage_critical_percent
                ),
                timestamp: chrono::Utc::now(),
                metadata: serde_json::json!({
                    "memory_usage_bytes": metrics.memory_usage_bytes,
                    "memory_usage_percent": memory_usage_percent
                }),
            }).await;
        }

        Ok(())
    }

    pub async fn get_alert_summary(&self) -> AlertSummary {
        let alerts = self.recent_alerts.read().await;

        let mut counts = HashMap::new();
        for alert in alerts.iter() {
            *counts.entry(alert.severity.clone()).or_insert(0) += 1;
        }

        AlertSummary {
            total_alerts: alerts.len(),
            alert_counts_by_severity: counts,
            most_recent_alert: alerts.last().cloned(),
        }
    }
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct Alert {
    pub severity: AlertSeverity,
    pub category: AlertCategory,
    pub message: String,
    pub timestamp: chrono::DateTime<chrono::Utc>,
    pub metadata: serde_json::Value,
}

#[derive(Debug, Clone, Serialize, Deserialize, Hash, Eq, PartialEq)]
pub enum AlertSeverity {
    Info,
    Warning,
    Critical,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum AlertCategory {
    Latency,
    Memory,
    Throughput,
    ErrorRate,
    Resource,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct AlertSummary {
    pub total_alerts: usize,
    pub alert_counts_by_severity: HashMap<AlertSeverity, usize>,
    pub most_recent_alert: Option<Alert>,
}

// Additional supporting structures
#[derive(Debug, Clone)]
pub struct MetricsCollector {
    config: PerformanceConfig,
}

impl MetricsCollector {
    pub fn new(config: &PerformanceConfig) -> Self {
        Self {
            config: config.clone(),
        }
    }

    pub async fn collect_metrics(&self) -> Result<(), PerformanceError> {
        // Collect and aggregate metrics
        Ok(())
    }

    pub async fn record_operation(&self, _operation: &str, _duration: Duration, _success: bool) {
        // Record operation metrics
    }
}

#[derive(Debug, thiserror::Error)]
pub enum PerformanceError {
    #[error("Operation {operation} failed after {duration_ms}ms: {error}")]
    OperationFailed {
        operation: String,
        error: String,
        duration_ms: u64,
    },

    #[error("Performance threshold exceeded: {metric} = {value} > {threshold}")]
    ThresholdExceeded {
        metric: String,
        value: f64,
        threshold: f64,
    },

    #[error("Resource monitoring error: {error}")]
    ResourceMonitoringError { error: String },

    #[error("Metrics collection error: {error}")]
    MetricsCollectionError { error: String },

    #[error("Alert system error: {error}")]
    AlertSystemError { error: String },
}
```

## Implementation Details

### Performance Monitoring Strategy
- Implement comprehensive latency tracking with histogram-based percentile calculation
- Use efficient concurrent data structures for minimal monitoring overhead
- Support configurable monitoring intervals and retention policies
- Provide real-time performance dashboard integration with Tauri events

### Optimization Framework
- Implement automatic performance profiling for different workload patterns
- Support A/B testing framework for performance optimizations
- Provide performance regression detection with automated alerts
- Enable gradual rollout of performance improvements with rollback capability

### Resource Management
- Monitor system resources (CPU, memory, disk, network) with minimal overhead
- Implement dynamic resource limits based on available system capacity
- Support backpressure mechanisms to prevent resource exhaustion
- Provide predictive scaling recommendations based on usage patterns

## Testing Strategy

### Performance Tests
- Benchmark all operations against target SLAs with realistic workloads
- Test concurrent request handling up to 200% of expected capacity
- Validate memory usage patterns under sustained load
- Measure monitoring overhead impact on system performance

### Load Tests
- Test sustained throughput at target RPS for extended periods
- Validate performance under burst traffic scenarios
- Test graceful degradation under resource constraints
- Verify alert system responsiveness under various conditions

### Regression Tests
- Automated performance regression testing in CI/CD pipeline
- Baseline performance benchmarks for all major operations
- Performance comparison testing for optimization changes
- Historical performance trend analysis and reporting

## Acceptance Criteria

### AC-2.2.4.1 Performance Targets
- [ ] `kb.hybrid_search` achieves <100ms P50 latency
- [ ] System maintains <200ms P95 latency under normal load
- [ ] Support for 100+ RPS sustained throughput
- [ ] Memory usage stays under 50MB during normal operations

### AC-2.2.4.2 Monitoring Implementation
- [ ] Comprehensive latency tracking with percentile calculation
- [ ] Real-time throughput and concurrency monitoring
- [ ] Resource usage monitoring with configurable alerts
- [ ] Performance dashboard integration with UI

### AC-2.2.4.3 Alerting and Health
- [ ] Configurable performance alerts with severity levels
- [ ] Health scoring system with actionable recommendations
- [ ] Performance regression detection and notification
- [ ] Integration with existing error handling and logging

### AC-2.2.4.4 Optimization Framework
- [ ] Performance profiling and benchmarking tools
- [ ] Configurable performance profiles for different environments
- [ ] A/B testing framework for optimization validation
- [ ] Clear upgrade path to advanced monitoring post-MVP